"""
åœ–è­œå°å…¥ API - Excel/CSV æª”æ¡ˆæ™ºèƒ½è§£æ
æ•´åˆ LLM é€²è¡Œè‡ªå‹•åŒ–æ¨™é¡Œç”Ÿæˆã€æè¿°æ’°å¯«èˆ‡é—œä¿‚æ¨è–¦
"""
from fastapi import APIRouter, HTTPException, UploadFile, File
from typing import List, Dict, Any, Optional
import pandas as pd
import io
import logging
import json
import os
from datetime import datetime

logger = logging.getLogger(__name__)
router = APIRouter()


# ===== LLM Prompt é…ç½® (v2.0 - å¢å¼·ç‰ˆ) =====
def build_node_analysis_prompt(
    raw_content: str, 
    existing_nodes: Optional[List[Dict[str, Any]]] = None
) -> str:
    """
    å»ºæ§‹ç¯€é»åˆ†æçš„ LLM Promptï¼ˆå¢å¼·ç‰ˆï¼‰
    
    âœ¨ v2.0 æ–°å¢åŠŸèƒ½ï¼š
    - é ˜åŸŸç‰¹åŒ–ï¼šIDP æ•¸æ“šè™•ç†èˆ‡é€£è²«æ€§åˆ†æ
    - è³ªé‡æ§åˆ¶ï¼šåš´æ ¼çš„ JSON æ ¼å¼é©—è­‰
    - å¤šèªè¨€æ”¯æŒï¼šä¸­è‹±æ–‡æ··åˆè™•ç†å„ªåŒ–
    - ä¸Šä¸‹æ–‡è¨˜æ†¶ï¼šåƒè€ƒç¾æœ‰ç¯€é»é¿å…é‡è¤‡
    - å¢å¼·é—œä¿‚é¡å‹ï¼š6 ç¨®æ™ºèƒ½é€£ç·šæ¨¡å¼
    """
    # æ§‹å»ºç¾æœ‰ç¯€é»ä¸Šä¸‹æ–‡ï¼ˆå«é¡å‹çµ±è¨ˆï¼‰
    existing_nodes_summary = ""
    node_types_count = {}
    
    if existing_nodes:
        # çµ±è¨ˆç¯€é»é¡å‹åˆ†ä½ˆ
        for node in existing_nodes:
            node_type = node.get('type', 'unknown')
            node_types_count[node_type] = node_types_count.get(node_type, 0) + 1
        
        # ç”Ÿæˆç¯€é»åˆ—è¡¨ï¼ˆé™åˆ¶ 25 å€‹æœ€ç›¸é—œçš„ç¯€é»ï¼‰
        node_list = "\n".join([
            f"- [{node.get('id', 'unknown')}] {node.get('name', 'unknown')} "
            f"({node.get('type', 'unknown')}) - {node.get('description', '')[:50]}..."
            for node in existing_nodes[:25]
        ])
        
        # é¡å‹çµ±è¨ˆæ‘˜è¦
        type_summary = ", ".join([f"{t}: {c}å€‹" for t, c in node_types_count.items()])
        
        existing_nodes_summary = f"""
### ğŸ“Š ç¾æœ‰çŸ¥è­˜åœ–è­œæ¦‚è¦½
**ç¯€é»ç¸½æ•¸**: {len(existing_nodes)} å€‹
**é¡å‹åˆ†ä½ˆ**: {type_summary}

**ç¯€é»åˆ—è¡¨**:
{node_list}

âš ï¸ **é‡è¦**: é¿å…å‰µå»ºèˆ‡ç¾æœ‰ç¯€é»é«˜åº¦ç›¸ä¼¼çš„å…§å®¹ï¼Œå„ªå…ˆå»ºç«‹é€£ç·šé—œä¿‚ã€‚
"""
    
    prompt = f"""ä½ æ˜¯ IDP Co., Ltd. çš„ä¼æ¥­ç´šçŸ¥è­˜åœ–è­œæ¶æ§‹å¸«ï¼Œå°ˆç²¾æ–¼ï¼š
- ğŸ“‹ **æ•¸æ“šè™•ç†æµç¨‹åˆ†æ**ï¼šèº«ä»½é©—è­‰ã€æ–‡ä»¶è™•ç†ã€è³‡æ–™é€£è²«æ€§
- ğŸ”— **è·¨ç³»çµ±æ•´åˆè¨­è¨ˆ**ï¼šAPI ä¸²æ¥ã€æ•¸æ“šåŒæ­¥ã€æµç¨‹è‡ªå‹•åŒ–
- ğŸ§  **æ™ºèƒ½çŸ¥è­˜èƒå–**ï¼šæŠ€è¡“æ–‡æª”ã€æ¥­å‹™æµç¨‹ã€æœ€ä½³å¯¦è¸

## ğŸ¯ æ ¸å¿ƒä»»å‹™

### 1ï¸âƒ£ æ¨™é¡Œç”Ÿæˆ (label) - ç²¾æº–å‘½å
**è¦æ±‚**:
- ä½¿ç”¨ 3-10 å€‹å­—çš„å°ˆæ¥­è¡“èª
- æ”¯æŒä¸­è‹±æ–‡æ··åˆï¼ˆå¦‚ï¼šOAuth 2.0 é©—è­‰æµç¨‹ï¼‰
- å„ªå…ˆç´šï¼šæŠ€è¡“é—œéµè© > æ¥­å‹™æ¦‚å¿µ > é€šç”¨æè¿°

**ç¯„ä¾‹**:
âœ… å¥½ï¼šã€ŒJWT Token åˆ·æ–°æ©Ÿåˆ¶ã€ã€ã€ŒGDPR å€‹è³‡ä¿è­·åˆè¦ã€
âŒ å·®ï¼šã€Œé—œæ–¼ç³»çµ±çš„èªªæ˜ã€ã€ã€Œæ–‡ä»¶å…§å®¹ã€

### 2ï¸âƒ£ æ·±åº¦æè¿° (description) - çµæ§‹åŒ–é—¡è¿°
**å­—æ•¸**: 200-250 å­—ï¼ˆæ›´è©³ç´°çš„ä¸Šä¸‹æ–‡ï¼‰
**å¿…é ˆåŒ…å«**:
```
ã€èƒŒæ™¯ã€‘æ­¤çŸ¥è­˜çš„ä¾†æºå ´æ™¯ã€è§¸ç™¼æ¢ä»¶æˆ–å‰ç½®éœ€æ±‚
ã€æ ¸å¿ƒå…§å®¹ã€‘é—œéµæŠ€è¡“é»ã€æµç¨‹æ­¥é©Ÿæˆ–æ¥­å‹™é‚è¼¯ï¼ˆä½¿ç”¨æ¢åˆ—å¼ï¼‰
ã€å¯¦éš›æ‡‰ç”¨ã€‘å¯è§£æ±ºçš„å•é¡Œã€é©ç”¨å ´æ™¯ã€é æœŸæ•ˆç›Š
ã€æ³¨æ„äº‹é …ã€‘æ½›åœ¨é¢¨éšªã€é™åˆ¶æ¢ä»¶æˆ–æœ€ä½³å¯¦è¸å»ºè­°
```

**å“è³ªæ¨™æº–**:
- é¿å…ç± çµ±æè¿°ï¼Œæä¾›å…·é«”çš„æŠ€è¡“ç´°ç¯€æˆ–æ¥­å‹™å ´æ™¯
- ä½¿ç”¨å°ˆæ¥­è¡“èªä½†ä¿æŒå¯è®€æ€§
- æ¨™è¨»é—œéµæ•¸æ“šã€ç‰ˆæœ¬è™Ÿã€æ™‚é–“é»ç­‰ç²¾ç¢ºä¿¡æ¯

### 3ï¸âƒ£ çŸ¥è­˜é¡å‹ (type) - ç²¾ç¢ºåˆ†é¡
**æ¨™æº–é¡å‹** (å„ªå…ˆä½¿ç”¨):
- `æŠ€è¡“æ¶æ§‹` - ç³»çµ±è¨­è¨ˆã€æ¡†æ¶é¸å‹ã€æŠ€è¡“æ£§
- `APIä»‹é¢` - ç«¯é»å®šç¾©ã€åƒæ•¸è¦ç¯„ã€éŸ¿æ‡‰æ ¼å¼
- `æ•¸æ“šæµç¨‹` - ETLã€è³‡æ–™è½‰æ›ã€è™•ç†é‚è¼¯
- `å®‰å…¨è¦ç¯„` - é©—è­‰æ©Ÿåˆ¶ã€åŠ å¯†æ–¹å¼ã€æ¬Šé™æ§åˆ¶
- `æ¥­å‹™æµç¨‹` - å·¥ä½œæµã€å¯©æ‰¹æµç¨‹ã€æ“ä½œæŒ‡å—
- `æœ€ä½³å¯¦è¸` - ç·¨ç¢¼è¦ç¯„ã€è¨­è¨ˆæ¨¡å¼ã€å„ªåŒ–æ–¹æ¡ˆ
- `å•é¡Œæ’æŸ¥` - éŒ¯èª¤è™•ç†ã€é™¤éŒ¯æŠ€å·§ã€æ•…éšœæ¢å¾©
- `é…ç½®æ–‡æª”` - ç’°å¢ƒè¨­å®šã€åƒæ•¸èªªæ˜ã€éƒ¨ç½²æŒ‡å—

**è‡ªè¨‚é¡å‹** (è‹¥ä¸ç¬¦åˆä¸Šè¿°):
- ä½¿ç”¨ç°¡çŸ­çš„å°ˆæ¥­è¡“èªï¼ˆ2-4 å­—ï¼‰

### 4ï¸âƒ£ æ™ºèƒ½é€£ç·šæ¨è–¦ (links) - å…­ç¶­é—œä¿‚åˆ†æ
{existing_nodes_summary}

**é—œä¿‚é¡å‹å®šç¾©** (è«‹æ ¹æ“šå¯¦éš›æƒ…æ³é¸æ“‡):

1. **å› æœé—œä¿‚** (`causality`)
   - A æ˜¯ B çš„å‰ææ¢ä»¶ã€è§¸ç™¼å› ç´ æˆ–ç›´æ¥åŸå› 
   - ç¯„ä¾‹ï¼šã€Œç”¨æˆ¶èªè­‰ã€â†’ã€Œæˆæ¬Šä»¤ç‰Œç”Ÿæˆã€

2. **ä¾è³´é—œä¿‚** (`dependency`)
   - A çš„é‹ä½œéœ€è¦ B çš„æ”¯æŒæˆ– A èª¿ç”¨ B çš„åŠŸèƒ½
   - ç¯„ä¾‹ï¼šã€Œå‰ç«¯ç™»éŒ„é ã€â†’ã€Œèº«ä»½é©—è­‰ APIã€

3. **æ™‚åºé—œä¿‚** (`sequence`)
   - A åœ¨æ™‚é–“é †åºä¸Šå…ˆæ–¼ B åŸ·è¡Œ
   - ç¯„ä¾‹ï¼šã€Œæ•¸æ“šæ¡é›†ã€â†’ã€Œæ•¸æ“šæ¸…æ´—ã€â†’ã€Œæ•¸æ“šåˆ†æã€

4. **åŒ…å«é—œä¿‚** (`composition`)
   - A æ˜¯ B çš„çµ„æˆéƒ¨åˆ†æˆ–å­æ¨¡çµ„
   - ç¯„ä¾‹ï¼šã€ŒOAuth ç³»çµ±ã€åŒ…å«ã€ŒToken ç®¡ç†ã€ã€ã€ŒScope æ¬Šé™ã€

5. **äº’è£œé—œä¿‚** (`complement`)
   - A èˆ‡ B å…±åŒå®ŒæˆæŸå€‹ç›®æ¨™ï¼Œç¼ºä¸€ä¸å¯
   - ç¯„ä¾‹ï¼šã€Œè³‡æ–™åŠ å¯†ã€â†â†’ã€Œå¯†é‘°ç®¡ç†ã€

6. **å°æ¯”é—œä¿‚** (`contrast`)
   - A èˆ‡ B æ˜¯ä¸åŒçš„å¯¦ç¾æ–¹æ¡ˆæˆ–å­˜åœ¨æ¬Šè¡¡å–æ¨
   - ç¯„ä¾‹ï¼šã€ŒSession é©—è­‰ã€ vs ã€ŒJWT é©—è­‰ã€

**æ¨è–¦è¦å‰‡**:
- æ¯å€‹æ–°ç¯€é»å»ºè­° **2-5 å€‹é€£ç·š**ï¼ˆæœ€å¤š 5 å€‹ï¼‰
- å„ªå…ˆé€£æ¥é«˜ç›¸é—œæ€§ç¯€é»ï¼ˆç›¸åŒé¡å‹æˆ–æ¥­å‹™å ´æ™¯ï¼‰
- å¿…é ˆæä¾› **è©³ç´°ä¸”å…·é«”çš„ç†ç”±**ï¼ˆ50-100 å­—ï¼‰
- é¿å…å»ºç«‹éæ–¼æ³›æ³›æˆ–ç‰½å¼·çš„é—œä¿‚

## ğŸ“¥ å¾…åˆ†æå…§å®¹
```text
{raw_content}
```

## ğŸ”’ è¼¸å‡ºæ ¼å¼è¦æ±‚ï¼ˆåš´æ ¼éµå®ˆï¼‰

**æ ¼å¼**: ç´” JSONï¼Œç„¡ Markdown åŒ…è£¹ï¼Œç„¡é¡å¤–è¨»è§£
**ç·¨ç¢¼**: UTF-8
**çµæ§‹**: å¿…é ˆå®Œå…¨ç¬¦åˆä»¥ä¸‹ Schema

```json
{{
  "label": "6-10å­—çš„ç²¾æº–æ¨™é¡Œ",
  "description": "200-250å­—çš„çµæ§‹åŒ–æè¿°ï¼ŒåŒ…å«ã€èƒŒæ™¯ã€‘ã€æ ¸å¿ƒå…§å®¹ã€‘ã€å¯¦éš›æ‡‰ç”¨ã€‘ã€æ³¨æ„äº‹é …ã€‘å››å€‹éƒ¨åˆ†",
  "type": "å¾æ¨™æº–é¡å‹ä¸­é¸æ“‡æˆ–è‡ªè¨‚ï¼ˆ2-4å­—ï¼‰",
  "links": [
    {{
      "target_id": "existing_node_123",
      "relation": "causality/dependency/sequence/composition/complement/contrast",
      "reason": "è©³ç´°èªªæ˜ç‚ºä½•å»ºç«‹æ­¤é€£ç·šï¼ŒåŒ…å«å…·é«”çš„æ¥­å‹™å ´æ™¯æˆ–æŠ€è¡“é‚è¼¯ï¼ˆ50-100å­—ï¼‰"
    }}
  ],
  "metadata": {{
    "confidence": 0.85,
    "keywords": ["é—œéµè©1", "é—œéµè©2", "é—œéµè©3"],
    "language": "zh-TW"
  }}
}}
```

## âš ï¸ å“è³ªæª¢æŸ¥æ¸…å–®
- [ ] label æ˜¯å¦å…·å‚™å°ˆæ¥­æ€§ä¸”ç„¡æ­§ç¾©ï¼Ÿ
- [ ] description æ˜¯å¦åŒ…å«å››å€‹å¿…è¦éƒ¨åˆ†ä¸”å­—æ•¸é”æ¨™ï¼Ÿ
- [ ] type æ˜¯å¦ä½¿ç”¨æ¨™æº–é¡å‹æˆ–åˆç†çš„è‡ªè¨‚é¡å‹ï¼Ÿ
- [ ] links çš„ reason æ˜¯å¦å…·é«”ä¸”æœ‰èªªæœåŠ›ï¼Ÿ
- [ ] JSON æ ¼å¼æ˜¯å¦å®Œå…¨æ­£ç¢ºï¼ˆç„¡é¡å¤–å­—ç¬¦ï¼‰ï¼Ÿ

âš¡ **ç«‹å³é–‹å§‹åˆ†æä¸¦è¼¸å‡ºæ¨™æº– JSONï¼**
"""
    return prompt


def parse_llm_response(llm_output: str) -> Dict[str, Any]:
    """
    è§£æ LLM å›æ‡‰ï¼ˆå¢å¼·ç‰ˆï¼‰
    
    âœ¨ æ”¯æ´æ ¼å¼ï¼š
    - ç´” JSON
    - Markdown ä»£ç¢¼å¡ŠåŒ…è£¹çš„ JSON
    - å«é¡å¤–æ–‡å­—çš„ JSONï¼ˆè‡ªå‹•æå–ï¼‰
    
    âœ… æ–°å¢é©—è­‰ï¼š
    - å¿…è¦æ¬„ä½æª¢æŸ¥
    - é¡å‹é©—è­‰
    - æ•¸æ“šæ¸…æ´—
    """
    try:
        # å˜—è©¦ç›´æ¥è§£æ JSON
        data = json.loads(llm_output)
    except json.JSONDecodeError:
        # å˜—è©¦å¾ markdown ä»£ç¢¼å¡Šä¸­æå– JSON
        import re
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', llm_output, re.DOTALL)
        if json_match:
            data = json.loads(json_match.group(1))
        else:
            # å˜—è©¦æ‰¾åˆ°ç¬¬ä¸€å€‹ { å’Œæœ€å¾Œä¸€å€‹ }
            start = llm_output.find('{')
            end = llm_output.rfind('}')
            if start != -1 and end != -1:
                data = json.loads(llm_output[start:end+1])
            else:
                raise ValueError("ç„¡æ³•å¾ LLM å›æ‡‰ä¸­è§£æ JSON")
    
    # ===== æ•¸æ“šé©—è­‰èˆ‡æ¸…æ´— =====
    
    # 1. å¿…è¦æ¬„ä½æª¢æŸ¥
    required_fields = ['label', 'description', 'type']
    for field in required_fields:
        if field not in data:
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {field}")
    
    # 2. label é©—è­‰ï¼ˆ3-20 å­—ï¼‰
    if not (3 <= len(data['label']) <= 20):
        logger.warning(f"label é•·åº¦ä¸ç¬¦åˆè¦ç¯„: {data['label']}")
    
    # 3. description é©—è­‰ï¼ˆ50-500 å­—ï¼‰
    desc_len = len(data['description'])
    if desc_len < 50:
        logger.warning(f"description éçŸ­ ({desc_len} å­—)ï¼Œå»ºè­°è‡³å°‘ 200 å­—")
    elif desc_len > 500:
        logger.warning(f"description éé•· ({desc_len} å­—)ï¼Œå·²æˆªæ–·")
        data['description'] = data['description'][:500] + "..."
    
    # 4. links é©—è­‰
    if 'links' not in data:
        data['links'] = []
    elif len(data['links']) > 5:
        logger.warning(f"links æ•¸é‡éå¤š ({len(data['links'])}), ä¿ç•™å‰ 5 å€‹")
        data['links'] = data['links'][:5]
    
    # 5. metadata æå–æˆ–ç”Ÿæˆ
    if 'metadata' not in data:
        data['metadata'] = {
            'confidence': 0.75,
            'keywords': [],
            'language': 'zh-TW'
        }
    
    return data


async def call_llm_analysis(prompt: str) -> Dict[str, Any]:
    """
    èª¿ç”¨ LLM é€²è¡Œå…§å®¹åˆ†æ
    TODO: æ•´åˆå¯¦éš›çš„ LLM APIï¼ˆOpenAIã€Claudeã€Dify ç­‰ï¼‰
    """
    # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„ LLM API
    # ç›®å‰å…ˆè¿”å›æ¨¡æ“¬æ•¸æ“š
    logger.warning("âš ï¸ LLM åŠŸèƒ½å°šæœªæ•´åˆï¼Œä½¿ç”¨æ¨¡æ“¬å›æ‡‰")
    
    return {
        "label": "å¾…æ•´åˆ LLM åˆ†æ",
        "description": "æ­¤ç¯€é»æ­£åœ¨ç­‰å¾… LLM æœå‹™æ•´åˆã€‚å®Œæˆå¾Œå°‡è‡ªå‹•ç”Ÿæˆæ·±åº¦æè¿°ï¼ŒåŒ…å«å…§å®¹èƒŒæ™¯ã€æ ¸å¿ƒçµè«–èˆ‡æ‡‰ç”¨å ´æ™¯ã€‚",
        "type": "æœªåˆ†é¡",
        "links": []
    }


# ===== API ç«¯é» =====
@router.post("/import/excel")
async def import_excel(file: UploadFile = File(...)):
    """
    å°å…¥ Excel/CSV æª”æ¡ˆä¸¦ä½¿ç”¨ LLM æ™ºèƒ½è§£æ
    
    æ”¯æ´æ ¼å¼ï¼š
    - .xlsx (Excel)
    - .csv (é€—è™Ÿåˆ†éš”)
    
    å›å‚³æ ¼å¼ï¼š
    [
        {
            "id": "node_uuid",
            "name": "ç¯€é»åç¨±",
            "label": "AI ç”Ÿæˆçš„æ¨™é¡Œ",
            "description": "AI ç”Ÿæˆçš„æè¿°",
            "type": "ç¯€é»é¡å‹",
            "group": 1,
            "links": [...]
        }
    ]
    """
    try:
        # é©—è­‰æª”æ¡ˆé¡å‹
        if not file.filename:
            raise HTTPException(status_code=400, detail="æª”æ¡ˆåç¨±ç„¡æ•ˆ")
        
        filename = file.filename.lower()
        if not (filename.endswith('.xlsx') or filename.endswith('.csv')):
            raise HTTPException(
                status_code=400,
                detail="ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼Œè«‹ä¸Šå‚³ .xlsx æˆ– .csv æª”æ¡ˆ"
            )
        
        # è®€å–æª”æ¡ˆå…§å®¹
        contents = await file.read()
        
        # è§£ææª”æ¡ˆ
        if filename.endswith('.xlsx'):
            df = pd.read_excel(io.BytesIO(contents))
        else:
            df = pd.read_csv(io.BytesIO(contents))
        
        logger.info(f"ğŸ“Š æˆåŠŸè®€å–æª”æ¡ˆ: {file.filename}, è¡Œæ•¸: {len(df)}")
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç©º
        if df.empty:
            raise HTTPException(status_code=400, detail="æª”æ¡ˆå…§å®¹ç‚ºç©º")
        
        # TODO: ç²å–ç¾æœ‰åœ–è­œç¯€é»åˆ—è¡¨ï¼ˆç”¨æ–¼ LLM é—œè¯åˆ†æï¼‰
        existing_nodes = []  # å¾ kuzu_manager æˆ–å…¶ä»–ä¾†æºç²å–
        
        # è™•ç†æ¯ä¸€è¡Œæ•¸æ“š
        nodes = []
        for idx, row in df.iterrows():
            try:
                # å°‡ index è½‰æ›ç‚º int é¿å…é¡å‹éŒ¯èª¤
                index = int(idx) if isinstance(idx, (int, float)) else 0
                
                # æå–åŸå§‹å…§å®¹
                raw_content = " | ".join([
                    f"{col}: {row[col]}" 
                    for col in df.columns 
                    if pd.notna(row[col])
                ])
                
                # å»ºæ§‹ LLM Prompt
                prompt = build_node_analysis_prompt(raw_content, existing_nodes)
                
                # èª¿ç”¨ LLM åˆ†æï¼ˆç›®å‰ç‚ºæ¨¡æ“¬ï¼‰
                llm_result = await call_llm_analysis(prompt)
                
                # å»ºæ§‹ç¯€é»å°è±¡
                first_column = str(df.columns[0])
                node_name = str(row[first_column]) if first_column in row and pd.notna(row[first_column]) else f"ç¯€é» {index + 1}"
                
                node = {
                    "id": f"node_{datetime.now().timestamp()}_{index}",
                    "name": node_name,
                    "label": llm_result.get("label", "æœªå‘½å"),
                    "description": llm_result.get("description", ""),
                    "type": llm_result.get("type", "æœªåˆ†é¡"),
                    "group": 1,
                    "size": 20,
                    "links": llm_result.get("links", []),
                    "raw_data": row.to_dict()  # ä¿ç•™åŸå§‹æ•¸æ“š
                }
                
                nodes.append(node)
                logger.info(f"âœ… ç¯€é» {index + 1} è™•ç†å®Œæˆ: {node['label']}")
                
            except Exception as e:
                # ä½¿ç”¨ idx ç›´æ¥é€²è¡ŒéŒ¯èª¤è™•ç†
                error_index = int(idx) if isinstance(idx, (int, float)) else 0
                logger.error(f"âŒ è™•ç†ç¬¬ {error_index + 1} è¡Œæ™‚å‡ºéŒ¯: {e}")
                # å‰µå»ºæœ€å°åŒ–ç¯€é»
                nodes.append({
                    "id": f"node_error_{error_index}",
                    "name": f"éŒ¯èª¤ç¯€é» {error_index + 1}",
                    "label": "è§£æå¤±æ•—",
                    "description": f"è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                    "type": "éŒ¯èª¤",
                    "group": 1,
                    "size": 15,
                    "links": []
                })
        
        logger.info(f"ğŸ‰ Excel å°å…¥å®Œæˆï¼ŒæˆåŠŸè™•ç† {len(nodes)} å€‹ç¯€é»")
        return nodes
        
    except pd.errors.EmptyDataError:
        raise HTTPException(status_code=400, detail="æª”æ¡ˆå…§å®¹ç‚ºç©ºæˆ–æ ¼å¼éŒ¯èª¤")
    except Exception as e:
        logger.error(f"âŒ å°å…¥å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å°å…¥å¤±æ•—: {str(e)}")


@router.get("/import/template")
async def download_template():
    """
    ä¸‹è¼‰ Excel å°å…¥æ¨¡æ¿
    """
    # TODO: å¯¦ç¾æ¨¡æ¿ä¸‹è¼‰åŠŸèƒ½
    return {
        "message": "æ¨¡æ¿ä¸‹è¼‰åŠŸèƒ½é–‹ç™¼ä¸­",
        "suggested_columns": [
            "æ¨™é¡Œ",
            "å…§å®¹",
            "é¡å‹",
            "æ¨™ç±¤",
            "ä¾†æº"
        ]
    }


@router.get("/import/status")
async def get_import_status():
    """
    ç²å–ç•¶å‰å°å…¥ä»»å‹™ç‹€æ…‹
    """
    # TODO: å¯¦ç¾ä»»å‹™ç‹€æ…‹è¿½è¹¤
    return {
        "status": "ready",
        "message": "å°±ç·’ï¼Œå¯é–‹å§‹å°å…¥"
    }
