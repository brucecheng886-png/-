"""
åœ–è­œå°å…¥ API - Excel/CSV æª”æ¡ˆæ™ºèƒ½è§£æ
æ•´åˆ LLM é€²è¡Œè‡ªå‹•åŒ–æ¨™é¡Œç”Ÿæˆã€æè¿°æ’°å¯«èˆ‡é—œä¿‚æ¨è–¦

v5.0 â€” 3000 ç­†ä¸€æ¬¡æ€§åˆ†æ:
- è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°: æ ¹æ“šæ–‡å­—é•·åº¦å‹•æ…‹èª¿æ•´ BATCH_SIZE (5~50)
- å¤§é‡æ¨¡å¼: >100 ç­†å•Ÿç”¨ fast-mode prompt (çœç•¥ suggested_links, ç²¾ç°¡è¼¸å‡º)
- æ–‡å­—æˆªæ–·: æ¯ç­† â‰¤500 å­—é€ LLMï¼ŒåŸæ–‡ä¿ç•™åœ¨ raw_data
- é«˜ä½µç™¼: MAX_CONCURRENCY=8, BATCH_DELAY=0.3s
- ETA è¿½è¹¤: å³æ™‚å›å ±é è¨ˆå‰©é¤˜æ™‚é–“ + ååé‡
- èƒŒæ™¯ä»»å‹™ + æ–·é»çºŒå‚³ + æŒ‡æ•¸é€€é¿é‡è©¦
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Request
from typing import List, Dict, Any, Optional
import pandas as pd
import io
import logging
import json
import asyncio
import random
import uuid
import time
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)
router = APIRouter()

# ===== å¯èª¿åƒæ•¸ =====
MAX_CONCURRENCY = 2     # æœ€å¤§ä¸¦è¡Œ LLM è«‹æ±‚æ•¸ (æœ¬åœ° Ollama å–® GPU: 2 å³å¯)
LLM_TIMEOUT = 300       # å–®æ¬¡ LLM å‘¼å«è¶…æ™‚ (ç§’, æœ¬åœ°æ¨¡å‹è¼ƒæ…¢éœ€åŠ é•·)
MAX_RETRIES = 3         # æ¯æ‰¹æœ€å¤§é‡è©¦æ¬¡æ•¸
RETRY_BASE_DELAY = 3    # é‡è©¦åŸºç¤å»¶é² (ç§’)
BATCH_DELAY = 1.0       # æ‰¹æ¬¡é–“å»¶é² (ç§’), è®“ GPU å–˜å£æ°£
MAX_TEXT_LEN = 500      # æ¯ç­†é€ LLM çš„æœ€å¤§å­—æ•¸ (åŸæ–‡ä¿ç•™åœ¨ raw_data)
FAST_MODE_THRESHOLD = 100  # è³‡æ–™ç­†æ•¸è¶…éæ­¤å€¼å•Ÿç”¨ fast-mode prompt
TARGET_BATCH_TOKENS = 2000  # æ¯æ‰¹ç›®æ¨™ input token æ•¸ (å°æ‰¹æ¬¡é¿å… GPU OOM)

# ===== Checkpoint è·¯å¾‘ =====
CHECKPOINT_DIR = Path(__file__).resolve().parent.parent.parent / "data" / "import_checkpoints"
CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)

# ===== å…¨åŸŸä»»å‹™è¿½è¹¤å™¨ =====
_import_tasks: Dict[str, Dict[str, Any]] = {}
_TASK_EXPIRY_SECONDS = 3600  # å®Œæˆçš„ä»»å‹™ä¿ç•™ 1 å°æ™‚å¾Œè‡ªå‹•æ¸…ç†


def _cleanup_expired_tasks():
    """æ¸…ç†å·²éæœŸçš„å®Œæˆä»»å‹™ï¼ˆé‡‹æ”¾è¨˜æ†¶é«”ï¼Œç‰¹åˆ¥æ˜¯ 3000 ç¯€é»çš„ nodes é™£åˆ—ï¼‰"""
    now = datetime.now()
    expired = []
    for tid, task in _import_tasks.items():
        if task.get("status") in ("done", "error"):
            finished_str = task.get("finished_at")
            if finished_str:
                try:
                    finished = datetime.fromisoformat(finished_str)
                    if (now - finished).total_seconds() > _TASK_EXPIRY_SECONDS:
                        expired.append(tid)
                except (ValueError, TypeError):
                    pass
    for tid in expired:
        del _import_tasks[tid]
    if expired:
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç† {len(expired)} å€‹éæœŸä»»å‹™")


# ===== Token ä¼°ç®— & è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å° =====

def _estimate_tokens(text: str) -> int:
    """ç²—ä¼° token æ•¸ï¼šä¸­æ–‡å­— â‰ˆ 1 token / å­—ï¼Œè‹±æ–‡ â‰ˆ 1 token / 4 å­—å…ƒ"""
    if not text:
        return 0
    cn = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    return cn + (len(text) - cn) // 4


def _compute_adaptive_batch_size(row_texts: List[str]) -> int:
    """
    æ ¹æ“šå¹³å‡æ–‡å­—é•·åº¦è‡ªé©æ‡‰èª¿æ•´ BATCH_SIZE
    - çŸ­æ–‡æœ¬ (æ¨æ–‡ã€æ¨™é¡Œ): batch=40~50
    - ä¸­æ–‡æœ¬ (æ‘˜è¦): batch=15~30
    - é•·æ–‡æœ¬ (æ–‡ç« ): batch=5~10
    """
    if not row_texts:
        return 10
    # å–å‰ 50 ç­†æ¨£æœ¬ä¼°ç®— (å·²æˆªæ–·)
    sample = row_texts[:50]
    avg_tokens = sum(_estimate_tokens(t[:MAX_TEXT_LEN]) for t in sample) / len(sample)
    batch_size = max(5, min(50, int(TARGET_BATCH_TOKENS / max(avg_tokens, 10))))
    return batch_size


def _truncate_text(text: str, max_len: int = MAX_TEXT_LEN) -> str:
    """æˆªæ–·æ–‡å­—è‡³æŒ‡å®šé•·åº¦ï¼ˆé™„çœç•¥è™Ÿï¼‰"""
    if len(text) <= max_len:
        return text
    return text[:max_len] + "..."


# ===== LLM Prompt é…ç½® =====

SYSTEM_ROLE = """ä½ æ˜¯ä¼æ¥­ç´šçŸ¥è­˜åœ–è­œæ¶æ§‹å¸«ã€‚æ ¹æ“šè¼¸å…¥è³‡æ–™ï¼Œç‚ºæ¯ç­†è¨˜éŒ„ç”¢ç”Ÿçµæ§‹åŒ–çš„åœ–è­œç¯€é»ã€‚

è¼¸å‡ºè¦å‰‡ï¼š
- å›å‚³ä¸€å€‹ JSON é™£åˆ—ï¼Œæ¯å€‹å…ƒç´ å°æ‡‰ä¸€ç­†è¼¸å…¥
- ä¸è¦åŒ…å« Markdown æ¨™è¨˜æˆ–ä»»ä½•é¡å¤–æ–‡å­—
- åªè¼¸å‡ºç´” JSON"""

NODE_SCHEMA = """{
  "label": "3-10å­—ç²¾æº–æ¨™é¡Œ",
  "description": "100-200å­—æè¿°ï¼Œå«èƒŒæ™¯ã€æ ¸å¿ƒå…§å®¹ã€æ‡‰ç”¨å ´æ™¯",
  "type": "æŠ€è¡“æ¶æ§‹|APIä»‹é¢|æ•¸æ“šæµç¨‹|å®‰å…¨è¦ç¯„|æ¥­å‹™æµç¨‹|æœ€ä½³å¯¦è¸|å•é¡Œæ’æŸ¥|é…ç½®æ–‡æª”|è‡ªè¨‚(2-4å­—)",
  "keywords": ["é—œéµè©1", "é—œéµè©2", "é—œéµè©3"],
  "suggested_links": [
    {"target_index": 0, "relation": "dependency|causality|sequence|composition|complement|contrast", "reason": "é€£ç·šåŸå› (30å­—å…§)"}
  ]
}"""


def build_batch_prompt(
    rows: List[str],
    existing_node_names: Optional[List[str]] = None
) -> str:
    """
    å»ºæ§‹æ‰¹æ¬¡åˆ†æ Prompt â€” ä¸€æ¬¡é€å¤šç­†è³‡æ–™çµ¦ LLM
    
    Args:
        rows: æ¯ç­†è³‡æ–™çš„æ–‡å­—æè¿° (å·²æ ¼å¼åŒ–ç‚º "col: val | col: val")
        existing_node_names: ç¾æœ‰ç¯€é»åç¨±åˆ—è¡¨ (ç”¨æ–¼é¿å…é‡è¤‡)
    """
    # ç·¨è™Ÿæ¯ç­†è³‡æ–™
    numbered = "\n".join([f"[{i}] {row}" for i, row in enumerate(rows)])
    
    # ç¾æœ‰ç¯€é»ä¸Šä¸‹æ–‡ (ç²¾ç°¡ç‰ˆ â€” åªåˆ—åç¨±)
    existing_ctx = ""
    if existing_node_names:
        names = ", ".join(existing_node_names[:30])
        existing_ctx = f"\nå·²å­˜åœ¨çš„ç¯€é»: {names}\né¿å…å»ºç«‹é‡è¤‡ç¯€é»ï¼Œsuggested_links çš„ target_index å¯ç”¨ -1 ä»£è¡¨é€£ç·šåˆ°å·²å­˜åœ¨çš„ç¯€é»ã€‚\n"
    
    return f"""{SYSTEM_ROLE}

ç¯€é» Schema:
{NODE_SCHEMA}
{existing_ctx}
ä»¥ä¸‹æœ‰ {len(rows)} ç­†è³‡æ–™ï¼Œè«‹è¼¸å‡º JSON é™£åˆ—ï¼ˆé•·åº¦ = {len(rows)}ï¼‰ï¼š

{numbered}

suggested_links.target_index æŒ‡å‘æœ¬æ‰¹æ¬¡å…§å…¶ä»–è³‡æ–™çš„ç·¨è™Ÿ (0-based)ï¼Œè‹¥ç„¡é—œè¯å‰‡ç•™ç©ºé™£åˆ—ã€‚
è¼¸å‡ºç´” JSON é™£åˆ—ï¼Œä¸è¦ä»»ä½•å¤šé¤˜æ–‡å­—ï¼š"""


# ===== Fast-mode Promptï¼ˆå¤§é‡è³‡æ–™ > FAST_MODE_THRESHOLD ç­†å•Ÿç”¨ï¼‰ =====

SYSTEM_ROLE_FAST = """ä½ æ˜¯çŸ¥è­˜åœ–è­œæ¶æ§‹å¸«ã€‚å¿«é€Ÿç‚ºæ¯ç­†è¨˜éŒ„æ­¸é¡ã€‚

è¦å‰‡ï¼šå›å‚³ JSON é™£åˆ—ï¼Œæ¯å…ƒç´ å°æ‡‰ä¸€ç­†è¼¸å…¥ã€‚ä¸è¦ Markdownï¼Œåªæœ‰ç´” JSONã€‚"""

NODE_SCHEMA_FAST = """{
  "label": "3-8å­—æ¨™é¡Œ",
  "description": "30-80å­—æ‘˜è¦",
  "type": "åˆ†é¡(2-4å­—)",
  "keywords": ["é—œéµè©1", "é—œéµè©2"]
}"""


def build_batch_prompt_fast(rows: List[str]) -> str:
    """
    å¤§é‡æ¨¡å¼ Prompt â€” çœç•¥ suggested_linksï¼Œç²¾ç°¡ description
    è¼¸å‡º token ç´„ç‚ºå®Œæ•´æ¨¡å¼çš„ 1/3ï¼Œé©åˆ 30~50 ç­†/æ‰¹
    """
    numbered = "\n".join([f"[{i}] {row}" for i, row in enumerate(rows)])
    return f"""{SYSTEM_ROLE_FAST}

Schema:
{NODE_SCHEMA_FAST}

{len(rows)} ç­†è³‡æ–™:
{numbered}

è¼¸å‡º JSON é™£åˆ— (é•·åº¦={len(rows)})ï¼š"""


def build_node_analysis_prompt(
    raw_content: str, 
    existing_nodes: Optional[List[Dict[str, Any]]] = None
) -> str:
    """
    å–®ç­†åˆ†æ Promptï¼ˆå‘ä¸‹ç›¸å®¹ï¼Œç•¶æ‰¹æ¬¡ç‚º 1 æ™‚ä½¿ç”¨ï¼‰
    """
    existing_names = None
    if existing_nodes:
        existing_names = [n.get('name', '') for n in existing_nodes if n.get('name')]
    return build_batch_prompt([raw_content], existing_names)


def _extract_json(text: str):
    """å¾ LLM å›æ‡‰ä¸­æå– JSONï¼ˆæ”¯æ´é™£åˆ—å’Œç‰©ä»¶ï¼‰"""
    import re
    text = text.strip()
    
    # 1. ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # 2. å¾ markdown ä»£ç¢¼å¡Šæå–
    md = re.search(r'```(?:json)?\s*([\[\{].*?[\]\}])\s*```', text, re.DOTALL)
    if md:
        try:
            return json.loads(md.group(1))
        except json.JSONDecodeError:
            pass
    
    # 3. æ‰¾ç¬¬ä¸€å€‹ [ ] æˆ– { }
    for open_ch, close_ch in [('[', ']'), ('{', '}')]:
        start = text.find(open_ch)
        end = text.rfind(close_ch)
        if start != -1 and end > start:
            try:
                return json.loads(text[start:end + 1])
            except json.JSONDecodeError:
                continue
    
    raise ValueError("ç„¡æ³•å¾ LLM å›æ‡‰ä¸­è§£æ JSON")


def _validate_node(data: Dict[str, Any]) -> Dict[str, Any]:
    """é©—è­‰ä¸¦æ¸…æ´—å–®ä¸€ç¯€é»è³‡æ–™"""
    # å˜—è©¦å¾å¸¸è¦‹åˆ¥åæå– labelï¼ˆLLM å¯èƒ½å›å‚³ä¸åŒæ¬„ä½åï¼‰
    if 'label' not in data:
        for alt in ['title', 'name', 'æ¨™é¡Œ', 'åç¨±']:
            if alt in data and data[alt]:
                data['label'] = str(data[alt])[:50]
                break
    
    required = ['label', 'description', 'type']
    for f in required:
        if f not in data or not data[f]:
            data[f] = "æœªæä¾›" if f != 'type' else "æœªåˆ†é¡"
    
    # description æˆªæ–·
    if len(data.get('description', '')) > 500:
        data['description'] = data['description'][:500] + "..."
    
    # links / suggested_links çµ±ä¸€ç‚º suggested_links
    if 'links' in data and 'suggested_links' not in data:
        data['suggested_links'] = data.pop('links')
    data.setdefault('suggested_links', [])
    if len(data['suggested_links']) > 5:
        data['suggested_links'] = data['suggested_links'][:5]
    
    # keywords
    data.setdefault('keywords', [])
    
    return data


def parse_llm_response(llm_output: str) -> List[Dict[str, Any]]:
    """
    è§£æ LLM å›æ‡‰ï¼Œçµ±ä¸€å›å‚³ List[Dict]
    æ”¯æ´ï¼šç´” JSON / Markdown åŒ…è£¹ / é™£åˆ—æˆ–å–®ä¸€ç‰©ä»¶
    """
    raw = _extract_json(llm_output)
    
    # çµ±ä¸€ç‚ºåˆ—è¡¨
    items = raw if isinstance(raw, list) else [raw]
    
    return [_validate_node(item) for item in items]


# ===== é è¨­å›æ‡‰ =====
_DEFAULT_NODE = {
    "label": "LLM åˆ†æå¤±æ•—",
    "description": "è‡ªå‹•åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æ‰‹å‹•ç·¨è¼¯æ­¤ç¯€é»ã€‚",
    "type": "æœªåˆ†é¡",
    "keywords": [],
    "suggested_links": [],
}

_NO_KEY_NODE = {
    "label": "å¾…é…ç½® LLM",
    "description": "Dify API Key å°šæœªè¨­å®šï¼Œè«‹è‡³ç³»çµ±è¨­å®šé é¢é…ç½®å¾Œé‡æ–°åŒ¯å…¥ã€‚",
    "type": "æœªåˆ†é¡",
    "keywords": [],
    "suggested_links": [],
}


async def call_llm_batch(
    rows: List[str],
    existing_node_names: Optional[List[str]] = None,
    fast_mode: bool = False,
) -> List[Dict[str, Any]]:
    """
    æ‰¹æ¬¡å‘¼å« Dify LLM â€” ä¸€æ¬¡é€ N ç­†è³‡æ–™ï¼Œå›å‚³ N å€‹ç¯€é»åˆ†æçµæœ
    
    Args:
        rows: æ¯ç­†è³‡æ–™çš„æ–‡å­—æè¿° (å·²æˆªæ–·)
        existing_node_names: ç¾æœ‰ç¯€é»åç¨±åˆ—è¡¨
        fast_mode: True = ä½¿ç”¨ç²¾ç°¡ promptï¼ˆå¤§é‡æ¨¡å¼ï¼‰
    
    Returns:
        List[Dict] â€” é•·åº¦èˆ‡ rows ç›¸åŒï¼›å¤±æ•—æ™‚å¡«å…¥é è¨­ç¯€é»
    """
    from backend.core.config import get_current_api_keys, settings
    import httpx

    api_keys = get_current_api_keys()
    dify_api_key = api_keys.get('DIFY_API_KEY', '')
    dify_api_url = api_keys.get('DIFY_API_URL', settings.DIFY_API_URL)

    if not dify_api_key:
        logger.warning("Dify API Key æœªé…ç½®ï¼Œä½¿ç”¨é è¨­å›æ‡‰")
        return [dict(_NO_KEY_NODE) for _ in rows]

    # æ ¹æ“šæ¨¡å¼é¸æ“‡ prompt
    if fast_mode:
        prompt = build_batch_prompt_fast(rows)
    else:
        prompt = build_batch_prompt(rows, existing_node_names)

    try:
        async with httpx.AsyncClient(timeout=LLM_TIMEOUT) as client:
            resp = await client.post(
                f"{dify_api_url}/chat-messages",
                headers={
                    "Authorization": f"Bearer {dify_api_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "query": prompt,
                    "user": "graph-import-system",
                    "inputs": {},
                    "response_mode": "blocking",
                },
            )
            resp.raise_for_status()
            answer = resp.json().get("answer", "")

        if not answer:
            raise ValueError("Dify å›æ‡‰ç‚ºç©º")

        logger.info(f"Dify LLM å›æ‡‰ï¼ˆå‰ 300 å­—ï¼‰: {answer[:300]}")
        results = parse_llm_response(answer)
        
        # çµ±è¨ˆè§£æå¾Œçš„æœ‰æ•ˆ label æ•¸
        valid_labels = sum(1 for r in results if r.get("label") not in ("æœªæä¾›", None, ""))
        logger.info(f"ğŸ“Š æ‰¹æ¬¡è§£æçµæœ: {len(results)} å€‹ç¯€é», {valid_labels} å€‹æœ‰æ•ˆ label")

        # è‹¥ LLM å›å‚³æ•¸é‡ä¸è¶³ï¼Œè£œé½Šé è¨­
        while len(results) < len(rows):
            results.append(dict(_DEFAULT_NODE))

        return results[:len(rows)]

    except httpx.HTTPStatusError as e:
        logger.error(f"Dify API HTTP éŒ¯èª¤ {e.response.status_code}: {e.response.text[:300]}")
    except httpx.TimeoutException:
        logger.error(f"Dify API è«‹æ±‚è¶…æ™‚ ({LLM_TIMEOUT}s)")
    except Exception as e:
        logger.error(f"LLM æ‰¹æ¬¡åˆ†æå¤±æ•—: {e}")

    return [dict(_DEFAULT_NODE) for _ in rows]


async def call_llm_batch_with_retry(
    rows: List[str],
    existing_node_names: Optional[List[str]] = None,
    max_retries: int = MAX_RETRIES,
    fast_mode: bool = False,
) -> List[Dict[str, Any]]:
    """
    å¸¶æŒ‡æ•¸é€€é¿é‡è©¦çš„æ‰¹æ¬¡ LLM å‘¼å«
    
    - å¤±æ•—æ™‚æœ€å¤šé‡è©¦ max_retries æ¬¡
    - æ¯æ¬¡é‡è©¦å»¶é²: base * 2^attempt + random(0,1)
    - è‹¥çµæœå…¨éƒ¨ç‚ºã€ŒLLM åˆ†æå¤±æ•—ã€é è¨­ç¯€é»ä¹Ÿè¦–ç‚ºå¤±æ•—è§¸ç™¼é‡è©¦
    """
    for attempt in range(max_retries):
        try:
            result = await call_llm_batch(rows, existing_node_names, fast_mode=fast_mode)
            
            # æª¢æŸ¥æ˜¯å¦å…¨éƒ¨å›å‚³é è¨­å¤±æ•—ç¯€é»
            all_failed = all(
                r.get("label") == _DEFAULT_NODE["label"] for r in result
            )
            if all_failed and len(rows) > 0:
                raise ValueError("å…¨éƒ¨å›å‚³é è¨­å¤±æ•—ç¯€é»ï¼Œè¦–ç‚ºå¤±æ•—")
            
            return result
            
        except Exception as e:
            if attempt < max_retries - 1:
                delay = RETRY_BASE_DELAY * (2 ** attempt) + random.uniform(0, 1)
                logger.warning(
                    f"âš ï¸ æ‰¹æ¬¡ LLM å¤±æ•— (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)ï¼Œ"
                    f"{delay:.1f}s å¾Œé‡è©¦: {e}"
                )
                await asyncio.sleep(delay)
            else:
                logger.error(
                    f"âŒ æ‰¹æ¬¡ LLM é‡è©¦ {max_retries} æ¬¡ä»å¤±æ•—: {e}"
                )
                return [dict(_DEFAULT_NODE) for _ in rows]
    
    # ç†è«–ä¸Šä¸æœƒåˆ°é€™è£¡
    return [dict(_DEFAULT_NODE) for _ in rows]


async def call_llm_analysis(prompt: str) -> Dict[str, Any]:
    """å‘ä¸‹ç›¸å®¹ï¼šå–®ç­† LLM åˆ†æï¼ˆå…§éƒ¨èª¿ç”¨ call_llm_batch_with_retryï¼‰"""
    results = await call_llm_batch_with_retry([prompt])
    return results[0]


# ===== Checkpoint ç®¡ç† =====

def _save_checkpoint(task_id: str, completed_batches: set, partial_nodes: List[Dict]):
    """å„²å­˜ä»»å‹™ checkpoint"""
    checkpoint_file = CHECKPOINT_DIR / f"{task_id}.json"
    try:
        checkpoint_file.write_text(json.dumps({
            "task_id": task_id,
            "completed_batches": sorted(completed_batches),
            "partial_nodes_count": len(partial_nodes),
            "last_update": datetime.now().isoformat(),
        }, ensure_ascii=False), encoding="utf-8")
    except Exception as e:
        logger.warning(f"Checkpoint å„²å­˜å¤±æ•—: {e}")


def _load_checkpoint(task_id: str) -> set:
    """è¼‰å…¥å·²å®Œæˆçš„æ‰¹æ¬¡ç´¢å¼•"""
    checkpoint_file = CHECKPOINT_DIR / f"{task_id}.json"
    if checkpoint_file.exists():
        try:
            data = json.loads(checkpoint_file.read_text(encoding="utf-8"))
            return set(data.get("completed_batches", []))
        except Exception as e:
            logger.warning(f"Checkpoint è®€å–å¤±æ•—: {e}")
    return set()


def _cleanup_checkpoint(task_id: str):
    """ä»»å‹™å®Œæˆå¾Œæ¸…ç† checkpoint"""
    checkpoint_file = CHECKPOINT_DIR / f"{task_id}.json"
    try:
        if checkpoint_file.exists():
            checkpoint_file.unlink()
    except Exception as e:
        logger.warning(f"Checkpoint æ¸…ç†å¤±æ•—: {e}")


# ===== èƒŒæ™¯åŒ¯å…¥ä»»å‹™ =====

async def _run_import(
    task_id: str,
    row_texts: List[str],
    row_names: List[str],
    df: pd.DataFrame,
    existing_names: Optional[List[str]] = None,
    graph_id: Optional[str] = None,
    ragflow_dataset_id: Optional[str] = None,
    kuzu_manager=None,
):
    """
    èƒŒæ™¯åŸ·è¡Œ Excel åŒ¯å…¥ â€” åˆ†æ‰¹å‘¼å« LLM + é€æ‰¹æ›´æ–°é€²åº¦
    
    ç­–ç•¥:
    - ç”¨ Semaphore æ§åˆ¶æœ€å¤§ä½µç™¼æ•¸
    - æ¯æ‰¹å®Œæˆå¾Œæ›´æ–° _import_tasks é€²åº¦
    - æ¯æ‰¹å®Œæˆå¾Œå¯«å…¥ checkpoint
    - æ‰¹æ¬¡é–“åŠ å…¥å»¶é²é˜²æ­¢ rate limit
    """
    task = _import_tasks[task_id]
    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)
    
    try:
        total_rows = len(row_texts)
        
        # ---- è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å° ----
        batch_size = _compute_adaptive_batch_size(row_texts)
        
        # ---- å¤§é‡æ¨¡å¼åˆ¤å®š ----
        fast_mode = total_rows > FAST_MODE_THRESHOLD
        mode_label = "âš¡ Fast" if fast_mode else "ğŸ“ Full"
        
        # ---- æ–‡å­—æˆªæ–· (é€ LLM çš„ç‰ˆæœ¬) ----
        truncated_texts = [_truncate_text(t, MAX_TEXT_LEN) for t in row_texts]
        
        # ---- åˆ†æ‰¹ ----
        batches: List[List[int]] = []
        for i in range(0, total_rows, batch_size):
            batches.append(list(range(i, min(i + batch_size, total_rows))))
        
        total_batches = len(batches)
        logger.info(
            f"ğŸ“¦ ä»»å‹™ {task_id[:8]}... {mode_label} æ¨¡å¼: {total_rows} è¡Œ â†’ "
            f"{total_batches} æ‰¹ (batch_size={batch_size}, concurrency={MAX_CONCURRENCY})"
        )
        
        # ---- æ›´æ–° task ä¸­çš„ç­–ç•¥è³‡è¨Š ----
        task["batch_size"] = batch_size
        task["total_batches"] = total_batches
        task["completed_batches"] = 0
        task["fast_mode"] = fast_mode
        task["eta_seconds"] = None
        task["rows_per_sec"] = 0
        
        # ---- è¼‰å…¥ checkpoint (æ–·é»çºŒå‚³) ----
        completed_batches = _load_checkpoint(task_id)
        if completed_batches:
            logger.info(f"ğŸ”„ å¾ checkpoint æ¢å¾©: å·²å®Œæˆ {len(completed_batches)} æ‰¹")
        
        # ---- å„²å­˜æ¯æ‰¹çš„ LLM çµæœ ----
        llm_results: List[Optional[List[Dict]]] = [None] * total_batches
        
        # ---- ETA è¿½è¹¤ ----
        batch_times: List[float] = []
        task_start_time = time.monotonic()
        
        # ---- å®šç¾©æ‰¹æ¬¡è™•ç†å‡½å¼ ----
        async def process_batch(batch_idx: int, indices: List[int]):
            async with semaphore:
                # è·³éå·²å®Œæˆçš„æ‰¹æ¬¡ (æ–·é»çºŒå‚³)
                if batch_idx in completed_batches:
                    logger.info(f"â­ï¸ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} å·²å®Œæˆï¼Œè·³é")
                    return
                
                # æ‰¹æ¬¡é–“å»¶é² (rate limit ä¿è­·)
                if batch_idx > 0:
                    await asyncio.sleep(BATCH_DELAY)
                
                texts = [truncated_texts[i] for i in indices]
                
                batch_start = time.monotonic()
                result = await call_llm_batch_with_retry(
                    texts, existing_names, fast_mode=fast_mode
                )
                batch_elapsed = time.monotonic() - batch_start
                batch_times.append(batch_elapsed)
                
                llm_results[batch_idx] = result
                
                # æ›´æ–°é€²åº¦
                completed_batches.add(batch_idx)
                completed_count = sum(
                    len(batches[bi]) for bi in completed_batches
                )
                task["completed"] = completed_count
                task["progress_pct"] = round(
                    completed_count / task["total"] * 100, 1
                )
                task["completed_batches"] = len(completed_batches)
                
                # ETA è¨ˆç®—
                if batch_times:
                    avg_batch_time = sum(batch_times) / len(batch_times)
                    remaining_batches = total_batches - len(completed_batches)
                    # è€ƒæ…®ä½µç™¼: æ¯è¼ªè·‘ MAX_CONCURRENCY å€‹æ‰¹æ¬¡
                    remaining_rounds = max(1, remaining_batches / MAX_CONCURRENCY)
                    eta = avg_batch_time * remaining_rounds
                    task["eta_seconds"] = round(eta, 1)
                    
                    elapsed = time.monotonic() - task_start_time
                    task["rows_per_sec"] = round(
                        completed_count / max(elapsed, 0.1), 1
                    )
                
                # å„²å­˜ checkpoint
                _save_checkpoint(task_id, completed_batches, [])
                
                logger.info(
                    f"âœ… æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} å®Œæˆ "
                    f"({batch_elapsed:.1f}s, é€²åº¦: {task['progress_pct']}%, "
                    f"ETA: {task.get('eta_seconds', '?')}s)"
                )
        
        # ---- ä¸¦è¡ŒåŸ·è¡Œæ‰¹æ¬¡ (Semaphore é™åˆ¶ä½µç™¼) ----
        tasks = [process_batch(bi, idxs) for bi, idxs in enumerate(batches)]
        await asyncio.gather(*tasks)
        
        # ---- çµ„è£ç¯€é» ----
        nodes: List[Dict[str, Any]] = []
        ts = datetime.now().timestamp()
        
        for batch_idx, indices in enumerate(batches):
            batch_results = llm_results[batch_idx] or []
            for local_i, global_i in enumerate(indices):
                if local_i < len(batch_results):
                    llm = batch_results[local_i]
                else:
                    llm = dict(_DEFAULT_NODE)
                
                node = {
                    "id": f"node_{ts}_{global_i}",
                    "name": row_names[global_i],
                    "label": llm.get("label", "æœªå‘½å"),
                    "description": llm.get("description", ""),
                    "type": llm.get("type", "æœªåˆ†é¡"),
                    "group": 1,
                    "size": 20,
                    "keywords": llm.get("keywords", []),
                    "suggested_links": llm.get("suggested_links", []),
                    "raw_data": {
                        k: (None if pd.isna(v) else v)
                        for k, v in df.iloc[global_i].to_dict().items()
                    },
                }
                nodes.append(node)
        
        # ---- å°‡ suggested_links çš„ batch-local index è½‰ç‚ºå…¨åŸŸ node id ----
        for batch_idx, indices in enumerate(batches):
            offset = indices[0]  # æ­¤æ‰¹æ¬¡åœ¨å…¨åŸŸ nodes ä¸­çš„èµ·å§‹ä½ç½®
            for local_i, global_i in enumerate(indices):
                node = nodes[global_i]
                resolved_links = []
                for link in node.get("suggested_links", []):
                    target_idx = link.get("target_index")
                    if target_idx is not None and isinstance(target_idx, int):
                        abs_idx = offset + target_idx
                        if 0 <= abs_idx < len(nodes) and abs_idx != global_i:
                            resolved_links.append({
                                "target_id": nodes[abs_idx]["id"],
                                "target_name": nodes[abs_idx]["name"],
                                "relation": link.get("relation", "complement"),
                                "reason": link.get("reason", ""),
                            })
                node["links"] = resolved_links
                del node["suggested_links"]
        
        # ---- è¨ˆç®—å¤±æ•—æ•¸ ----
        failed_count = sum(
            1 for n in nodes if n.get("label") == _DEFAULT_NODE["label"]
        )
        
        # ---- éšæ®µ: å¯«å…¥ KuzuDB ----
        kuzu_saved = 0
        if kuzu_manager and graph_id:
            logger.info(f"ğŸ“ é–‹å§‹å¯«å…¥ KuzuDB (graph_id={graph_id}, {len(nodes)} å€‹ç¯€é»)...")
            for node in nodes:
                try:
                    props = {
                        "description": node.get("description", ""),
                        "keywords": json.dumps(node.get("keywords", []), ensure_ascii=False),
                        "raw_data": json.dumps(node.get("raw_data", {}), ensure_ascii=False, default=str),
                        "source": "excel_import",
                        "import_task_id": task_id,
                    }
                    success = kuzu_manager.add_entity(
                        entity_id=node["id"],
                        name=node.get("label", node.get("name", "æœªå‘½å")),
                        entity_type=node.get("type", "æœªåˆ†é¡"),
                        properties=props,
                        graph_id=graph_id,
                    )
                    if success:
                        kuzu_saved += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ KuzuDB ç¯€é»å¯«å…¥å¤±æ•— ({node.get('id')}): {e}")
            logger.info(f"âœ… KuzuDB å¯«å…¥å®Œæˆ: {kuzu_saved}/{len(nodes)} å€‹ç¯€é»")
            
            # å¯«å…¥ç¯€é»é–“çš„ suggested_links ç‚º Relation
            links_created = 0
            for node in nodes:
                for link in node.get("links", []):
                    try:
                        target_id = link.get("target_id")
                        if target_id:
                            kuzu_manager.add_relation(
                                from_id=node["id"],
                                to_id=target_id,
                                relation_type=link.get("relation", "complement"),
                                properties={"reason": link.get("reason", "")}
                            )
                            links_created += 1
                    except Exception:
                        pass
            if links_created:
                logger.info(f"ğŸ”— å·²å»ºç«‹ {links_created} æ¢é€£ç·š")
        
        # ---- éšæ®µ: åˆä½µä¸Šå‚³ RAGFlow ----
        ragflow_uploaded = 0
        if ragflow_dataset_id:
            try:
                from backend.rag_client import RAGFlowClient
                from backend.core.config import get_current_api_keys, settings
                import tempfile
                import httpx
                
                api_keys = get_current_api_keys()
                if api_keys.get('RAGFLOW_API_KEY'):
                    rag_client = RAGFlowClient(
                        api_key=api_keys['RAGFLOW_API_KEY'],
                        base_url=api_keys['RAGFLOW_API_URL']
                    )
                    ragflow_api_url = api_keys.get('RAGFLOW_API_URL', 'http://localhost:9380/api/v1')
                    
                    logger.info(f"ğŸ“š åˆä½µ {len(nodes)} å€‹ç¯€é»ç‚ºå–®ä¸€æ–‡ä»¶ä¸Šå‚³ RAGFlow...")
                    task["ragflow_stage"] = "uploading"
                    
                    # ---- æŒ‰é¡å‹åˆ†çµ„ï¼Œæ¯çµ„åˆä½µç‚ºä¸€å€‹ Markdown æ–‡ä»¶ ----
                    from collections import defaultdict
                    type_groups = defaultdict(list)
                    for ri, (row_text, node) in enumerate(zip(row_texts, nodes)):
                        node_type = node.get("type", "æœªåˆ†é¡")
                        type_groups[node_type].append((ri, row_text, node))
                    
                    temp_dir = Path(tempfile.gettempdir()) / f"ragflow_merged_{task_id[:8]}"
                    temp_dir.mkdir(exist_ok=True)
                    uploaded_doc_ids = []
                    
                    try:
                        for type_name, group_items in type_groups.items():
                            # æ§‹å»ºåˆä½µ Markdown å…§å®¹
                            sections = []
                            for ri, row_text, node in group_items:
                                label = node.get("label", f"row_{ri}")
                                desc = node.get("description", "")
                                keywords = ", ".join(node.get("keywords", []))
                                section = f"## {label}\n\n"
                                if desc:
                                    section += f"{desc}\n\n"
                                if keywords:
                                    section += f"**é—œéµè©**: {keywords}\n\n"
                                section += f"**åŸå§‹è³‡æ–™**: {row_text}\n\n---\n"
                                sections.append(section)
                            
                            # æª”åä½¿ç”¨åŸå§‹ Excel å + é¡å‹
                            original_name = task.get("filename", "import").rsplit(".", 1)[0]
                            safe_type = type_name.replace("/", "_").replace("\\", "_")[:20]
                            merged_filename = f"{original_name}_{safe_type}_{len(group_items)}ç­†.md"
                            
                            merged_content = f"# {original_name} â€” {type_name}\n\n"
                            merged_content += f"> å…± {len(group_items)} ç­†è³‡æ–™ï¼Œä¾†æº: Excel æ‰¹æ¬¡åŒ¯å…¥\n\n"
                            merged_content += "\n".join(sections)
                            
                            tmp_file = temp_dir / merged_filename
                            tmp_file.write_text(merged_content, encoding='utf-8')
                            
                            try:
                                upload_result = await rag_client.async_upload_file(
                                    dataset_id=ragflow_dataset_id,
                                    file_path=str(tmp_file)
                                )
                                ragflow_uploaded += len(group_items)
                                
                                # æå– document_id
                                docs = upload_result.get('data', [])
                                if isinstance(docs, list):
                                    for doc in docs:
                                        if isinstance(doc, dict) and doc.get('id'):
                                            uploaded_doc_ids.append(doc['id'])
                                elif isinstance(docs, dict) and docs.get('id'):
                                    uploaded_doc_ids.append(docs['id'])
                                
                                logger.info(
                                    f"ğŸ“„ å·²ä¸Šå‚³: {merged_filename} "
                                    f"({len(group_items)} ç­†, {len(merged_content)} å­—)"
                                )
                            except Exception as e:
                                logger.warning(f"âš ï¸ RAGFlow åˆä½µæ–‡ä»¶ä¸Šå‚³å¤±æ•— ({type_name}): {e}")
                        
                        # è§¸ç™¼æ‰€æœ‰å·²ä¸Šå‚³æ–‡ä»¶çš„è§£æ
                        if uploaded_doc_ids:
                            try:
                                async with httpx.AsyncClient(timeout=300) as parse_client:
                                    await parse_client.post(
                                        f"{ragflow_api_url}/datasets/{ragflow_dataset_id}/chunks",
                                        headers={
                                            "Authorization": f"Bearer {api_keys['RAGFLOW_API_KEY']}",
                                            "Content-Type": "application/json"
                                        },
                                        json={"document_ids": uploaded_doc_ids}
                                    )
                                logger.info(
                                    f"ğŸ”„ å·²è§¸ç™¼ {len(uploaded_doc_ids)} å€‹åˆä½µæ–‡ä»¶çš„è§£æ "
                                    f"(åŸ {len(nodes)} è¡Œ â†’ {len(uploaded_doc_ids)} å€‹æ–‡ä»¶)"
                                )
                            except Exception as parse_err:
                                logger.warning(f"âš ï¸ è§¸ç™¼è§£æå¤±æ•—: {parse_err}")
                        
                        logger.info(
                            f"âœ… RAGFlow åˆä½µä¸Šå‚³å®Œæˆ: {len(nodes)} è¡Œ â†’ "
                            f"{len(uploaded_doc_ids)} å€‹æ–‡ä»¶ (æŒ‰é¡å‹åˆ†çµ„)"
                        )
                    finally:
                        # æ¸…ç†è‡¨æ™‚ç›®éŒ„
                        import shutil
                        shutil.rmtree(temp_dir, ignore_errors=True)
                else:
                    logger.warning("âš ï¸ RAGFlow API Key æœªé…ç½®ï¼Œè·³é RAGFlow ä¸Šå‚³")
            except Exception as e:
                logger.error(f"âŒ RAGFlow é€è¡Œä¸Šå‚³å¤±æ•—: {e}")
        
        # ---- æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå®Œæˆ ----
        total_elapsed = time.monotonic() - task_start_time
        task.update({
            "status": "done",
            "completed": len(row_texts),
            "failed": failed_count,
            "progress_pct": 100.0,
            "nodes": nodes,
            "kuzu_saved": kuzu_saved,
            "ragflow_uploaded": ragflow_uploaded,
            "finished_at": datetime.now().isoformat(),
            "eta_seconds": 0,
            "elapsed_seconds": round(total_elapsed, 1),
            "rows_per_sec": round(len(row_texts) / max(total_elapsed, 0.1), 1),
        })
        
        # æ¸…ç† checkpoint
        _cleanup_checkpoint(task_id)
        
        logger.info(
            f"ğŸ‰ ä»»å‹™ {task_id[:8]}... å®Œæˆ: "
            f"{len(nodes)} å€‹ç¯€é», {kuzu_saved} å¯«å…¥ KuzuDB, "
            f"{ragflow_uploaded} ä¸Šå‚³ RAGFlow, {failed_count} å€‹å¤±æ•—"
        )
        
    except Exception as e:
        logger.error(f"âŒ ä»»å‹™ {task_id[:8]}... å¤±æ•—: {e}", exc_info=True)
        task.update({
            "status": "error",
            "error": str(e),
            "finished_at": datetime.now().isoformat(),
        })


# ===== API ç«¯é» =====
@router.post("/import/excel")
async def import_excel(
    request: Request,
    file: UploadFile = File(...),
    graph_id: str = Form(None),
    ragflow_dataset_id: str = Form(None),
):
    """
    å°å…¥ Excel/CSV æª”æ¡ˆä¸¦ä½¿ç”¨ LLM æ™ºèƒ½è§£æï¼ˆv5.0 å®Œæ•´æ•´åˆç‰ˆï¼‰
    
    åŠŸèƒ½:
    - ç«‹å³å›å‚³ task_idï¼Œä¸å†åŒæ­¥ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
    - ç”¨ GET /import/status/{task_id} æŸ¥è©¢é€²åº¦
    - æ”¯æ´ 3000+ ç­†è³‡æ–™ç©©å®šè™•ç†
    - è‡ªå‹•å¯«å…¥ KuzuDB åœ–è­œç¯€é»
    - é€è¡Œä¸Šå‚³ RAGFlow çŸ¥è­˜åº« (å¯é¸)
    """
    try:
        # é©—è­‰æª”æ¡ˆé¡å‹
        if not file.filename:
            raise HTTPException(status_code=400, detail="æª”æ¡ˆåç¨±ç„¡æ•ˆ")
        
        filename = file.filename.lower()
        if not (filename.endswith('.xlsx') or filename.endswith('.csv')):
            raise HTTPException(
                status_code=400,
                detail="ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼Œè«‹ä¸Šå‚³ .xlsx æˆ– .csv æª”æ¡ˆ"
            )
        
        # è®€å–ä¸¦è§£ææª”æ¡ˆ
        contents = await file.read()
        if filename.endswith('.xlsx'):
            df = pd.read_excel(io.BytesIO(contents))
        else:
            df = pd.read_csv(io.BytesIO(contents))
        
        logger.info(
            f"æˆåŠŸè®€å–æª”æ¡ˆ: {file.filename}, "
            f"è¡Œæ•¸: {len(df)}, æ¬„ä½: {list(df.columns)}, "
            f"graph_id={graph_id}, ragflow_dataset_id={ragflow_dataset_id}"
        )
        
        if df.empty:
            raise HTTPException(status_code=400, detail="æª”æ¡ˆå…§å®¹ç‚ºç©º")
        
        # ---- å–å¾— KuzuDB Manager ----
        kuzu_manager = None
        if graph_id and hasattr(request.app.state, 'kuzu_manager'):
            kuzu_manager = request.app.state.kuzu_manager
        
        # ---- æº–å‚™æ¯è¡Œçš„æ–‡å­—æè¿° ----
        row_texts: List[str] = []
        row_names: List[str] = []
        first_col = str(df.columns[0])
        
        for row_idx, (idx, row) in enumerate(df.iterrows()):
            raw = " | ".join(
                f"{col}: {row[col]}" for col in df.columns if pd.notna(row[col])
            )
            row_texts.append(raw)
            name = (
                str(row[first_col])
                if first_col in row and pd.notna(row[first_col])
                else f"ç¯€é» {row_idx + 1}"
            )
            row_names.append(name)
        
        # ---- è¨ˆç®—é ä¼°æ‰¹æ¬¡å¤§å° (å…ˆç®—å†å•Ÿå‹•èƒŒæ™¯) ----
        est_batch_size = _compute_adaptive_batch_size(row_texts)
        est_batches = (len(row_texts) + est_batch_size - 1) // est_batch_size
        fast_mode = len(row_texts) > FAST_MODE_THRESHOLD

        # ---- å»ºç«‹èƒŒæ™¯ä»»å‹™ ----
        task_id = str(uuid.uuid4())
        _import_tasks[task_id] = {
            "status": "running",
            "total": len(row_texts),
            "completed": 0,
            "failed": 0,
            "progress_pct": 0.0,
            "started_at": datetime.now().isoformat(),
            "finished_at": None,
            "filename": file.filename,
            "graph_id": graph_id,
            "nodes": None,
            "error": None,
            # v5.0 æ–°æ¬„ä½
            "batch_size": est_batch_size,
            "total_batches": est_batches,
            "completed_batches": 0,
            "fast_mode": fast_mode,
            "eta_seconds": None,
            "rows_per_sec": 0,
            "elapsed_seconds": None,
        }
        
        # å•Ÿå‹•èƒŒæ™¯ä»»å‹™ (å« KuzuDB + RAGFlow æ•´åˆ)
        asyncio.create_task(_run_import(
            task_id, row_texts, row_names, df,
            graph_id=graph_id,
            ragflow_dataset_id=ragflow_dataset_id,
            kuzu_manager=kuzu_manager,
        ))
        
        mode_desc = "âš¡å¿«é€Ÿæ¨¡å¼" if fast_mode else "ğŸ“å®Œæ•´æ¨¡å¼"
        logger.info(
            f"ğŸ“¤ åŒ¯å…¥ä»»å‹™å·²å•Ÿå‹•: task_id={task_id[:8]}..., "
            f"total={len(row_texts)}, batch_size={est_batch_size}, "
            f"batches={est_batches}, mode={mode_desc}"
        )
        
        return {
            "task_id": task_id,
            "total": len(row_texts),
            "graph_id": graph_id,
            "batch_size": est_batch_size,
            "total_batches": est_batches,
            "fast_mode": fast_mode,
            "message": f"åŒ¯å…¥ä»»å‹™å·²å•Ÿå‹• ({mode_desc})ï¼Œå…± {len(row_texts)} ç­† â†’ {est_batches} æ‰¹",
        }
        
    except pd.errors.EmptyDataError:
        raise HTTPException(status_code=400, detail="æª”æ¡ˆå…§å®¹ç‚ºç©ºæˆ–æ ¼å¼éŒ¯èª¤")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å°å…¥å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å°å…¥å¤±æ•—: {str(e)}")


@router.get("/import/status/{task_id}")
async def get_import_status(task_id: str):
    """
    æŸ¥è©¢åŒ¯å…¥ä»»å‹™å³æ™‚ç‹€æ…‹
    
    å›å‚³:
    - status: running | done | error
    - total: ç¸½è³‡æ–™ç­†æ•¸
    - completed: å·²å®Œæˆç­†æ•¸
    - failed: å¤±æ•—ç­†æ•¸
    - progress_pct: å®Œæˆç™¾åˆ†æ¯” (0-100)
    - eta_seconds: é ä¼°å‰©é¤˜ç§’æ•¸
    - rows_per_sec: è™•ç†ååé‡
    - batch_size: æ¯æ‰¹å¤§å°
    - total_batches / completed_batches: æ‰¹æ¬¡é€²åº¦
    - fast_mode: æ˜¯å¦ä½¿ç”¨ç²¾ç°¡æ¨¡å¼
    - nodes: å®Œæ•´ç¯€é»çµæœ (åƒ…åœ¨ status=done æ™‚å›å‚³)
    - error: éŒ¯èª¤è¨Šæ¯ (åƒ…åœ¨ status=error æ™‚å›å‚³)
    """
    # æ¯æ¬¡æŸ¥è©¢æ™‚é †ä¾¿æ¸…ç†éæœŸä»»å‹™
    _cleanup_expired_tasks()
    
    task = _import_tasks.get(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–å·²éæœŸ")
    
    # æ§‹å»ºå›æ‡‰ï¼ˆrunning æ™‚ä¸å›å‚³å®Œæ•´ nodes ä»¥ç¯€çœé »å¯¬ï¼‰
    response = {
        "task_id": task_id,
        "status": task["status"],
        "total": task["total"],
        "completed": task["completed"],
        "failed": task["failed"],
        "progress_pct": task["progress_pct"],
        "filename": task.get("filename", ""),
        "graph_id": task.get("graph_id", ""),
        "kuzu_saved": task.get("kuzu_saved", 0),
        "ragflow_uploaded": task.get("ragflow_uploaded", 0),
        "started_at": task.get("started_at"),
        "finished_at": task.get("finished_at"),
        # v5.0 æ–°æ¬„ä½
        "eta_seconds": task.get("eta_seconds"),
        "rows_per_sec": task.get("rows_per_sec", 0),
        "batch_size": task.get("batch_size", 0),
        "total_batches": task.get("total_batches", 0),
        "completed_batches": task.get("completed_batches", 0),
        "fast_mode": task.get("fast_mode", False),
        "elapsed_seconds": task.get("elapsed_seconds"),
    }
    
    if task["status"] == "done" and task.get("nodes"):
        response["nodes"] = task["nodes"]
    
    if task["status"] == "error":
        response["error"] = task.get("error", "æœªçŸ¥éŒ¯èª¤")
    
    return response


@router.get("/import/template")
async def download_template():
    """
    ä¸‹è¼‰ Excel å°å…¥æ¨¡æ¿
    """
    # TODO: å¯¦ç¾æ¨¡æ¿ä¸‹è¼‰åŠŸèƒ½
    return {
        "message": "æ¨¡æ¿ä¸‹è¼‰åŠŸèƒ½é–‹ç™¼ä¸­",
        "suggested_columns": [
            "æ¨™é¡Œ",
            "å…§å®¹",
            "é¡å‹",
            "æ¨™ç±¤",
            "ä¾†æº"
        ]
    }


@router.get("/import/tasks")
async def list_import_tasks():
    """
    åˆ—å‡ºæ‰€æœ‰åŒ¯å…¥ä»»å‹™ï¼ˆç”¨æ–¼ç®¡ç†/é™¤éŒ¯ï¼‰
    """
    tasks_summary = []
    for tid, task in _import_tasks.items():
        tasks_summary.append({
            "task_id": tid,
            "status": task["status"],
            "total": task["total"],
            "completed": task["completed"],
            "progress_pct": task["progress_pct"],
            "filename": task.get("filename", ""),
            "started_at": task.get("started_at"),
            "finished_at": task.get("finished_at"),
        })
    
    return {
        "tasks": tasks_summary,
        "count": len(tasks_summary),
    }
