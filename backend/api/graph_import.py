"""
åœ–è­œå°å…¥ API â€” è–„è·¯ç”±å±¤

v5.0 â€” è·¯ç”±å®šç¾© + è«‹æ±‚è™•ç†
æ ¸å¿ƒé‚è¼¯å·²æ‹†åˆ†è‡³:
  - import_engine.py  (åŒ¯å…¥å¼•æ“ã€LLM å‘¼å«ã€Checkpointã€ä»»å‹™ç®¡ç·š)
  - import_prompts.py (Prompt æ¨¡æ¿)
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Request
from typing import List
import pandas as pd
import io
import logging
import uuid
import asyncio
from datetime import datetime

# ---- å¾å¼•æ“æ¨¡çµ„åŒ¯å…¥ ----
from .import_engine import (
    _import_tasks,
    _cleanup_expired_tasks,
    _compute_adaptive_batch_size,
    _run_import,
    FAST_MODE_THRESHOLD,
)

# ---- å‘ä¸‹ç›¸å®¹ï¼šè®“å¤–éƒ¨ import é€™äº›ç¬¦è™Ÿæ™‚ä»å¯ç”¨ ----
from .import_engine import (       # noqa: F401
    parse_llm_response,
    call_llm_analysis,
    call_llm_batch,
    call_llm_batch_with_retry,
)
from .import_prompts import (      # noqa: F401
    build_batch_prompt,
    build_batch_prompt_fast,
    build_node_analysis_prompt,
    SYSTEM_ROLE,
    NODE_SCHEMA,
)

logger = logging.getLogger(__name__)
router = APIRouter()


# ===== API ç«¯é» =====

@router.post("/import/excel")
async def import_excel(
    request: Request,
    file: UploadFile = File(...),
    graph_id: str = Form(None),
    ragflow_dataset_id: str = Form(None),
):
    """
    å°å…¥ Excel/CSV æª”æ¡ˆä¸¦ä½¿ç”¨ LLM æ™ºèƒ½è§£æï¼ˆv5.0 å®Œæ•´æ•´åˆç‰ˆï¼‰

    åŠŸèƒ½:
    - ç«‹å³å›å‚³ task_idï¼Œä¸å†åŒæ­¥ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
    - ç”¨ GET /import/status/{task_id} æŸ¥è©¢é€²åº¦
    - æ”¯æ´ 3000+ ç­†è³‡æ–™ç©©å®šè™•ç†
    - è‡ªå‹•å¯«å…¥ KuzuDB åœ–è­œç¯€é»
    - é€è¡Œä¸Šå‚³ RAGFlow çŸ¥è­˜åº« (å¯é¸)
    """
    try:
        if not file.filename:
            raise HTTPException(status_code=400, detail="æª”æ¡ˆåç¨±ç„¡æ•ˆ")

        filename = file.filename.lower()
        if not (filename.endswith('.xlsx') or filename.endswith('.csv')):
            raise HTTPException(
                status_code=400,
                detail="ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼Œè«‹ä¸Šå‚³ .xlsx æˆ– .csv æª”æ¡ˆ"
            )

        contents = await file.read()
        if filename.endswith('.xlsx'):
            df = pd.read_excel(io.BytesIO(contents))
        else:
            df = pd.read_csv(io.BytesIO(contents))

        logger.info(
            f"æˆåŠŸè®€å–æª”æ¡ˆ: {file.filename}, "
            f"è¡Œæ•¸: {len(df)}, æ¬„ä½: {list(df.columns)}, "
            f"graph_id={graph_id}, ragflow_dataset_id={ragflow_dataset_id}"
        )

        if df.empty:
            raise HTTPException(status_code=400, detail="æª”æ¡ˆå…§å®¹ç‚ºç©º")

        kuzu_manager = None
        if graph_id and hasattr(request.app.state, 'kuzu_manager'):
            kuzu_manager = request.app.state.kuzu_manager

        row_texts: List[str] = []
        row_names: List[str] = []
        first_col = str(df.columns[0])

        for row_idx, (idx, row) in enumerate(df.iterrows()):
            raw = " | ".join(
                f"{col}: {row[col]}" for col in df.columns if pd.notna(row[col])
            )
            row_texts.append(raw)
            name = (
                str(row[first_col])
                if first_col in row and pd.notna(row[first_col])
                else f"ç¯€é» {row_idx + 1}"
            )
            row_names.append(name)

        est_batch_size = _compute_adaptive_batch_size(row_texts)
        est_batches = (len(row_texts) + est_batch_size - 1) // est_batch_size
        fast_mode = len(row_texts) > FAST_MODE_THRESHOLD

        task_id = str(uuid.uuid4())
        _import_tasks[task_id] = {
            "status": "running",
            "total": len(row_texts),
            "completed": 0,
            "failed": 0,
            "progress_pct": 0.0,
            "started_at": datetime.now().isoformat(),
            "finished_at": None,
            "filename": file.filename,
            "graph_id": graph_id,
            "error": None,
            "batch_size": est_batch_size,
            "total_batches": est_batches,
            "completed_batches": 0,
            "fast_mode": fast_mode,
            "eta_seconds": None,
            "rows_per_sec": 0,
            "elapsed_seconds": None,
        }

        asyncio.create_task(_run_import(
            task_id, row_texts, row_names, df,
            graph_id=graph_id,
            ragflow_dataset_id=ragflow_dataset_id,
            kuzu_manager=kuzu_manager,
            http_client=getattr(request.app.state, 'http_client', None),
        ))

        mode_desc = "âš¡å¿«é€Ÿæ¨¡å¼" if fast_mode else "ğŸ“å®Œæ•´æ¨¡å¼"
        logger.info(
            f"ğŸ“¤ åŒ¯å…¥ä»»å‹™å·²å•Ÿå‹•: task_id={task_id[:8]}..., "
            f"total={len(row_texts)}, batch_size={est_batch_size}, "
            f"batches={est_batches}, mode={mode_desc}"
        )

        return {
            "task_id": task_id,
            "total": len(row_texts),
            "graph_id": graph_id,
            "batch_size": est_batch_size,
            "total_batches": est_batches,
            "fast_mode": fast_mode,
            "message": f"åŒ¯å…¥ä»»å‹™å·²å•Ÿå‹• ({mode_desc})ï¼Œå…± {len(row_texts)} ç­† â†’ {est_batches} æ‰¹",
        }

    except pd.errors.EmptyDataError:
        raise HTTPException(status_code=400, detail="æª”æ¡ˆå…§å®¹ç‚ºç©ºæˆ–æ ¼å¼éŒ¯èª¤")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å°å…¥å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å°å…¥å¤±æ•—: {str(e)}")


@router.get("/import/status/{task_id}")
async def get_import_status(task_id: str):
    """æŸ¥è©¢åŒ¯å…¥ä»»å‹™å³æ™‚ç‹€æ…‹"""
    _cleanup_expired_tasks()

    task = _import_tasks.get(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨æˆ–å·²éæœŸ")

    response = {
        "task_id": task_id,
        "status": task["status"],
        "total": task["total"],
        "completed": task["completed"],
        "failed": task["failed"],
        "progress_pct": task["progress_pct"],
        "filename": task.get("filename", ""),
        "graph_id": task.get("graph_id", ""),
        "kuzu_saved": task.get("kuzu_saved", 0),
        "ragflow_uploaded": task.get("ragflow_uploaded", 0),
        "started_at": task.get("started_at"),
        "finished_at": task.get("finished_at"),
        "eta_seconds": task.get("eta_seconds"),
        "rows_per_sec": task.get("rows_per_sec", 0),
        "batch_size": task.get("batch_size", 0),
        "total_batches": task.get("total_batches", 0),
        "completed_batches": task.get("completed_batches", 0),
        "fast_mode": task.get("fast_mode", False),
        "elapsed_seconds": task.get("elapsed_seconds"),
        "extracted_count": task.get("extracted_count", 0),
    }

    if task["status"] == "done":
        response["node_count"] = task.get("node_count", 0)

    if task["status"] == "error":
        response["error"] = task.get("error", "æœªçŸ¥éŒ¯èª¤")

    return response


@router.get("/import/template")
async def download_template():
    """ä¸‹è¼‰ Excel å°å…¥æ¨¡æ¿"""
    return {
        "message": "æ¨¡æ¿ä¸‹è¼‰åŠŸèƒ½é–‹ç™¼ä¸­",
        "suggested_columns": ["æ¨™é¡Œ", "å…§å®¹", "é¡å‹", "æ¨™ç±¤", "ä¾†æº"]
    }


@router.get("/import/tasks")
async def list_import_tasks():
    """åˆ—å‡ºæ‰€æœ‰åŒ¯å…¥ä»»å‹™"""
    tasks_summary = []
    for tid, task in _import_tasks.items():
        tasks_summary.append({
            "task_id": tid,
            "status": task["status"],
            "total": task["total"],
            "completed": task["completed"],
            "progress_pct": task["progress_pct"],
            "filename": task.get("filename", ""),
            "started_at": task.get("started_at"),
            "finished_at": task.get("finished_at"),
        })

    return {
        "tasks": tasks_summary,
        "count": len(tasks_summary),
    }
