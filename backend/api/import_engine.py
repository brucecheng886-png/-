"""
åŒ¯å…¥å¼•æ“ â€” Excel/CSV æ™ºèƒ½è§£ææ ¸å¿ƒé‚è¼¯

å¾ graph_import.py æ‹†åˆ†ï¼ŒåŒ…å«ï¼š
- å¯èª¿åƒæ•¸ & å¸¸æ•¸
- Token ä¼°ç®— & è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å°
- æ¬„ä½æ™ºèƒ½æå–ï¼ˆå… LLMï¼‰
- LLM çµæœå¿«å–
- JSON è§£æ & ç¯€é»é©—è­‰
- LLM å‘¼å«ï¼ˆæ‰¹æ¬¡ / é‡è©¦ / å–®ç­†ï¼‰
- Checkpoint æ–·é»çºŒå‚³
- _run_import ä¸»ç®¡ç·š
- ä»»å‹™è¿½è¹¤èˆ‡æ¸…ç†
"""
from typing import List, Dict, Any, Optional
import pandas as pd
import json
import logging
import asyncio
import random
import time
import hashlib
from datetime import datetime
from pathlib import Path

from .import_prompts import (
    build_batch_prompt,
    build_batch_prompt_fast,
)

logger = logging.getLogger(__name__)

# ===== å¯èª¿åƒæ•¸ =====
MAX_CONCURRENCY = 2
LLM_TIMEOUT = 300
MAX_RETRIES = 3
RETRY_BASE_DELAY = 3
BATCH_DELAY = 1.0
MAX_TEXT_LEN = 500
FAST_MODE_THRESHOLD = 100
TARGET_BATCH_TOKENS = 2000
TARGET_BATCH_TOKENS_FAST = 6000

# ===== Checkpoint è·¯å¾‘ =====
CHECKPOINT_DIR = Path(__file__).resolve().parent.parent.parent / "data" / "import_checkpoints"
CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)

# ===== å…¨åŸŸä»»å‹™è¿½è¹¤å™¨ =====
_import_tasks: Dict[str, Dict[str, Any]] = {}
_TASK_EXPIRY_SECONDS = 7200
MAX_RAGFLOW_FILE_BYTES = 200_000


# ------------------------------------------------------------------ #
#  ä»»å‹™æ¸…ç†
# ------------------------------------------------------------------ #

def _cleanup_expired_tasks():
    """æ¸…ç†å·²éæœŸçš„å®Œæˆä»»å‹™ï¼ˆé‡‹æ”¾è¨˜æ†¶é«”ï¼‰"""
    now = datetime.now()
    expired = []
    for tid, task in _import_tasks.items():
        if task.get("status") in ("done", "error"):
            finished_str = task.get("finished_at")
            if finished_str:
                try:
                    finished = datetime.fromisoformat(finished_str)
                    if (now - finished).total_seconds() > _TASK_EXPIRY_SECONDS:
                        expired.append(tid)
                except (ValueError, TypeError):
                    pass
    for tid in expired:
        del _import_tasks[tid]
    if expired:
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç† {len(expired)} å€‹éæœŸä»»å‹™")


# ------------------------------------------------------------------ #
#  Token ä¼°ç®— & æ‰¹æ¬¡å¤§å°
# ------------------------------------------------------------------ #

def _estimate_tokens(text: str) -> int:
    """ç²—ä¼° token æ•¸ï¼šä¸­æ–‡å­— â‰ˆ 1 token / å­—ï¼Œè‹±æ–‡ â‰ˆ 1 token / 4 å­—å…ƒ"""
    if not text:
        return 0
    cn = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    return cn + (len(text) - cn) // 4


def _compute_adaptive_batch_size(row_texts: List[str]) -> int:
    """æ ¹æ“šå¹³å‡æ–‡å­—é•·åº¦è‡ªé©æ‡‰èª¿æ•´ BATCH_SIZE"""
    if not row_texts:
        return 10
    sample = row_texts[:50]
    avg_tokens = sum(_estimate_tokens(t[:MAX_TEXT_LEN]) for t in sample) / len(sample)
    return max(5, min(50, int(TARGET_BATCH_TOKENS / max(avg_tokens, 10))))


def _truncate_text(text: str, max_len: int = MAX_TEXT_LEN) -> str:
    """æˆªæ–·æ–‡å­—è‡³æŒ‡å®šé•·åº¦ï¼ˆé™„çœç•¥è™Ÿï¼‰"""
    if len(text) <= max_len:
        return text
    return text[:max_len] + "..."


# ------------------------------------------------------------------ #
#  æ¬„ä½æ™ºèƒ½æå–ï¼ˆå… LLMï¼‰
# ------------------------------------------------------------------ #

_COLUMN_ALIASES = {
    'label': {'æ¨™é¡Œ', 'åç¨±', 'title', 'name', 'ä¸»é¡Œ', 'subject', 'é …ç›®', 'åå­—', 'å§“å'},
    'type': {'é¡å‹', 'type', 'åˆ†é¡', 'category', 'é¡åˆ¥', 'class', 'ç¨®é¡'},
    'description': {'æè¿°', 'description', 'å…§å®¹', 'content', 'èªªæ˜', 'æ‘˜è¦',
                     'summary', 'å‚™è¨»', 'note', 'notes', 'abstract'},
    'keywords': {'é—œéµè©', 'keywords', 'é—œéµå­—'},
    'tags': {'æ¨™ç±¤', 'tags', 'æ¨™è¨˜', 'tag', 'åˆ†é¡æ¨™ç±¤'},
}


def _try_extract_from_columns(df: pd.DataFrame) -> List[Optional[Dict]]:
    """
    å˜—è©¦å¾ Excel æ¬„ä½åç¨±ç›´æ¥æå–ç¯€é»è³‡æ–™ï¼ˆå… LLMï¼‰ã€‚
    è‡³å°‘éœ€è¦åŒ¹é…åˆ° label æ¬„ä½æ‰å•Ÿç”¨ã€‚
    """
    import re as _re

    col_strip = {col: col.strip().lower() for col in df.columns}
    field_map: Dict[str, str] = {}

    for field, aliases in _COLUMN_ALIASES.items():
        for col, lower in col_strip.items():
            if lower in aliases:
                field_map[field] = col
                break

    if 'label' not in field_map:
        return [None] * len(df)

    logger.info(f"ğŸ“‹ æ¬„ä½æ™ºèƒ½åŒ¹é…: {', '.join(f'{k}â†’{v}' for k, v in field_map.items())}")

    results: List[Optional[Dict]] = []
    for _, row in df.iterrows():
        label_val = row.get(field_map['label'], '')
        if pd.isna(label_val) or not str(label_val).strip():
            results.append(None)
            continue

        node: Dict[str, Any] = {
            'label': str(label_val).strip()[:50],
            'description': '',
            'type': 'æœªåˆ†é¡',
            'keywords': [],
            'tags': [],
            'suggested_links': [],
        }

        if 'type' in field_map:
            t = row.get(field_map['type'], '')
            if pd.notna(t) and str(t).strip():
                node['type'] = str(t).strip()[:20]

        if 'description' in field_map:
            d = row.get(field_map['description'], '')
            if pd.notna(d) and str(d).strip():
                node['description'] = str(d).strip()[:500]

        if 'keywords' in field_map:
            kw = row.get(field_map['keywords'], '')
            if pd.notna(kw) and str(kw).strip():
                node['keywords'] = [
                    k.strip() for k in _re.split(r'[,;ï¼Œï¼›ã€\s]+', str(kw).strip()) if k.strip()
                ][:5]

        if 'tags' in field_map:
            tg = row.get(field_map['tags'], '')
            if pd.notna(tg) and str(tg).strip():
                node['tags'] = [
                    t.strip() for t in _re.split(r'[,;ï¼Œï¼›ã€\s]+', str(tg).strip()) if t.strip()
                ][:5]

        results.append(node)

    return results


# ------------------------------------------------------------------ #
#  LLM çµæœå¿«å–
# ------------------------------------------------------------------ #

_llm_result_cache: Dict[str, Dict] = {}
_LLM_CACHE_MAX = 10000


def _get_cache_key(text: str) -> str:
    return hashlib.md5(text.strip().encode('utf-8')).hexdigest()


def _cache_llm_result(text: str, result: Dict):
    if len(_llm_result_cache) >= _LLM_CACHE_MAX:
        keys = list(_llm_result_cache.keys())
        for k in keys[:len(keys) // 2]:
            del _llm_result_cache[k]
    _llm_result_cache[_get_cache_key(text)] = result


# ------------------------------------------------------------------ #
#  JSON è§£æ & ç¯€é»é©—è­‰
# ------------------------------------------------------------------ #

def _extract_json(text: str):
    """å¾ LLM å›æ‡‰ä¸­æå– JSONï¼ˆæ”¯æ´é™£åˆ—å’Œç‰©ä»¶ï¼‰"""
    import re
    text = text.strip()

    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    md = re.search(r'```(?:json)?\s*([\[\{].*?[\]\}])\s*```', text, re.DOTALL)
    if md:
        try:
            return json.loads(md.group(1))
        except json.JSONDecodeError:
            pass

    for open_ch, close_ch in [('[', ']'), ('{', '}')]:
        start = text.find(open_ch)
        end = text.rfind(close_ch)
        if start != -1 and end > start:
            try:
                return json.loads(text[start:end + 1])
            except json.JSONDecodeError:
                continue

    raise ValueError("ç„¡æ³•å¾ LLM å›æ‡‰ä¸­è§£æ JSON")


def _validate_node(data: Dict[str, Any]) -> Dict[str, Any]:
    """é©—è­‰ä¸¦æ¸…æ´—å–®ä¸€ç¯€é»è³‡æ–™"""
    if 'label' not in data:
        for alt in ['title', 'name', 'æ¨™é¡Œ', 'åç¨±']:
            if alt in data and data[alt]:
                data['label'] = str(data[alt])[:50]
                break

    required = ['label', 'description', 'type']
    for f in required:
        if f not in data or not data[f]:
            data[f] = "æœªæä¾›" if f != 'type' else "æœªåˆ†é¡"

    if len(data.get('description', '')) > 500:
        data['description'] = data['description'][:500] + "..."

    if 'links' in data and 'suggested_links' not in data:
        data['suggested_links'] = data.pop('links')
    data.setdefault('suggested_links', [])
    if len(data['suggested_links']) > 5:
        data['suggested_links'] = data['suggested_links'][:5]

    data.setdefault('keywords', [])

    if 'tag' in data and 'tags' not in data:
        data['tags'] = data.pop('tag')
    if 'æ¨™ç±¤' in data and 'tags' not in data:
        data['tags'] = data.pop('æ¨™ç±¤')
    data.setdefault('tags', [])
    if isinstance(data['tags'], str):
        data['tags'] = [t.strip() for t in data['tags'].replace('ï¼Œ', ',').split(',') if t.strip()]
    if not isinstance(data['tags'], list):
        data['tags'] = []
    data['tags'] = [str(t).strip() for t in data['tags'] if t and str(t).strip()][:5]

    return data


def parse_llm_response(llm_output: str) -> List[Dict[str, Any]]:
    """è§£æ LLM å›æ‡‰ï¼Œçµ±ä¸€å›å‚³ List[Dict]"""
    raw = _extract_json(llm_output)
    items = raw if isinstance(raw, list) else [raw]
    return [_validate_node(item) for item in items]


# ------------------------------------------------------------------ #
#  é è¨­å›æ‡‰
# ------------------------------------------------------------------ #

_DEFAULT_NODE = {
    "label": "LLM åˆ†æå¤±æ•—",
    "description": "è‡ªå‹•åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹æ‰‹å‹•ç·¨è¼¯æ­¤ç¯€é»ã€‚",
    "type": "æœªåˆ†é¡",
    "keywords": [],
    "suggested_links": [],
}

_NO_KEY_NODE = {
    "label": "å¾…é…ç½® LLM",
    "description": "Dify API Key å°šæœªè¨­å®šï¼Œè«‹è‡³ç³»çµ±è¨­å®šé é¢é…ç½®å¾Œé‡æ–°åŒ¯å…¥ã€‚",
    "type": "æœªåˆ†é¡",
    "keywords": [],
    "suggested_links": [],
}


# ------------------------------------------------------------------ #
#  LLM å‘¼å«
# ------------------------------------------------------------------ #

async def call_llm_batch(
    rows: List[str],
    existing_node_names: Optional[List[str]] = None,
    fast_mode: bool = False,
) -> List[Dict[str, Any]]:
    """æ‰¹æ¬¡å‘¼å« Dify LLM"""
    from backend.core.config import get_current_api_keys, settings
    import httpx

    api_keys = get_current_api_keys()
    dify_api_key = api_keys.get('DIFY_API_KEY', '')
    dify_api_url = api_keys.get('DIFY_API_URL', settings.DIFY_API_URL)

    if not dify_api_key:
        logger.warning("Dify API Key æœªé…ç½®ï¼Œä½¿ç”¨é è¨­å›æ‡‰")
        return [dict(_NO_KEY_NODE) for _ in rows]

    if fast_mode:
        prompt = build_batch_prompt_fast(rows)
    else:
        prompt = build_batch_prompt(rows, existing_node_names)

    try:
        async with httpx.AsyncClient(timeout=LLM_TIMEOUT) as client:
            resp = await client.post(
                f"{dify_api_url}/chat-messages",
                headers={
                    "Authorization": f"Bearer {dify_api_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "query": prompt,
                    "user": "graph-import-system",
                    "inputs": {},
                    "response_mode": "blocking",
                },
            )
            resp.raise_for_status()
            answer = resp.json().get("answer", "")

        if not answer:
            raise ValueError("Dify å›æ‡‰ç‚ºç©º")

        logger.info(f"Dify LLM å›æ‡‰ï¼ˆå‰ 300 å­—ï¼‰: {answer[:300]}")
        results = parse_llm_response(answer)

        valid_labels = sum(1 for r in results if r.get("label") not in ("æœªæä¾›", None, ""))
        logger.info(f"ğŸ“Š æ‰¹æ¬¡è§£æçµæœ: {len(results)} å€‹ç¯€é», {valid_labels} å€‹æœ‰æ•ˆ label")

        while len(results) < len(rows):
            results.append(dict(_DEFAULT_NODE))

        return results[:len(rows)]

    except Exception as e:
        if hasattr(e, 'response'):
            logger.error(f"Dify API HTTP éŒ¯èª¤: {e}")
        else:
            logger.error(f"LLM æ‰¹æ¬¡åˆ†æå¤±æ•—: {e}")

    return [dict(_DEFAULT_NODE) for _ in rows]


async def call_llm_batch_with_retry(
    rows: List[str],
    existing_node_names: Optional[List[str]] = None,
    max_retries: int = MAX_RETRIES,
    fast_mode: bool = False,
) -> List[Dict[str, Any]]:
    """å¸¶æŒ‡æ•¸é€€é¿é‡è©¦çš„æ‰¹æ¬¡ LLM å‘¼å«"""
    for attempt in range(max_retries):
        try:
            result = await call_llm_batch(rows, existing_node_names, fast_mode=fast_mode)

            all_failed = all(
                r.get("label") == _DEFAULT_NODE["label"] for r in result
            )
            if all_failed and len(rows) > 0:
                raise ValueError("å…¨éƒ¨å›å‚³é è¨­å¤±æ•—ç¯€é»ï¼Œè¦–ç‚ºå¤±æ•—")

            return result

        except Exception as e:
            if attempt < max_retries - 1:
                delay = RETRY_BASE_DELAY * (2 ** attempt) + random.uniform(0, 1)
                logger.warning(
                    f"âš ï¸ æ‰¹æ¬¡ LLM å¤±æ•— (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)ï¼Œ"
                    f"{delay:.1f}s å¾Œé‡è©¦: {e}"
                )
                await asyncio.sleep(delay)
            else:
                logger.error(f"âŒ æ‰¹æ¬¡ LLM é‡è©¦ {max_retries} æ¬¡ä»å¤±æ•—: {e}")
                return [dict(_DEFAULT_NODE) for _ in rows]

    return [dict(_DEFAULT_NODE) for _ in rows]


async def call_llm_analysis(prompt: str) -> Dict[str, Any]:
    """å‘ä¸‹ç›¸å®¹ï¼šå–®ç­† LLM åˆ†æ"""
    results = await call_llm_batch_with_retry([prompt])
    return results[0]


# ------------------------------------------------------------------ #
#  Checkpoint ç®¡ç†
# ------------------------------------------------------------------ #

def _save_checkpoint(task_id: str, completed_batches: set, partial_nodes: List[Dict]):
    checkpoint_file = CHECKPOINT_DIR / f"{task_id}.json"
    try:
        checkpoint_file.write_text(json.dumps({
            "task_id": task_id,
            "completed_batches": sorted(completed_batches),
            "partial_nodes_count": len(partial_nodes),
            "last_update": datetime.now().isoformat(),
        }, ensure_ascii=False), encoding="utf-8")
    except Exception as e:
        logger.warning(f"Checkpoint å„²å­˜å¤±æ•—: {e}")


def _load_checkpoint(task_id: str) -> set:
    checkpoint_file = CHECKPOINT_DIR / f"{task_id}.json"
    if checkpoint_file.exists():
        try:
            data = json.loads(checkpoint_file.read_text(encoding="utf-8"))
            return set(data.get("completed_batches", []))
        except Exception as e:
            logger.warning(f"Checkpoint è®€å–å¤±æ•—: {e}")
    return set()


def _cleanup_checkpoint(task_id: str):
    checkpoint_file = CHECKPOINT_DIR / f"{task_id}.json"
    try:
        if checkpoint_file.exists():
            checkpoint_file.unlink()
    except Exception as e:
        logger.warning(f"Checkpoint æ¸…ç†å¤±æ•—: {e}")


# ------------------------------------------------------------------ #
#  èƒŒæ™¯åŒ¯å…¥ä¸»ç®¡ç·š
# ------------------------------------------------------------------ #

async def _run_import(
    task_id: str,
    row_texts: List[str],
    row_names: List[str],
    df: pd.DataFrame,
    existing_names: Optional[List[str]] = None,
    graph_id: Optional[str] = None,
    ragflow_dataset_id: Optional[str] = None,
    kuzu_manager=None,
    http_client=None,
):
    """
    èƒŒæ™¯åŸ·è¡Œ Excel åŒ¯å…¥ â€” åˆ†æ‰¹å‘¼å« LLM + é€æ‰¹æ›´æ–°é€²åº¦

    ç­–ç•¥:
    1. æ¬„ä½æ™ºèƒ½æå–: åŒ¹é…å·²çŸ¥æ¬„ä½åå… LLM
    2. LLM çµæœå¿«å–: ç›¸åŒå…§å®¹è·¨æ‰¹æ¬¡å»é‡
    3. è‡ªé©æ‡‰å¤§æ‰¹æ¬¡: Fast mode åŠ å¤§æ¯æ‰¹ç­†æ•¸
    4. Semaphore ä½µç™¼æ§åˆ¶ + æ–·é»çºŒå‚³ + æŒ‡æ•¸é€€é¿é‡è©¦
    """
    task = _import_tasks[task_id]
    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

    try:
        total_rows = len(row_texts)

        # ==== ç­–ç•¥ 1: æ¬„ä½æ™ºèƒ½æå– (å… LLM) ====
        pre_extracted = _try_extract_from_columns(df)
        extracted_count = sum(1 for p in pre_extracted if p is not None)
        llm_indices = [i for i in range(total_rows) if pre_extracted[i] is None]
        llm_row_count = len(llm_indices)

        if extracted_count > 0:
            logger.info(
                f"ğŸ“‹ æ¬„ä½æ™ºèƒ½æå–: {extracted_count}/{total_rows} è¡Œå… LLMï¼Œ"
                f"åƒ… {llm_row_count} è¡Œéœ€è¦ LLM åˆ†æ"
            )
            task["extracted_count"] = extracted_count

        truncated_texts = [_truncate_text(t, MAX_TEXT_LEN) for t in row_texts]

        fast_mode = total_rows > FAST_MODE_THRESHOLD
        mode_label = "âš¡ Fast" if fast_mode else "ğŸ“ Full"

        # ==== ç­–ç•¥ 3: è‡ªé©æ‡‰æ‰¹æ¬¡å¤§å° ====
        if llm_row_count > 0:
            llm_sample = [row_texts[i] for i in llm_indices[:50]]
            avg_tokens = sum(_estimate_tokens(t[:MAX_TEXT_LEN]) for t in llm_sample) / len(llm_sample)
            target_tokens = TARGET_BATCH_TOKENS_FAST if fast_mode else TARGET_BATCH_TOKENS
            batch_size = max(5, min(50, int(target_tokens / max(avg_tokens, 10))))
        else:
            batch_size = 10

        batches: List[List[int]] = []
        for i in range(0, llm_row_count, batch_size):
            batches.append(llm_indices[i:min(i + batch_size, llm_row_count)])

        total_batches = len(batches)
        logger.info(
            f"ğŸ“¦ ä»»å‹™ {task_id[:8]}... {mode_label} æ¨¡å¼: {total_rows} è¡Œ "
            f"({extracted_count} å… LLM + {llm_row_count} è¡Œ LLM) â†’ "
            f"{total_batches} æ‰¹ (batch_size={batch_size}, concurrency={MAX_CONCURRENCY})"
        )

        task["batch_size"] = batch_size
        task["total_batches"] = total_batches
        task["completed_batches"] = 0
        task["fast_mode"] = fast_mode
        task["eta_seconds"] = None
        task["rows_per_sec"] = 0

        initial_completed = extracted_count
        task["completed"] = initial_completed
        if total_rows > 0:
            task["progress_pct"] = round(initial_completed / total_rows * 100, 1)

        completed_batches = _load_checkpoint(task_id)
        if completed_batches:
            logger.info(f"ğŸ”„ å¾ checkpoint æ¢å¾©: å·²å®Œæˆ {len(completed_batches)} æ‰¹")

        llm_results: List[Optional[List[Dict]]] = [None] * total_batches

        batch_times: List[float] = []
        task_start_time = time.monotonic()

        async def process_batch(batch_idx: int, indices: List[int]):
            async with semaphore:
                if batch_idx in completed_batches:
                    logger.info(f"â­ï¸ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} å·²å®Œæˆï¼Œè·³é")
                    return

                if batch_idx > 0:
                    await asyncio.sleep(BATCH_DELAY)

                texts = [truncated_texts[i] for i in indices]
                batch_start = time.monotonic()

                if fast_mode:
                    cached_results: Dict[int, Dict] = {}
                    uncached_pairs: List[tuple] = []

                    for local_i, text in enumerate(texts):
                        cache_key = _get_cache_key(text)
                        cached = _llm_result_cache.get(cache_key)
                        if cached is not None:
                            cached_results[local_i] = cached
                        else:
                            uncached_pairs.append((local_i, text))

                    cache_hits = len(cached_results)

                    if uncached_pairs:
                        uncached_texts = [t for _, t in uncached_pairs]
                        llm_response = await call_llm_batch_with_retry(
                            uncached_texts, existing_names, fast_mode=True
                        )
                        for ui, (local_i, text) in enumerate(uncached_pairs):
                            if ui < len(llm_response):
                                _cache_llm_result(text, llm_response[ui])
                    else:
                        llm_response = []

                    result: List[Dict] = [dict(_DEFAULT_NODE)] * len(texts)
                    for local_i, cached in cached_results.items():
                        result[local_i] = cached
                    for ui, (local_i, _) in enumerate(uncached_pairs):
                        if ui < len(llm_response):
                            result[local_i] = llm_response[ui]
                    for i in range(len(result)):
                        if result[i] is _DEFAULT_NODE or result[i].get("label") is None:
                            result[i] = dict(_DEFAULT_NODE)

                    llm_results[batch_idx] = result
                else:
                    result = await call_llm_batch_with_retry(
                        texts, existing_names, fast_mode=False
                    )
                    llm_results[batch_idx] = result
                    cache_hits = 0

                batch_elapsed = time.monotonic() - batch_start
                batch_times.append(batch_elapsed)

                completed_batches.add(batch_idx)
                completed_count = initial_completed + sum(
                    len(batches[bi]) for bi in completed_batches
                )
                task["completed"] = completed_count
                task["progress_pct"] = round(
                    completed_count / task["total"] * 100, 1
                )
                task["completed_batches"] = len(completed_batches)

                if batch_times:
                    avg_batch_time = sum(batch_times) / len(batch_times)
                    remaining_batches = total_batches - len(completed_batches)
                    remaining_rounds = max(1, remaining_batches / MAX_CONCURRENCY)
                    eta = avg_batch_time * remaining_rounds
                    task["eta_seconds"] = round(eta, 1)

                    elapsed = time.monotonic() - task_start_time
                    task["rows_per_sec"] = round(
                        completed_count / max(elapsed, 0.1), 1
                    )

                _save_checkpoint(task_id, completed_batches, [])

                cache_info = f", {cache_hits} å¿«å–å‘½ä¸­" if cache_hits else ""
                logger.info(
                    f"âœ… æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} å®Œæˆ "
                    f"({batch_elapsed:.1f}s, é€²åº¦: {task['progress_pct']}%, "
                    f"ETA: {task.get('eta_seconds', '?')}s{cache_info})"
                )

        # ---- ä¸¦è¡ŒåŸ·è¡Œæ‰¹æ¬¡ ----
        if batches:
            tasks = [process_batch(bi, idxs) for bi, idxs in enumerate(batches)]
            await asyncio.gather(*tasks)

        # ---- çµ„è£ç¯€é» ----
        row_results: List[Optional[Dict]] = list(pre_extracted)

        for batch_idx, indices in enumerate(batches):
            batch_results = llm_results[batch_idx] or []
            for local_i, global_i in enumerate(indices):
                if local_i < len(batch_results):
                    row_results[global_i] = batch_results[local_i]

        for i in range(total_rows):
            if row_results[i] is None:
                row_results[i] = dict(_DEFAULT_NODE)

        nodes: List[Dict[str, Any]] = []
        ts = datetime.now().timestamp()

        for global_i in range(total_rows):
            llm = row_results[global_i] or dict(_DEFAULT_NODE)
            node = {
                "id": f"node_{ts}_{global_i}",
                "name": row_names[global_i],
                "label": llm.get("label", "æœªå‘½å"),
                "description": llm.get("description", ""),
                "type": llm.get("type", "æœªåˆ†é¡"),
                "group": 1,
                "size": 20,
                "keywords": llm.get("keywords", []),
                "tags": llm.get("tags", []),
                "suggested_links": llm.get("suggested_links", []),
                "raw_data": {
                    k: (None if pd.isna(v) else v)
                    for k, v in df.iloc[global_i].to_dict().items()
                },
            }
            nodes.append(node)

        # ---- suggested_links batch-local â†’ å…¨åŸŸ ----
        for batch_idx, indices in enumerate(batches):
            for local_i, global_i in enumerate(indices):
                node = nodes[global_i]
                resolved_links = []
                for link in node.get("suggested_links", []):
                    target_idx = link.get("target_index")
                    if target_idx is not None and isinstance(target_idx, int):
                        if 0 <= target_idx < len(indices) and target_idx != local_i:
                            target_global = indices[target_idx]
                            if 0 <= target_global < len(nodes):
                                resolved_links.append({
                                    "target_id": nodes[target_global]["id"],
                                    "target_name": nodes[target_global]["name"],
                                    "relation": link.get("relation", "complement"),
                                    "reason": link.get("reason", ""),
                                })
                node["links"] = resolved_links
                if "suggested_links" in node:
                    del node["suggested_links"]

        for node in nodes:
            if "suggested_links" in node:
                node["links"] = []
                del node["suggested_links"]

        failed_count = sum(
            1 for n in nodes if n.get("label") == _DEFAULT_NODE["label"]
        )

        # ---- å¯«å…¥ KuzuDB ----
        kuzu_saved = 0
        if kuzu_manager and graph_id:
            logger.info(f"ğŸ“ é–‹å§‹å¯«å…¥ KuzuDB (graph_id={graph_id}, {len(nodes)} å€‹ç¯€é»)...")
            for node in nodes:
                try:
                    props = {
                        "description": node.get("description", ""),
                        "keywords": json.dumps(node.get("keywords", []), ensure_ascii=False),
                        "tags": json.dumps(node.get("tags", []), ensure_ascii=False),
                        "raw_data": json.dumps(node.get("raw_data", {}), ensure_ascii=False, default=str),
                        "source": "excel_import",
                        "import_task_id": task_id,
                    }
                    success = kuzu_manager.add_entity(
                        entity_id=node["id"],
                        name=node.get("label", node.get("name", "æœªå‘½å")),
                        entity_type=node.get("type", "æœªåˆ†é¡"),
                        properties=props,
                        graph_id=graph_id,
                    )
                    if success:
                        kuzu_saved += 1
                except Exception as e:
                    logger.warning(f"âš ï¸ KuzuDB ç¯€é»å¯«å…¥å¤±æ•— ({node.get('id')}): {e}")
            logger.info(f"âœ… KuzuDB å¯«å…¥å®Œæˆ: {kuzu_saved}/{len(nodes)} å€‹ç¯€é»")

            links_created = 0
            for node in nodes:
                for link in node.get("links", []):
                    try:
                        target_id = link.get("target_id")
                        if target_id:
                            kuzu_manager.add_relation(
                                from_id=node["id"],
                                to_id=target_id,
                                relation_type=link.get("relation", "complement"),
                                properties={"reason": link.get("reason", "")}
                            )
                            links_created += 1
                    except Exception:
                        pass
            if links_created:
                logger.info(f"ğŸ”— å·²å»ºç«‹ {links_created} æ¢é€£ç·š")

        # ---- åˆä½µä¸Šå‚³ RAGFlow ----
        ragflow_uploaded = 0
        if ragflow_dataset_id:
            try:
                from backend.rag_client import RAGFlowClient
                from backend.core.config import get_current_api_keys, settings
                import tempfile
                import httpx

                api_keys = get_current_api_keys()
                if api_keys.get('RAGFLOW_API_KEY'):
                    rag_client = RAGFlowClient(
                        api_key=api_keys['RAGFLOW_API_KEY'],
                        base_url=api_keys['RAGFLOW_API_URL'],
                        http_client=http_client,
                    )
                    ragflow_api_url = api_keys.get('RAGFLOW_API_URL', 'http://localhost:9380/api/v1')

                    logger.info(f"ğŸ“š åˆä½µ {len(row_texts)} å€‹ç¯€é»ç‚ºå–®ä¸€æ–‡ä»¶ä¸Šå‚³ RAGFlow...")
                    task["ragflow_stage"] = "uploading"

                    from collections import defaultdict
                    type_groups = defaultdict(list)
                    for ri, (row_text, node) in enumerate(zip(row_texts, nodes)):
                        node_type = node.get("type", "æœªåˆ†é¡")
                        type_groups[node_type].append((ri, row_text, node))

                    temp_dir = Path(tempfile.gettempdir()) / f"ragflow_merged_{task_id[:8]}"
                    temp_dir.mkdir(exist_ok=True)
                    uploaded_doc_ids = []

                    try:
                        for type_name, group_items in type_groups.items():
                            sections = []
                            for ri, row_text, node in group_items:
                                label = node.get("label", f"row_{ri}")
                                desc = node.get("description", "")
                                keywords = ", ".join(node.get("keywords", []))
                                section = f"## {label}\n\n"
                                if desc:
                                    section += f"{desc}\n\n"
                                if keywords:
                                    section += f"**é—œéµè©**: {keywords}\n\n"
                                section += f"**åŸå§‹è³‡æ–™**: {row_text}\n\n---\n"
                                sections.append(section)

                            original_name = task.get("filename", "import").rsplit(".", 1)[0]
                            safe_type = type_name.replace("/", "_").replace("\\", "_")[:20]

                            chunks = []
                            current_chunk = []
                            current_size = 0
                            for sec in sections:
                                sec_bytes = len(sec.encode('utf-8'))
                                if current_size + sec_bytes > MAX_RAGFLOW_FILE_BYTES and current_chunk:
                                    chunks.append(current_chunk)
                                    current_chunk = []
                                    current_size = 0
                                current_chunk.append(sec)
                                current_size += sec_bytes
                            if current_chunk:
                                chunks.append(current_chunk)

                            for chunk_idx, chunk_sections in enumerate(chunks):
                                chunk_count = len(chunk_sections)
                                suffix = f"_part{chunk_idx + 1}" if len(chunks) > 1 else ""
                                merged_filename = f"{original_name}_{safe_type}_{chunk_count}ç­†{suffix}.md"

                                merged_content = f"# {original_name} â€” {type_name}"
                                if len(chunks) > 1:
                                    merged_content += f" (Part {chunk_idx + 1}/{len(chunks)})"
                                merged_content += f"\n\n> å…± {chunk_count} ç­†è³‡æ–™ï¼Œä¾†æº: Excel æ‰¹æ¬¡åŒ¯å…¥\n\n"
                                merged_content += "\n".join(chunk_sections)

                                tmp_file = temp_dir / merged_filename
                                tmp_file.write_text(merged_content, encoding='utf-8')

                                try:
                                    upload_result = await rag_client.async_upload_file(
                                        dataset_id=ragflow_dataset_id,
                                        file_path=str(tmp_file)
                                    )
                                    ragflow_uploaded += chunk_count

                                    docs = upload_result.get('data', [])
                                    if isinstance(docs, list):
                                        for doc in docs:
                                            if isinstance(doc, dict) and doc.get('id'):
                                                uploaded_doc_ids.append(doc['id'])
                                    elif isinstance(docs, dict) and docs.get('id'):
                                        uploaded_doc_ids.append(docs['id'])

                                    logger.info(
                                        f"ğŸ“„ å·²ä¸Šå‚³: {merged_filename} "
                                        f"({chunk_count} ç­†, {len(merged_content)} å­—)"
                                    )
                                except Exception as e:
                                    logger.warning(f"âš ï¸ RAGFlow åˆä½µæ–‡ä»¶ä¸Šå‚³å¤±æ•— ({type_name}{suffix}): {e}")

                        if uploaded_doc_ids:
                            try:
                                async with httpx.AsyncClient(timeout=300) as parse_client:
                                    await parse_client.post(
                                        f"{ragflow_api_url}/datasets/{ragflow_dataset_id}/chunks",
                                        headers={
                                            "Authorization": f"Bearer {api_keys['RAGFLOW_API_KEY']}",
                                            "Content-Type": "application/json"
                                        },
                                        json={"document_ids": uploaded_doc_ids}
                                    )
                                logger.info(
                                    f"ğŸ”„ å·²è§¸ç™¼ {len(uploaded_doc_ids)} å€‹åˆä½µæ–‡ä»¶çš„è§£æ "
                                    f"(åŸ {len(row_texts)} è¡Œ â†’ {len(uploaded_doc_ids)} å€‹æ–‡ä»¶)"
                                )
                            except Exception as parse_err:
                                logger.warning(f"âš ï¸ è§¸ç™¼è§£æå¤±æ•—: {parse_err}")

                        logger.info(
                            f"âœ… RAGFlow åˆä½µä¸Šå‚³å®Œæˆ: {len(row_texts)} è¡Œ â†’ "
                            f"{len(uploaded_doc_ids)} å€‹æ–‡ä»¶ (æŒ‰é¡å‹åˆ†çµ„)"
                        )
                    finally:
                        import shutil
                        shutil.rmtree(temp_dir, ignore_errors=True)
                else:
                    logger.warning("âš ï¸ RAGFlow API Key æœªé…ç½®ï¼Œè·³é RAGFlow ä¸Šå‚³")
            except Exception as e:
                logger.error(f"âŒ RAGFlow é€è¡Œä¸Šå‚³å¤±æ•—: {e}")

        # ---- é‡‹æ”¾è¨˜æ†¶é«” ----
        node_count = len(nodes)
        node_summaries = [
            {"id": n["id"], "label": n.get("label", ""), "type": n.get("type", "")}
            for n in nodes
        ]
        del nodes

        total_elapsed = time.monotonic() - task_start_time
        task.update({
            "status": "done",
            "completed": len(row_texts),
            "failed": failed_count,
            "progress_pct": 100.0,
            "node_count": node_count,
            "node_summaries": node_summaries,
            "kuzu_saved": kuzu_saved,
            "ragflow_uploaded": ragflow_uploaded,
            "finished_at": datetime.now().isoformat(),
            "eta_seconds": 0,
            "elapsed_seconds": round(total_elapsed, 1),
            "rows_per_sec": round(len(row_texts) / max(total_elapsed, 0.1), 1),
        })

        _cleanup_checkpoint(task_id)

        logger.info(
            f"ğŸ‰ ä»»å‹™ {task_id[:8]}... å®Œæˆ: "
            f"{node_count} å€‹ç¯€é», {kuzu_saved} å¯«å…¥ KuzuDB, "
            f"{ragflow_uploaded} ä¸Šå‚³ RAGFlow, {failed_count} å€‹å¤±æ•—"
        )

    except Exception as e:
        logger.error(f"âŒ ä»»å‹™ {task_id[:8]}... å¤±æ•—: {e}", exc_info=True)
        task.update({
            "status": "error",
            "error": str(e),
            "finished_at": datetime.now().isoformat(),
        })
