"""
æª”æ¡ˆä¸Šå‚³åŠŸèƒ½ â€” system.py æ‹†åˆ†

åŒ…å« /upload è·¯ç”±åŠå…¶å®Œæ•´çš„å››éšæ®µä¸Šå‚³ç®¡ç·šï¼š
1. meta.json é å…ˆå¯«å…¥
2. RAGFlow ä¸Šå‚³ + chunking
3. å›å¡« meta.json
4. ä¸»æª”æ¡ˆå¯«å…¥ï¼ˆè§¸ç™¼ Watcherï¼‰
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Request
from typing import Optional
import os
import re
import logging
import asyncio
from pathlib import Path
from datetime import datetime

from backend.core.config import get_current_api_keys, settings

logger = logging.getLogger(__name__)
router = APIRouter()


@router.post("/upload")
async def upload_file(
    raw_request: Request,
    file: UploadFile = File(...),
    graph_id: str = Form(None),
    graph_mode: str = Form("existing"),
    graph_name: str = Form(None),
    enable_ai_link: str = Form("false"),
    ragflow_dataset_id: str = Form(None)
):
    """
    ä¸Šå‚³æª”æ¡ˆåˆ°ç›£æ§è³‡æ–™å¤¾ï¼Œè‡ªå‹•è§¸ç™¼ WatcherService è™•ç†

    Args:
        file: ä¸Šå‚³çš„æª”æ¡ˆ
        graph_id: ç›®æ¨™åœ–è­œ ID
        graph_mode: åœ–è­œæ¨¡å¼ ("new" æˆ– "existing")
        graph_name: æ–°åœ–è­œåç¨± (ç•¶ graph_mode="new" æ™‚ä½¿ç”¨)
        enable_ai_link: æ˜¯å¦å•Ÿç”¨ AI æ™ºèƒ½é€£ç·š ("true" æˆ– "false")
        ragflow_dataset_id: RAGFlow çŸ¥è­˜åº« ID (ç•¶ enable_ai_link="true" æ™‚ä½¿ç”¨)
    """
    try:
        ai_enabled = enable_ai_link.lower() == "true"
        ragflow_doc_ids = []
        logger.info(f"æ”¶åˆ°æ–‡ä»¶ä¸Šå‚³è«‹æ±‚: {file.filename}, graph_mode={graph_mode}, graph_id={graph_id}")

        # æª”æ¡ˆå¤§å°é™åˆ¶
        content = await file.read()
        if len(content) > settings.MAX_UPLOAD_SIZE:
            max_mb = settings.MAX_UPLOAD_SIZE // (1024 * 1024)
            raise HTTPException(status_code=413, detail=f"æª”æ¡ˆå¤§å°è¶…éé™åˆ¶ï¼ˆæœ€å¤§ {max_mb} MBï¼‰")

        upload_dir = Path(settings.AUTO_IMPORT_DIR)
        upload_dir.mkdir(parents=True, exist_ok=True)

        if not file.filename:
            raise HTTPException(status_code=400, detail="æª”æ¡ˆåç¨±ä¸èƒ½ç‚ºç©º")

        safe_filename = re.sub(r'[\\/:*?"<>|]', '_', Path(file.filename).name)
        if safe_filename.startswith('.'):
            safe_filename = '_' + safe_filename

        file_path = upload_dir / safe_filename

        if file_path.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_stem = file_path.stem
            file_suffix = file_path.suffix
            file_path = upload_dir / f"{file_stem}_{timestamp}{file_suffix}"

        # â”€â”€ éšæ®µ 1: å…ˆå¯« meta.json â”€â”€
        import json
        metadata_file = file_path.with_suffix(file_path.suffix + '.meta.json')
        metadata = {
            "graph_id": graph_id,
            "graph_mode": graph_mode,
            "graph_name": graph_name,
            "upload_time": datetime.now().isoformat(),
            "ai_enabled": ai_enabled,
            "ragflow_dataset_id": ragflow_dataset_id if ai_enabled else None,
            "ragflow_result": None,
            "ragflow_doc_ids": None
        }
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“‹ åœ–è­œå…ƒæ•¸æ“šå·²é å¯«å…¥: graph_id={graph_id}, path={metadata_file.name}")

        # â”€â”€ éšæ®µ 2: RAGFlow ä¸Šå‚³ â”€â”€
        ragflow_result = None
        temp_file_for_ragflow = None
        if ai_enabled and ragflow_dataset_id:
            try:
                logger.info(f"ğŸ¤– æ­£åœ¨ä¸Šå‚³åˆ° RAGFlow çŸ¥è­˜åº«: {ragflow_dataset_id}")
                from backend.rag_client import RAGFlowClient

                api_keys = get_current_api_keys()

                if not api_keys['RAGFLOW_API_KEY']:
                    logger.warning("âš ï¸ RAGFlow API Key æœªé…ç½®ï¼Œè·³é RAGFlow ä¸Šå‚³")
                else:
                    import tempfile
                    temp_dir = tempfile.gettempdir()
                    temp_file_for_ragflow = Path(temp_dir) / file_path.name
                    with open(temp_file_for_ragflow, "wb") as tmp:
                        tmp.write(content)

                    ragflow_api_url = api_keys['RAGFLOW_API_URL']
                    rag_client = RAGFlowClient(
                        api_key=api_keys['RAGFLOW_API_KEY'],
                        base_url=ragflow_api_url,
                        http_client=getattr(raw_request.app.state, 'http_client', None),
                    )

                    ragflow_result = await rag_client.async_upload_file(
                        dataset_id=ragflow_dataset_id,
                        file_path=str(temp_file_for_ragflow)
                    )
                    logger.info(f"âœ… RAGFlow ä¸Šå‚³æˆåŠŸ: {ragflow_result}")

                    # è‡ªå‹•è§¸ç™¼æ–‡æª”è§£æ
                    uploaded_docs = ragflow_result.get("data", [])
                    if uploaded_docs:
                        doc_ids = [d["id"] for d in uploaded_docs if "id" in d]
                        if doc_ids:
                            chunk_token_num = settings.RAGFLOW_CHUNK_TOKEN_NUM
                            logger.info(f"ğŸ“‹ æº–å‚™è¨­å®š {len(doc_ids)} ä»½æ–‡æª” chunk_token_num={chunk_token_num}")

                            for doc_id in doc_ids:
                                try:
                                    await rag_client.async_update_document(
                                        dataset_id=ragflow_dataset_id,
                                        document_id=doc_id,
                                        chunk_method="naive",
                                        parser_config={"chunk_token_num": chunk_token_num}
                                    )
                                    logger.info(f"âœ… å·²è¨­å®šæ–‡æª” {doc_id} chunk_token_num={chunk_token_num}")
                                except Exception as cfg_err:
                                    logger.warning(f"âš ï¸ è¨­å®š parser_config å¤±æ•—: {cfg_err}")

                            await asyncio.sleep(2)
                            for doc_id in doc_ids:
                                try:
                                    doc_status = await rag_client.async_get_document_status(
                                        dataset_id=ragflow_dataset_id,
                                        document_id=doc_id
                                    )
                                    actual_config = doc_status.get('parser_config', {})
                                    actual_chunk = actual_config.get('chunk_token_num', 'æœªçŸ¥')
                                    logger.info(f"ğŸ” é©—è­‰æ–‡æª” {doc_id}: chunk_token_num={actual_chunk} (é æœŸ={chunk_token_num})")
                                    if actual_chunk != chunk_token_num and actual_chunk != 'æœªçŸ¥':
                                        logger.warning(f"âš ï¸ chunk_token_num ä¸ç¬¦! é‡è©¦è¨­å®š...")
                                        await rag_client.async_update_document(
                                            dataset_id=ragflow_dataset_id,
                                            document_id=doc_id,
                                            chunk_method="naive",
                                            parser_config={"chunk_token_num": chunk_token_num}
                                        )
                                        await asyncio.sleep(1)
                                except Exception as verify_err:
                                    logger.warning(f"âš ï¸ é©—è­‰ parser_config å¤±æ•—: {verify_err}")

                            import httpx
                            async with httpx.AsyncClient(timeout=300) as parse_client:
                                parse_resp = await parse_client.post(
                                    f"{ragflow_api_url}/datasets/{ragflow_dataset_id}/chunks",
                                    headers={
                                        "Authorization": f"Bearer {api_keys['RAGFLOW_API_KEY']}",
                                        "Content-Type": "application/json"
                                    },
                                    json={"document_ids": doc_ids}
                                )
                                parse_resp.raise_for_status()
                                logger.info(f"âœ… å·²è§¸ç™¼ RAGFlow æ–‡æª”è§£æ: {doc_ids} (chunk_token_num={chunk_token_num})")
                                ragflow_doc_ids = doc_ids
            except Exception as e:
                logger.warning(f"âš ï¸ RAGFlow ä¸Šå‚³å¤±æ•—ï¼ˆç¹¼çºŒè™•ç†ï¼‰: {e}")
            finally:
                if temp_file_for_ragflow and temp_file_for_ragflow.exists():
                    try:
                        temp_file_for_ragflow.unlink()
                    except OSError:
                        pass

        # â”€â”€ éšæ®µ 3: å›å¡« meta.json â”€â”€
        metadata["ragflow_result"] = ragflow_result
        metadata["ragflow_doc_ids"] = ragflow_doc_ids if ai_enabled else None
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        # â”€â”€ éšæ®µ 3.5: ragflow_dataset_id å›å¯«åœ–è­œ â”€â”€
        if ai_enabled and ragflow_dataset_id and graph_id:
            try:
                from backend.api.graph import get_kuzu_manager
                kuzu_mgr = get_kuzu_manager()
                if kuzu_mgr:
                    existing_meta = kuzu_mgr.get_graph_metadata(graph_id)
                    if existing_meta and not existing_meta.get('ragflow_dataset_id'):
                        kuzu_mgr.update_graph_metadata(graph_id, ragflow_dataset_id=ragflow_dataset_id)
                        logger.info(f"ğŸ“Œ å·²å°‡ ragflow_dataset_id={ragflow_dataset_id} å¯«å…¥åœ–è­œ {graph_id}")
            except Exception as e:
                logger.warning(f"âš ï¸ å›å¯« ragflow_dataset_id å¤±æ•—ï¼ˆä¸å½±éŸ¿ä¸Šå‚³ï¼‰: {e}")

        # â”€â”€ éšæ®µ 4: å¯«å…¥ä¸»æª”æ¡ˆï¼ˆè§¸ç™¼ Watcherï¼‰ â”€â”€
        with open(file_path, "wb") as buffer:
            buffer.write(content)

        logger.info(f"âœ… æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œå·²é€²å…¥ç›£æ§ä½‡åˆ—: {file_path}")

        message = "æª”æ¡ˆå·²é€å…¥ç¥ç¶“ç¶²è·¯ï¼Œæ­£åœ¨è§£æä¸­..."
        if ai_enabled:
            if ragflow_result:
                message = "âœ¨ æª”æ¡ˆå·²ä¸Šå‚³åˆ° RAGFlow ä¸¦é€å…¥ç¥ç¶“ç¶²è·¯ï¼Œæ­£åœ¨ AI åˆ†æä¸­..."
            else:
                message = "âš ï¸ æª”æ¡ˆå·²é€å…¥ç¥ç¶“ç¶²è·¯ï¼ˆRAGFlow ä¸Šå‚³å¤±æ•—ï¼‰ï¼Œæ­£åœ¨è§£æä¸­..."

        return {
            "success": True,
            "message": message,
            "filename": file.filename,
            "saved_path": str(file_path),
            "size": os.path.getsize(file_path),
            "upload_time": datetime.now().isoformat(),
            "ai_enabled": ai_enabled,
            "ragflow_processed": ragflow_result is not None,
            "ragflow_dataset_id": ragflow_dataset_id if ai_enabled else None,
            "ragflow_doc_ids": ragflow_doc_ids if ai_enabled else []
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æª”æ¡ˆä¸Šå‚³å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æª”æ¡ˆä¸Šå‚³å¤±æ•—: {str(e)}")
