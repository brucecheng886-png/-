"""
ç³»çµ±é…ç½®ç®¡ç† API
ç”¨æ–¼å‹•æ…‹æ›´æ–°é…ç½®ï¼ˆå„ªå…ˆä½¿ç”¨ config.jsonï¼Œå…¼å®¹ .envï¼‰
"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from pydantic import BaseModel
from typing import Optional
import os
import re
import logging
import shutil
from pathlib import Path
from datetime import datetime
from backend.core.config import load_config_from_file, save_config_to_file, get_current_api_keys, settings
from backend.core.circuit_breaker import dify_breaker, ragflow_breaker

logger = logging.getLogger(__name__)
router = APIRouter()


class ConfigUpdateRequest(BaseModel):
    """é…ç½®æ›´æ–°è«‹æ±‚æ¨¡å‹"""
    dify_key: Optional[str] = None
    ragflow_key: Optional[str] = None
    dify_api_url: Optional[str] = None
    ragflow_api_url: Optional[str] = None


class ConfigResponse(BaseModel):
    """é…ç½®éŸ¿æ‡‰æ¨¡å‹"""
    success: bool
    message: str
    config: Optional[dict] = None


def get_env_file_path() -> Path:
    """ç²å– .env æª”æ¡ˆè·¯å¾‘"""
    # å¾ç•¶å‰å·¥ä½œç›®éŒ„é–‹å§‹å°‹æ‰¾ .env
    current_dir = Path.cwd()
    env_path = current_dir / ".env"
    
    # å¦‚æœç•¶å‰ç›®éŒ„æ²’æœ‰ï¼Œå˜—è©¦çˆ¶ç›®éŒ„ï¼ˆé©æ‡‰ä¸åŒå•Ÿå‹•ä½ç½®ï¼‰
    if not env_path.exists():
        env_path = current_dir.parent / ".env"
    
    return env_path


def mask_api_key(key: Optional[str]) -> str:
    """é®è”½ API Keyï¼Œåªé¡¯ç¤ºå‰ 5 ç¢¼"""
    if not key or len(key) <= 5:
        return "******"
    return f"{key[:5]}{'*' * (len(key) - 5)}"


def is_masked_key(key: Optional[str]) -> bool:
    """æª¢æ¸¬æ˜¯å¦ç‚ºé®ç½©å¾Œçš„ API Keyï¼ˆåŒ…å«é€£çºŒ * è™Ÿï¼‰"""
    if not key:
        return False
    return '***' in key


def read_env_file() -> dict:
    """è®€å– .env æª”æ¡ˆå…§å®¹"""
    env_path = get_env_file_path()
    
    if not env_path.exists():
        logger.warning(f".env æª”æ¡ˆä¸å­˜åœ¨: {env_path}")
        return {}
    
    env_vars = {}
    try:
        with open(env_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³éè¨»é‡‹å’Œç©ºè¡Œ
                if not line or line.startswith('#'):
                    continue
                # è§£æ KEY=VALUE
                if '=' in line:
                    key, value = line.split('=', 1)
                    env_vars[key.strip()] = value.strip()
    except Exception as e:
        logger.error(f"è®€å– .env æª”æ¡ˆå¤±æ•—: {e}")
    
    return env_vars


def update_env_file(updates: dict) -> bool:
    """æ›´æ–° .env æª”æ¡ˆä¸­çš„è®Šæ•¸"""
    env_path = get_env_file_path()
    
    try:
        # è®€å–ç¾æœ‰å…§å®¹
        if env_path.exists():
            with open(env_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        else:
            logger.info(f"å‰µå»ºæ–°çš„ .env æª”æ¡ˆ: {env_path}")
            lines = []
        
        # è¿½è¹¤å“ªäº›è®Šæ•¸å·²ç¶“æ›´æ–°
        updated_vars = set()
        
        # è™•ç†ç¾æœ‰è¡Œ
        for i, line in enumerate(lines):
            original_line = line
            line_stripped = line.strip()
            
            # è·³éè¨»é‡‹å’Œç©ºè¡Œ
            if not line_stripped or line_stripped.startswith('#'):
                continue
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ­¤è¡Œ
            for key, value in updates.items():
                # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é… KEY=...
                pattern = rf'^{re.escape(key)}\s*=\s*.*$'
                if re.match(pattern, line_stripped):
                    lines[i] = f"{key}={value}\n"
                    updated_vars.add(key)
                    logger.info(f"æ›´æ–°è®Šæ•¸: {key}")
                    break
        
        # æ·»åŠ æœªæ‰¾åˆ°çš„æ–°è®Šæ•¸
        for key, value in updates.items():
            if key not in updated_vars:
                lines.append(f"{key}={value}\n")
                logger.info(f"æ–°å¢è®Šæ•¸: {key}")
        
        # å¯«å›æª”æ¡ˆ
        with open(env_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)
        
        logger.info(f"æˆåŠŸæ›´æ–° .env æª”æ¡ˆ: {env_path}")
        return True
        
    except Exception as e:
        logger.error(f"æ›´æ–° .env æª”æ¡ˆå¤±æ•—: {e}")
        return False


def reload_env_to_os() -> None:
    """é‡æ–°è¼‰å…¥ .env åˆ° os.environï¼ˆå˜—è©¦å³æ™‚ç”Ÿæ•ˆï¼‰"""
    env_vars = read_env_file()
    for key, value in env_vars.items():
        os.environ[key] = value
        logger.debug(f"è¼‰å…¥ç’°å¢ƒè®Šæ•¸: {key}")


@router.get("/config")
async def get_config():
    """
    ç²å–ç•¶å‰ç³»çµ±é…ç½®
    
    è¿”å›æ ¼å¼:
    {
        "success": true,
        "config": {
            "dify_key": "app-xxxxxxxx",
            "ragflow_key": "ragflow-xxxxxxxx",
            "dify_api_url": "http://localhost:82/v1",
            "ragflow_api_url": "http://localhost:9380/api/v1"
        }
    }
    """
    try:
        # å¾çµ±ä¸€çš„é…ç½®æºç²å–ï¼ˆconfig.json å„ªå…ˆï¼Œç„¶å¾Œæ˜¯ç’°å¢ƒè®Šæ•¸ï¼‰
        api_keys = get_current_api_keys()
        
        return ConfigResponse(
            success=True,
            message="é…ç½®ç²å–æˆåŠŸ",
            config={
                "dify_key": mask_api_key(api_keys['DIFY_API_KEY']),
                "ragflow_key": mask_api_key(api_keys['RAGFLOW_API_KEY']),
                "dify_api_url": api_keys['DIFY_API_URL'],
                "ragflow_api_url": api_keys['RAGFLOW_API_URL'],
                "config_source": "config.json (C:/BruV_Data/config.json)"
            }
        )
    
    except Exception as e:
        logger.error(f"ç²å–é…ç½®å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–é…ç½®å¤±æ•—: {str(e)}")


@router.post("/config")
async def update_config(request: ConfigUpdateRequest):
    """
    æ›´æ–°ç³»çµ±é…ç½®ï¼ˆä¿å­˜åˆ° config.jsonï¼‰
    
    è«‹æ±‚æ ¼å¼:
    {
        "dify_key": "app-xxxxxxxxxxxxxxxx",
        "ragflow_key": "ragflow-xxxxxxxxxxxxxxxx",
        "dify_api_url": "http://localhost:82/v1",
        "ragflow_api_url": "http://localhost:9380/api/v1"
    }
    
    è¿”å›æ ¼å¼:
    {
        "success": true,
        "message": "é…ç½®æ›´æ–°æˆåŠŸ",
        "config": { ... }
    }
    """
    try:
        config_updates = {}
        
        # æº–å‚™è¦æ›´æ–°çš„é…ç½®ï¼ˆè·³éé®ç½©å€¼ï¼Œé˜²æ­¢å‰ç«¯å°‡ masked key å­˜å›ï¼‰
        if request.dify_key:
            if is_masked_key(request.dify_key):
                logger.warning("å¿½ç•¥é®ç½©çš„ dify_api_keyï¼Œä¸æ›´æ–°")
            else:
                config_updates['dify_api_key'] = request.dify_key
                logger.info("æº–å‚™æ›´æ–° dify_api_key")
        
        if request.ragflow_key:
            if is_masked_key(request.ragflow_key):
                logger.warning("å¿½ç•¥é®ç½©çš„ ragflow_api_keyï¼Œä¸æ›´æ–°")
            else:
                config_updates['ragflow_api_key'] = request.ragflow_key
                logger.info("æº–å‚™æ›´æ–° ragflow_api_key")
        
        if request.dify_api_url:
            config_updates['dify_api_url'] = request.dify_api_url
            logger.info(f"æº–å‚™æ›´æ–° dify_api_url: {request.dify_api_url}")
        
        if request.ragflow_api_url:
            config_updates['ragflow_api_url'] = request.ragflow_api_url
            logger.info(f"æº–å‚™æ›´æ–° ragflow_api_url: {request.ragflow_api_url}")
        
        if not config_updates:
            raise HTTPException(
                status_code=400, 
                detail="è‡³å°‘éœ€è¦æä¾›ä¸€å€‹è¨­å®šé …ç›®"
            )
        
        # ä¿å­˜åˆ° config.json
        success = save_config_to_file(config_updates)
        
        if not success:
            raise HTTPException(status_code=500, detail="æ›´æ–°é…ç½®æª”æ¡ˆå¤±æ•—")
        
        # æº–å‚™å›æ‡‰é…ç½®ï¼ˆè¿”å›å®Œæ•´ API Keyï¼Œä¸é®ç½©ï¼‰
        response_config = {}
        if request.dify_key:
            response_config['dify_key'] = request.dify_key
        if request.ragflow_key:
            response_config['ragflow_key'] = request.ragflow_key
        if request.dify_api_url:
            response_config['dify_api_url'] = request.dify_api_url
        if request.ragflow_api_url:
            response_config['ragflow_api_url'] = request.ragflow_api_url
        
        return ConfigResponse(
            success=True,
            message="âœ… é…ç½®å·²ä¿å­˜åˆ° config.jsonï¼ä¿®æ”¹å°‡ç«‹å³ç”Ÿæ•ˆ",
            config=response_config
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°é…ç½®å¤±æ•—: {str(e)}")


@router.get("/env-file")
async def get_env_file_location():
    """
    ç²å– .env æª”æ¡ˆä½ç½®å’Œç‹€æ…‹
    
    è¿”å›æ ¼å¼:
    {
        "success": true,
        "path": "/path/to/.env",
        "exists": true,
        "writable": true
    }
    """
    try:
        env_path = get_env_file_path()
        exists = env_path.exists()
        writable = os.access(env_path.parent, os.W_OK) if exists else os.access(env_path.parent, os.W_OK)
        
        return {
            "success": True,
            "path": str(env_path),
            "exists": exists,
            "writable": writable
        }
    
    except Exception as e:
        logger.error(f"ç²å– .env æª”æ¡ˆè³‡è¨Šå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç²å–æª”æ¡ˆè³‡è¨Šå¤±æ•—: {str(e)}")


@router.post("/test-connection")
async def test_connection():
    """
    æ¸¬è©¦ Dify å’Œ RAGFlow æœå‹™é€£æ¥
    
    è¿”å›æ ¼å¼:
    {
        "success": true,
        "dify": {
            "status": "ok" | "error",
            "url": "http://localhost:5001/v1",
            "message": "é€£æ¥æˆåŠŸ" | "éŒ¯èª¤è¨Šæ¯",
            "api_key_configured": true
        },
        "ragflow": {
            "status": "ok" | "error",
            "url": "http://localhost:9380/api/v1",
            "message": "é€£æ¥æˆåŠŸ" | "éŒ¯èª¤è¨Šæ¯",
            "api_key_configured": true
        }
    }
    """
    from backend.core.config import get_current_api_keys
    import httpx
    
    api_keys = get_current_api_keys()
    results = {
        "success": True,
        "dify": {
            "status": "unknown",
            "url": api_keys['DIFY_API_URL'],
            "message": "",
            "api_key_configured": bool(api_keys['DIFY_API_KEY'])
        },
        "ragflow": {
            "status": "unknown",
            "url": api_keys['RAGFLOW_API_URL'],
            "message": "",
            "api_key_configured": bool(api_keys['RAGFLOW_API_KEY'])
        }
    }
    
    # æ¸¬è©¦ Dify é€£æ¥
    try:
        if not api_keys['DIFY_API_KEY']:
            results['dify']['status'] = 'warning'
            results['dify']['message'] = 'âŒ API Key æœªé…ç½®'
        else:
            async with httpx.AsyncClient(timeout=5.0) as client:
                # ç°¡å–®æ¸¬è©¦ Dify API æ˜¯å¦å¯è¨ªå•
                response = await client.get(
                    api_keys['DIFY_API_URL'],
                    headers={"Authorization": f"Bearer {api_keys['DIFY_API_KEY']}"}
                )
                # ä»»ä½•æœ‰æ•ˆçš„ HTTP éŸ¿æ‡‰éƒ½è¡¨ç¤ºæœå‹™åœ¨é‹è¡Œ
                # 401/404 éƒ½æ˜¯æ­£å¸¸çš„ï¼ˆæœå‹™é‹è¡Œä½†ç«¯é»/æ¬Šé™å•é¡Œï¼‰
                if response.status_code in [200, 401, 404, 422]:
                    results['dify']['status'] = 'ok'
                    results['dify']['message'] = 'âœ… é€£æ¥æˆåŠŸ'
                elif response.status_code == 403:
                    results['dify']['status'] = 'error'
                    results['dify']['message'] = 'âŒ API Key ç„¡æ¬Šé™'
                else:
                    results['dify']['status'] = 'error'
                    results['dify']['message'] = f'âŒ æœå‹™éŒ¯èª¤ ({response.status_code})'
    except httpx.ConnectError:
        results['dify']['status'] = 'error'
        results['dify']['message'] = f'âŒ ç„¡æ³•é€£æ¥åˆ° {api_keys["DIFY_API_URL"]} - è«‹ç¢ºèªæœå‹™å·²å•Ÿå‹•'
    except httpx.TimeoutException:
        results['dify']['status'] = 'error'
        results['dify']['message'] = 'âŒ é€£æ¥è¶…æ™‚'
    except Exception as e:
        results['dify']['status'] = 'error'
        results['dify']['message'] = f'âŒ {str(e)}'
    
    # æ¸¬è©¦ RAGFlow é€£æ¥
    try:
        if not api_keys['RAGFLOW_API_KEY']:
            results['ragflow']['status'] = 'warning'
            results['ragflow']['message'] = 'âŒ API Key æœªé…ç½®'
        else:
            async with httpx.AsyncClient(timeout=5.0) as client:
                # æ¸¬è©¦ RAGFlow datasets ç«¯é»
                response = await client.get(
                    f"{api_keys['RAGFLOW_API_URL'].rstrip('/')}/datasets",
                    headers={"Authorization": f"Bearer {api_keys['RAGFLOW_API_KEY']}"}
                )
                if response.status_code == 200:
                    results['ragflow']['status'] = 'ok'
                    data = response.json()
                    # å®‰å…¨åœ°ç²å–æ•¸æ“šé›†æ•¸é‡
                    datasets = data.get('data', [])
                    if isinstance(datasets, list):
                        dataset_count = len(datasets)
                        results['ragflow']['message'] = f'âœ… é€£æ¥æˆåŠŸï¼ˆæ‰¾åˆ° {dataset_count} å€‹çŸ¥è­˜åº«ï¼‰'
                    else:
                        results['ragflow']['message'] = 'âœ… é€£æ¥æˆåŠŸ'
                elif response.status_code == 401:
                    results['ragflow']['status'] = 'error'
                    results['ragflow']['message'] = 'âŒ API Key ç„¡æ•ˆæˆ–å·²éæœŸ'
                else:
                    results['ragflow']['status'] = 'error'
                    results['ragflow']['message'] = f'âŒ æœå‹™éŒ¯èª¤ ({response.status_code})'
    except httpx.ConnectError:
        results['ragflow']['status'] = 'error'
        results['ragflow']['message'] = f'âŒ ç„¡æ³•é€£æ¥åˆ° {api_keys["RAGFLOW_API_URL"]} - è«‹ç¢ºèªæœå‹™å·²å•Ÿå‹•'
    except httpx.TimeoutException:
        results['ragflow']['status'] = 'error'
        results['ragflow']['message'] = 'âŒ é€£æ¥è¶…æ™‚'
    except Exception as e:
        results['ragflow']['status'] = 'error'
        results['ragflow']['message'] = f'âŒ {str(e)}'
    
    # åˆ¤æ–·æ•´é«”ç‹€æ…‹
    if results['dify']['status'] == 'error' or results['ragflow']['status'] == 'error':
        results['success'] = False
    
    return results


@router.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    graph_id: str = Form(None),
    graph_mode: str = Form("existing"),
    graph_name: str = Form(None),
    enable_ai_link: str = Form("false"),
    ragflow_dataset_id: str = Form(None)
):
    """
    ä¸Šå‚³æª”æ¡ˆåˆ°ç›£æ§è³‡æ–™å¤¾ï¼Œè‡ªå‹•è§¸ç™¼ WatcherService è™•ç†
    
    Args:
        file: ä¸Šå‚³çš„æª”æ¡ˆ
        graph_id: ç›®æ¨™åœ–è­œ ID
        graph_mode: åœ–è­œæ¨¡å¼ ("new" æˆ– "existing")
        graph_name: æ–°åœ–è­œåç¨± (ç•¶ graph_mode="new" æ™‚ä½¿ç”¨)
        enable_ai_link: æ˜¯å¦å•Ÿç”¨ AI æ™ºèƒ½é€£ç·š ("true" æˆ– "false")
        ragflow_dataset_id: RAGFlow çŸ¥è­˜åº« ID (ç•¶ enable_ai_link="true" æ™‚ä½¿ç”¨)
    
    Returns:
        ä¸Šå‚³çµæœè³‡è¨Š
    """
    try:
        ai_enabled = enable_ai_link.lower() == "true"
        ragflow_doc_ids = []
        logger.info(f"æ”¶åˆ°æ–‡ä»¶ä¸Šå‚³è«‹æ±‚: {file.filename}, graph_mode={graph_mode}, graph_id={graph_id}")
        
        # æª”æ¡ˆå¤§å°é™åˆ¶æª¢æŸ¥
        content = await file.read()
        if len(content) > settings.MAX_UPLOAD_SIZE:
            max_mb = settings.MAX_UPLOAD_SIZE // (1024 * 1024)
            raise HTTPException(
                status_code=413,
                detail=f"æª”æ¡ˆå¤§å°è¶…éé™åˆ¶ï¼ˆæœ€å¤§ {max_mb} MBï¼‰"
            )
        
        # ä½¿ç”¨ç›£æ§è³‡æ–™å¤¾ä½œç‚ºä¸Šå‚³ç›®éŒ„ï¼ˆè·¨å¹³å°è·¯å¾‘ï¼‰
        upload_dir = Path(settings.AUTO_IMPORT_DIR)
        
        # ç¢ºä¿ä¸Šå‚³ç›®éŒ„å­˜åœ¨
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        # æª¢æŸ¥æª”æ¡ˆåç¨±
        if not file.filename:
            raise HTTPException(status_code=400, detail="æª”æ¡ˆåç¨±ä¸èƒ½ç‚ºç©º")
        
        # æ¸…æ´—æª”æ¡ˆåç¨±ï¼Œé˜²æ­¢è·¯å¾‘éæ­·æ”»æ“Š
        import re
        safe_filename = re.sub(r'[\\/:*?"<>|]', '_', Path(file.filename).name)
        if safe_filename.startswith('.'):
            safe_filename = '_' + safe_filename
        
        # ç”Ÿæˆæª”æ¡ˆè·¯å¾‘
        file_path = upload_dir / safe_filename
        
        # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œæ·»åŠ æ™‚é–“æˆ³
        if file_path.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_stem = file_path.stem
            file_suffix = file_path.suffix
            file_path = upload_dir / f"{file_stem}_{timestamp}{file_suffix}"
        
        # â”€â”€ éšæ®µ 1: å…ˆå¯« meta.jsonï¼ˆç¢ºä¿ Watcher è®€å–ä¸æœƒå¤±æ•—ï¼‰â”€â”€
        # æ³¨æ„ï¼šä¸»æª”æ¡ˆå°šæœªå¯«å…¥ï¼ŒWatcher ä¸æœƒè¢«è§¸ç™¼
        import json
        metadata_file = file_path.with_suffix(file_path.suffix + '.meta.json')
        metadata = {
            "graph_id": graph_id,
            "graph_mode": graph_mode,
            "graph_name": graph_name,
            "upload_time": datetime.now().isoformat(),
            "ai_enabled": ai_enabled,
            "ragflow_dataset_id": ragflow_dataset_id if ai_enabled else None,
            "ragflow_result": None,
            "ragflow_doc_ids": None
        }
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“‹ åœ–è­œå…ƒæ•¸æ“šå·²é å¯«å…¥: graph_id={graph_id}, path={metadata_file.name}")
        
        # â”€â”€ éšæ®µ 2: RAGFlow ä¸Šå‚³ï¼ˆè€—æ™‚æ“ä½œï¼Œåœ¨ä¸»æª”æ¡ˆå¯«å…¥å‰å®Œæˆï¼‰â”€â”€
        # å…ˆç”¨è‡¨æ™‚æª”å¯«å…¥ content ä¾› RAGFlow ä¸Šå‚³ï¼Œä½†ä¸æ”¾åœ¨ Auto_Import è§¸ç™¼ Watcher
        ragflow_result = None
        temp_file_for_ragflow = None
        if ai_enabled and ragflow_dataset_id:
            try:
                logger.info(f"ğŸ¤– æ­£åœ¨ä¸Šå‚³åˆ° RAGFlow çŸ¥è­˜åº«: {ragflow_dataset_id}")
                from backend.rag_client import RAGFlowClient
                from backend.core.config import get_current_api_keys
                
                # å‹•æ…‹ç²å–ç•¶å‰ API Keys
                api_keys = get_current_api_keys()
                
                # æª¢æŸ¥ API Key æ˜¯å¦é…ç½®
                if not api_keys['RAGFLOW_API_KEY']:
                    logger.warning("âš ï¸ RAGFlow API Key æœªé…ç½®ï¼Œè·³é RAGFlow ä¸Šå‚³")
                else:
                    # å°‡å…§å®¹å¯«å…¥è‡¨æ™‚æª”æ¡ˆä¾› RAGFlow ä¸Šå‚³ï¼ˆé¿å…è§¸ç™¼ Watcherï¼‰
                    import tempfile
                    temp_dir = tempfile.gettempdir()
                    temp_file_for_ragflow = Path(temp_dir) / file_path.name
                    with open(temp_file_for_ragflow, "wb") as tmp:
                        tmp.write(content)
                    
                    # ä½¿ç”¨é…ç½®ä¸­çš„ RAGFlow API URL
                    ragflow_api_url = api_keys['RAGFLOW_API_URL']
                    rag_client = RAGFlowClient(
                        api_key=api_keys['RAGFLOW_API_KEY'],
                        base_url=ragflow_api_url
                    )
                    
                    # éåŒæ­¥ä¸Šå‚³åˆ° RAGFlowï¼ˆé¿å…é˜»å¡äº‹ä»¶è¿´åœˆï¼‰
                    ragflow_result = await rag_client.async_upload_file(
                        dataset_id=ragflow_dataset_id,
                        file_path=str(temp_file_for_ragflow)
                    )
                    logger.info(f"âœ… RAGFlow ä¸Šå‚³æˆåŠŸ: {ragflow_result}")
                    
                    # è‡ªå‹•è§¸ç™¼æ–‡æª”è§£æï¼ˆchunking + embeddingï¼‰
                    uploaded_docs = ragflow_result.get("data", [])
                    if uploaded_docs:
                        doc_ids = [d["id"] for d in uploaded_docs if "id" in d]
                        if doc_ids:
                            import httpx
                            async with httpx.AsyncClient(timeout=300) as parse_client:
                                parse_resp = await parse_client.post(
                                    f"{ragflow_api_url}/datasets/{ragflow_dataset_id}/chunks",
                                    headers={
                                        "Authorization": f"Bearer {api_keys['RAGFLOW_API_KEY']}",
                                        "Content-Type": "application/json"
                                    },
                                    json={"document_ids": doc_ids}
                                )
                                parse_resp.raise_for_status()
                                logger.info(f"âœ… å·²è§¸ç™¼ RAGFlow æ–‡æª”è§£æ: {doc_ids}")
                                ragflow_doc_ids = doc_ids
            except Exception as e:
                logger.warning(f"âš ï¸ RAGFlow ä¸Šå‚³å¤±æ•—ï¼ˆç¹¼çºŒè™•ç†ï¼‰: {e}")
            finally:
                # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                if temp_file_for_ragflow and temp_file_for_ragflow.exists():
                    try:
                        temp_file_for_ragflow.unlink()
                    except OSError:
                        pass
        
        # â”€â”€ éšæ®µ 3: å›å¡« meta.jsonï¼ˆè£œå…… RAGFlow çµæœï¼‰â”€â”€
        metadata["ragflow_result"] = ragflow_result
        metadata["ragflow_doc_ids"] = ragflow_doc_ids if ai_enabled else None
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # â”€â”€ éšæ®µ 3.5: å°‡ ragflow_dataset_id å›å¯«åˆ°åœ–è­œå…ƒæ•¸æ“š â”€â”€
        if ai_enabled and ragflow_dataset_id and graph_id:
            try:
                from backend.api.graph import get_kuzu_manager
                kuzu_mgr = get_kuzu_manager()
                if kuzu_mgr:
                    existing_meta = kuzu_mgr.get_graph_metadata(graph_id)
                    if existing_meta and not existing_meta.get('ragflow_dataset_id'):
                        kuzu_mgr.update_graph_metadata(graph_id, ragflow_dataset_id=ragflow_dataset_id)
                        logger.info(f"ğŸ“Œ å·²å°‡ ragflow_dataset_id={ragflow_dataset_id} å¯«å…¥åœ–è­œ {graph_id}")
            except Exception as e:
                logger.warning(f"âš ï¸ å›å¯« ragflow_dataset_id å¤±æ•—ï¼ˆä¸å½±éŸ¿ä¸Šå‚³ï¼‰: {e}")
        
        # â”€â”€ éšæ®µ 4: æœ€å¾Œå¯«å…¥ä¸»æª”æ¡ˆï¼ˆè§¸ç™¼ Watcherï¼Œæ­¤æ™‚ meta.json å·²å°±ç·’ï¼‰â”€â”€
        with open(file_path, "wb") as buffer:
            buffer.write(content)
        
        logger.info(f"âœ… æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œå·²é€²å…¥ç›£æ§ä½‡åˆ—: {file_path}")
        logger.info(f"ğŸ“‹ åœ–è­œå…ƒæ•¸æ“šå·²ä¿å­˜: {metadata}")
        
        message = "æª”æ¡ˆå·²é€å…¥ç¥ç¶“ç¶²è·¯ï¼Œæ­£åœ¨è§£æä¸­..."
        if ai_enabled:
            if ragflow_result:
                message = "âœ¨ æª”æ¡ˆå·²ä¸Šå‚³åˆ° RAGFlow ä¸¦é€å…¥ç¥ç¶“ç¶²è·¯ï¼Œæ­£åœ¨ AI åˆ†æä¸­..."
            else:
                message = "âš ï¸ æª”æ¡ˆå·²é€å…¥ç¥ç¶“ç¶²è·¯ï¼ˆRAGFlow ä¸Šå‚³å¤±æ•—ï¼‰ï¼Œæ­£åœ¨è§£æä¸­..."
        
        return {
            "success": True,
            "message": message,
            "filename": file.filename,
            "saved_path": str(file_path),
            "size": os.path.getsize(file_path),
            "upload_time": datetime.now().isoformat(),
            "ai_enabled": ai_enabled,
            "ragflow_processed": ragflow_result is not None,
            "ragflow_dataset_id": ragflow_dataset_id if ai_enabled else None,
            "ragflow_doc_ids": ragflow_doc_ids if ai_enabled else []
        }
        
    except Exception as e:
        logger.error(f"âŒ æª”æ¡ˆä¸Šå‚³å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æª”æ¡ˆä¸Šå‚³å¤±æ•—: {str(e)}")


# ==================== æ–·è·¯å™¨ç‹€æ…‹ API ====================

@router.get("/circuit-breakers")
async def get_circuit_breaker_status():
    """
    å–å¾—æ‰€æœ‰æ–·è·¯å™¨ç‹€æ…‹

    Returns:
        å„ä¸‹æ¸¸æœå‹™æ–·è·¯å™¨çš„ç•¶å‰ç‹€æ…‹ã€å¤±æ•—æ¬¡æ•¸ã€å‰©é¤˜æ¢å¾©æ™‚é–“
    """
    return {
        "success": True,
        "data": {
            "dify": dify_breaker.get_status(),
            "ragflow": ragflow_breaker.get_status(),
        }
    }


# ==================== DLQ (Dead Letter Queue) API ====================

@router.get("/saga-dlq")
async def get_saga_dlq():
    """
    å–å¾— Saga æ­»ä¿¡ä½‡åˆ—ä¸­çš„æœªè§£æ±ºé …ç›®

    ç”¨é€”ï¼šç®¡ç†å“¡æ’æŸ¥ RAGFlow â†” KuzuDB ä¸ä¸€è‡´å•é¡Œ
    """
    try:
        from backend.services.watcher import dlq
        items = dlq.list_unresolved(limit=50)
        return {
            "success": True,
            "data": items,
            "total": len(items)
        }
    except Exception as e:
        logger.error(f"DLQ æŸ¥è©¢å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"DLQ æŸ¥è©¢å¤±æ•—: {str(e)}")


@router.post("/saga-dlq/{dlq_id}/resolve")
async def resolve_dlq_item(dlq_id: str):
    """æ¨™è¨˜ DLQ é …ç›®ç‚ºå·²è§£æ±º"""
    try:
        from backend.services.watcher import dlq
        success = dlq.mark_resolved(dlq_id)
        if success:
            return {"success": True, "message": f"DLQ é …ç›® {dlq_id} å·²æ¨™è¨˜ç‚ºå·²è§£æ±º"}
        raise HTTPException(status_code=404, detail="DLQ é …ç›®ä¸å­˜åœ¨")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"DLQ æ¨™è¨˜å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== ç³»çµ±ç¶­è­· API ====================

@router.post("/maintenance/cleanup")
async def system_cleanup(retention_days: int = 7):
    """
    æ¸…ç†éæœŸçš„ SagaLog èˆ‡ TaskQueue è¨˜éŒ„

    - SagaLog: åˆªé™¤ç‹€æ…‹ç‚º completed / compensation_complete ä¸”è¶…é retention_days çš„è¨˜éŒ„
    - TaskQueue: åˆªé™¤ç‹€æ…‹ç‚º completed / failed ä¸”è¶…é retention_days çš„è¨˜éŒ„
    - DLQ: ä¿ç•™ä¸å‹•ï¼Œéœ€äººå·¥ç¢ºèªå¾Œæ‰‹å‹•è§£æ±º

    Args:
        retention_days: ä¿ç•™å¤©æ•¸ï¼Œé è¨­ 7 å¤©
    """
    import sqlite3
    from datetime import datetime, timedelta

    cutoff = (datetime.now() - timedelta(days=retention_days)).isoformat()
    results = {"saga_deleted": 0, "task_deleted": 0, "retention_days": retention_days, "cutoff": cutoff}

    # â”€â”€ æ¸…ç† SagaLog â”€â”€
    try:
        from backend.services.saga import _SAGA_DB_PATH
        if _SAGA_DB_PATH.exists():
            conn = sqlite3.connect(str(_SAGA_DB_PATH), timeout=5)
            cursor = conn.execute(
                "DELETE FROM saga_logs WHERE status IN ('completed', 'compensation_complete') "
                "AND created_at < ?",
                (cutoff,)
            )
            results["saga_deleted"] = cursor.rowcount
            conn.commit()
            conn.close()
            logger.info(f"ğŸ§¹ å·²æ¸…ç† {results['saga_deleted']} ç­†éæœŸ SagaLog")
    except Exception as e:
        logger.error(f"SagaLog æ¸…ç†å¤±æ•—: {e}")
        results["saga_error"] = str(e)

    # â”€â”€ æ¸…ç† TaskQueue â”€â”€
    try:
        from backend.services.task_queue import _TASK_DB_PATH
        if _TASK_DB_PATH.exists():
            conn = sqlite3.connect(str(_TASK_DB_PATH), timeout=5)
            cursor = conn.execute(
                "DELETE FROM tasks WHERE status IN ('completed', 'failed') "
                "AND completed_at < ?",
                (cutoff,)
            )
            results["task_deleted"] = cursor.rowcount
            conn.commit()
            conn.close()
            logger.info(f"ğŸ§¹ å·²æ¸…ç† {results['task_deleted']} ç­†éæœŸ TaskQueue è¨˜éŒ„")
    except Exception as e:
        logger.error(f"TaskQueue æ¸…ç†å¤±æ•—: {e}")
        results["task_error"] = str(e)

    return {
        "success": True,
        "message": f"æ¸…ç†å®Œæˆ: SagaLog {results['saga_deleted']} ç­†, TaskQueue {results['task_deleted']} ç­† (DLQ ä¿ç•™ä¸å‹•)",
        "data": results
    }
