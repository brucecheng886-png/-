"""
æª”æ¡ˆè™•ç†å¼•æ“ â€” å¾ watcher.py æ‹†åˆ†

è² è²¬ï¼š
  - Saga æµç¨‹ (RAGFlow ä¸Šå‚³ â†’ KuzuDB å¯«å…¥ â†’ Excel æ·±åº¦è§£æ â†’ ç¯€é»äº’é€£)
  - RAGFlow doc ID æ“·å–
  - KuzuDB ç¯€é»å¯«å…¥
  - Excel æ·±åº¦è§£æèˆ‡å­ç¯€é»å»ºç«‹
"""
import time
import logging
import hashlib
import json
from pathlib import Path
from typing import Optional
from datetime import datetime

import pandas as pd

from backend.rag_client import RAGFlowClient

logger = logging.getLogger(__name__)

# æœ€å¤§é‡è©¦æ¬¡æ•¸èˆ‡é€€é¿åŸºæ•¸
MAX_RETRY_ATTEMPTS = 3
RETRY_BACKOFF_BASE = 2  # seconds


# ================================================================
# RAGFlow ç›¸é—œ
# ================================================================

def extract_ragflow_doc_id(upload_result: dict) -> Optional[str]:
    """å¾ RAGFlow ä¸Šå‚³çµæœä¸­æå– document ID"""
    if not upload_result:
        return None
    data = upload_result.get('data')
    if isinstance(data, dict):
        return data.get('id')
    elif isinstance(data, list) and data:
        return data[0].get('id')
    return None


def extract_entity_id(file_path: Path, upload_result: dict) -> str:
    """
    æå–å¯¦é«” IDï¼ˆå„ªå…ˆä½¿ç”¨ RAGFlow å›å‚³çš„ IDï¼Œå¦å‰‡ä½¿ç”¨æª”æ¡ˆåç¨±çš„ Hashï¼‰
    """
    if 'data' in upload_result and upload_result['data']:
        doc_data = upload_result['data']
        if isinstance(doc_data, dict) and 'id' in doc_data:
            entity_id = doc_data['id']
            logger.debug(f"ä½¿ç”¨ RAGFlow å›å‚³çš„ ID: {entity_id}")
            return entity_id
        elif isinstance(doc_data, list) and len(doc_data) > 0 and 'id' in doc_data[0]:
            entity_id = doc_data[0]['id']
            logger.debug(f"ä½¿ç”¨ RAGFlow å›å‚³çš„ ID: {entity_id}")
            return entity_id

    file_hash = hashlib.md5(str(file_path.absolute()).encode()).hexdigest()
    entity_id = f"doc_{file_hash[:16]}"
    logger.debug(f"ä½¿ç”¨ Hash ID: {entity_id}")
    return entity_id


# ================================================================
# KuzuDB å¯«å…¥
# ================================================================

def add_to_graph(kuzu_manager, file_path: Path, upload_result: dict,
                 graph_id: str, dataset_id: str) -> Optional[str]:
    """
    å°‡æª”æ¡ˆè³‡è¨Šæ·»åŠ åˆ°çŸ¥è­˜åœ–è­œï¼ˆå‰µå»ºæª”æ¡ˆä¸»ç¯€é»ï¼‰

    Returns:
        æˆåŠŸè¿”å›ç¯€é» IDï¼Œå¤±æ•—æˆ– kuzu_manager ä¸å¯ç”¨è¿”å› None
    """
    if not kuzu_manager:
        logger.debug(f"â­ï¸  KuzuDB ä¸å¯ç”¨ï¼Œè·³éåœ–è­œå¯«å…¥: {file_path.name}")
        return None

    try:
        entity_id = extract_entity_id(file_path, upload_result)

        properties = {
            'path': str(file_path.absolute()),
            'size': file_path.stat().st_size,
            'extension': file_path.suffix.lower(),
            'created_time': file_path.stat().st_ctime,
            'dataset_id': dataset_id,
        }

        if 'data' in upload_result and upload_result['data']:
            doc_data = upload_result['data']
            if isinstance(doc_data, dict):
                properties['document_id'] = doc_data.get('id', entity_id)
            elif isinstance(doc_data, list) and len(doc_data) > 0:
                properties['document_id'] = doc_data[0].get('id', entity_id)

        logger.info(f"ğŸ“Š æ­£åœ¨å¯«å…¥çŸ¥è­˜åœ–è­œ: {file_path.name}")

        success = kuzu_manager.add_entity(
            entity_id=entity_id,
            name=file_path.name,
            entity_type='document',
            properties=properties,
            graph_id=graph_id,
        )

        if success:
            logger.info(f"âœ… åœ–è­œå¯«å…¥æˆåŠŸ: {file_path.name} (ID: {entity_id})")
            return entity_id
        else:
            logger.error(f"âŒ åœ–è­œå¯«å…¥å¤±æ•—: {file_path.name}")
            return None

    except Exception as e:
        logger.error(f"âŒ æ·»åŠ åˆ°åœ–è­œå¤±æ•—: {file_path.name} - {type(e).__name__}: {e}", exc_info=True)
        return None


# ================================================================
# Excel æ·±åº¦è§£æ
# ================================================================

def parse_excel_and_link(kuzu_manager, file_path: Path, file_node_id: str,
                         graph_id: str = "1", dataset_id: str = "") -> None:
    """
    è§£æ Excel æª”æ¡ˆï¼Œå°‡æ¯ä¸€åˆ—è³‡æ–™è½‰æ›ç‚ºç¨ç«‹çš„åœ–è­œå­ç¯€é»ï¼Œ
    ä¸¦å»ºç«‹èˆ‡ä¸»ç¯€é»çš„é€£ç·šã€‚
    """
    if not kuzu_manager:
        logger.debug(f"â­ï¸  KuzuDB ä¸å¯ç”¨ï¼Œè·³é Excel è§£æ: {file_path.name}")
        return

    try:
        logger.info(f"ğŸ“Š é–‹å§‹ Excel æ·±åº¦è§£æ: {file_path.name}")

        df = pd.read_excel(file_path)
        df.columns = df.columns.str.lower()

        required_columns = ['srl', 'title', 'link']
        missing_columns = [col for col in required_columns if col not in df.columns]

        has_description = 'description' in df.columns or 'distribtion' in df.columns
        if not has_description:
            missing_columns.append('description/distribtion')

        if missing_columns:
            logger.warning(
                f"âš ï¸  Excel æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ {missing_columns}ï¼Œè·³éæ·±åº¦è§£æ: {file_path.name}"
            )
            return

        logger.info(f"âœ… Excel æ ¼å¼é©—è­‰é€šéï¼ŒåŒ…å« {len(df)} åˆ—è³‡æ–™")

        success_count = 0
        error_count = 0
        link_count = 0

        for index, row in df.iterrows():
            try:
                srl = str(row.get('srl', '')).strip()
                title = str(row.get('title', '')).strip()
                link = str(row.get('link', '')).strip()
                description = str(row.get('description', row.get('distribtion', ''))).strip()

                if not srl or not title:
                    logger.debug(f"â­ï¸  è·³éç©ºç™½åˆ— {index + 1}")
                    continue

                child_node_id = f"{file_path.stem}_row_{srl}"

                properties = {
                    'srl': srl,
                    'link': link,
                    'description': description,
                    'source_file': file_path.name,
                    'row_index': index + 1,
                    'dataset_id': dataset_id,
                }

                node_success = kuzu_manager.add_entity(
                    entity_id=child_node_id,
                    name=title,
                    entity_type='Resource',
                    properties=properties,
                    graph_id=graph_id,
                )

                if node_success:
                    success_count += 1
                    logger.debug(f"âœ… åˆ— {index + 1} å­ç¯€é»å‰µå»ºæˆåŠŸ: {title} (ID: {child_node_id})")

                    link_success = kuzu_manager.add_relation(
                        source_id=file_node_id,
                        target_id=child_node_id,
                        relation_type="contains",
                        properties={'row': index + 1},
                    )
                    if link_success:
                        link_count += 1
                        logger.debug(f"ğŸ”— åˆ— {index + 1} é€£ç·šå‰µå»ºæˆåŠŸ: {file_node_id} -[contains]-> {child_node_id}")
                    else:
                        logger.warning(f"âš ï¸  åˆ— {index + 1} é€£ç·šå‰µå»ºå¤±æ•—")
                else:
                    error_count += 1
                    logger.warning(f"âš ï¸  åˆ— {index + 1} å­ç¯€é»å‰µå»ºå¤±æ•—: {title}")

            except Exception as e:
                error_count += 1
                logger.error(
                    f"âŒ è™•ç†åˆ— {index + 1} å¤±æ•—: {type(e).__name__}: {e}",
                    exc_info=True,
                )

        logger.info(
            f"ğŸ“Š Excel æ·±åº¦è§£æå®Œæˆ: {file_path.name} | "
            f"å­ç¯€é»: {success_count}, é€£ç·š: {link_count}, å¤±æ•—: {error_count}"
        )

    except pd.errors.EmptyDataError:
        logger.warning(f"âš ï¸  Excel æª”æ¡ˆç‚ºç©º: {file_path.name}")
    except pd.errors.ParserError as e:
        logger.error(f"âŒ Excel è§£æéŒ¯èª¤: {file_path.name} - {e}")
    except Exception as e:
        logger.error(
            f"âŒ Excel æ·±åº¦è§£æå¤±æ•—: {file_path.name} - {type(e).__name__}: {e}",
            exc_info=True,
        )


# ================================================================
# Saga ä¸»æµç¨‹
# ================================================================

def process_file(rag_client: RAGFlowClient, kuzu_manager, dataset_id: str,
                 file_path: Path, dlq, build_inter_node_links_fn=None) -> None:
    """
    è™•ç†æª”æ¡ˆï¼šä¸Šå‚³è‡³ RAGFlow ä¸¦æ›´æ–°çŸ¥è­˜åœ–è­œï¼ˆSaga è£œå„Ÿæ©Ÿåˆ¶ï¼‰

    Saga æµç¨‹ï¼š
      Step A: ä¸Šå‚³è‡³ RAGFlow (with retry + exponential backoff)
      Step B: å¯«å…¥ KuzuDB ä¸»ç¯€é» (å¤±æ•—æ™‚è£œå„Ÿ Step A)
      Step C: Excel æ·±åº¦è§£æ (å¯é¸ï¼Œå¤±æ•—ä¸è£œå„Ÿ)
      Step D: ç¯€é»äº’é€£ (å¯é¸ï¼Œå¤±æ•—ä¸è£œå„Ÿ)

    Args:
        rag_client: RAGFlow å®¢æˆ¶ç«¯
        kuzu_manager: KuzuDB ç®¡ç†å™¨
        dataset_id: RAGFlow çŸ¥è­˜åº« ID
        file_path: æª”æ¡ˆè·¯å¾‘
        dlq: DeadLetterQueue å¯¦ä¾‹
        build_inter_node_links_fn: ç¯€é»äº’é€£å›å‘¼ï¼ˆä¾†è‡ª node_linker æ¨¡çµ„ï¼‰
    """
    saga_steps = {}
    ragflow_doc_id = None
    kuzu_entity_id = None
    try:
        # è®€å–åœ–è­œå…ƒæ•¸æ“š + å†ªç­‰æ€§æª¢æŸ¥
        metadata_file = file_path.with_suffix(file_path.suffix + '.meta.json')
        graph_id = None

        if metadata_file.exists():
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    graph_id = metadata.get('graph_id')

                if metadata.get('processed') is True:
                    file_mtime = datetime.fromtimestamp(
                        file_path.stat().st_mtime
                    ).isoformat()
                    last_processed = metadata.get('last_processed_time', '')
                    if last_processed and file_mtime <= last_processed:
                        logger.info(f"â© å†ªç­‰æ€§è·³é (å·²è™•ç†ä¸”æœªä¿®æ”¹): {file_path.name}")
                        return
                    logger.info(
                        f"ğŸ”„ æª”æ¡ˆå·²ä¿®æ”¹ï¼Œé‡æ–°è™•ç†: {file_path.name} "
                        f"(mtime={file_mtime} > last={last_processed})"
                    )
                else:
                    logger.info(f"ğŸ“‹ è®€å–åœ–è­œå…ƒæ•¸æ“š: graph_id={graph_id}")
            except Exception as e:
                logger.warning(f"âš ï¸  è®€å–å…ƒæ•¸æ“šå¤±æ•—ï¼Œä½¿ç”¨é è¨­åœ–è­œ: {e}")

        if not graph_id:
            logger.error(f"âŒ ç„¡æ³•ç¢ºå®šç›®æ¨™åœ–è­œ IDï¼Œè·³éè™•ç†: {file_path.name}")
            return

        # â”€â”€ Step A: ä¸Šå‚³è‡³ RAGFlow (with retry) â”€â”€
        logger.info(f"ğŸ“¤ æ­£åœ¨ä¸Šå‚³æª”æ¡ˆè‡³ RAGFlow: {file_path.name}")
        upload_result = None

        for attempt in range(MAX_RETRY_ATTEMPTS):
            try:
                upload_result = rag_client.upload_file(
                    dataset_id=dataset_id,
                    file_path=str(file_path),
                )
                ragflow_doc_id = extract_ragflow_doc_id(upload_result)
                saga_steps["ragflow_upload"] = {
                    "status": "COMPLETED",
                    "doc_id": ragflow_doc_id,
                    "attempt": attempt + 1,
                }
                logger.info(f"âœ… RAGFlow ä¸Šå‚³æˆåŠŸ (attempt {attempt + 1}): {file_path.name}")
                break
            except Exception as upload_err:
                if attempt < MAX_RETRY_ATTEMPTS - 1:
                    backoff = RETRY_BACKOFF_BASE ** attempt
                    logger.warning(
                        f"âš ï¸  RAGFlow ä¸Šå‚³é‡è©¦ {attempt + 1}/{MAX_RETRY_ATTEMPTS} "
                        f"(backoff {backoff}s): {upload_err}"
                    )
                    time.sleep(backoff)
                else:
                    saga_steps["ragflow_upload"] = {
                        "status": "FAILED",
                        "error": str(upload_err),
                        "attempts": MAX_RETRY_ATTEMPTS,
                    }
                    logger.error(f"âŒ RAGFlow ä¸Šå‚³æœ€çµ‚å¤±æ•—: {file_path.name} - {upload_err}")
                    dlq.record(file_path, "ragflow_upload", str(upload_err),
                               graph_id=graph_id, saga_steps=saga_steps)
                    return

        logger.debug(f"ä¸Šå‚³å›æ‡‰: {upload_result}")

        # â”€â”€ Step B: å¯«å…¥çŸ¥è­˜åœ–è­œ â”€â”€
        kuzu_entity_id = add_to_graph(kuzu_manager, file_path, upload_result, graph_id, dataset_id)

        if kuzu_entity_id:
            saga_steps["kuzu_write"] = {"status": "COMPLETED", "entity_id": kuzu_entity_id}
        else:
            saga_steps["kuzu_write"] = {"status": "FAILED"}
            # è£œå„Ÿ Step A
            if ragflow_doc_id:
                try:
                    rag_client.delete_document(dataset_id=dataset_id, document_id=ragflow_doc_id)
                    logger.info(f"ğŸ”„ è£œå„Ÿå®Œæˆ: å·²æ’¤éŠ· RAGFlow ä¸Šå‚³ {ragflow_doc_id}")
                    saga_steps["compensation_ragflow_delete"] = {"status": "COMPLETED"}
                except Exception as comp_err:
                    logger.error(f"âŒ è£œå„Ÿå¤±æ•— (åˆªé™¤ RAGFlow æ–‡ä»¶): {comp_err}")
                    saga_steps["compensation_ragflow_delete"] = {
                        "status": "FAILED", "error": str(comp_err)
                    }
                    dlq.record(file_path, "compensation_ragflow_delete",
                               str(comp_err), ragflow_doc_id=ragflow_doc_id,
                               graph_id=graph_id, saga_steps=saga_steps)
            else:
                logger.warning("ragflow_doc_id ç‚ºç©ºï¼Œè·³é RAGFlow è£œå„Ÿ")
            return

        # â”€â”€ Step C: Excel æ·±åº¦è§£æ â”€â”€
        if file_path.suffix.lower() == '.xlsx' and kuzu_entity_id:
            try:
                parse_excel_and_link(kuzu_manager, file_path, kuzu_entity_id, graph_id, dataset_id)
                saga_steps["excel_parse"] = {"status": "COMPLETED"}
            except Exception as excel_err:
                saga_steps["excel_parse"] = {"status": "FAILED", "error": str(excel_err)}
                logger.error(f"âš ï¸  Excel è§£æå¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {excel_err}", exc_info=True)

        # â”€â”€ Step D: ç¯€é»äº’é€£ â”€â”€
        if kuzu_entity_id and build_inter_node_links_fn:
            try:
                inter_links = build_inter_node_links_fn(kuzu_manager, file_path, kuzu_entity_id, graph_id)
                saga_steps["inter_node_links"] = {"status": "COMPLETED", "links_created": inter_links}
            except Exception as link_err:
                saga_steps["inter_node_links"] = {"status": "FAILED", "error": str(link_err)}
                logger.error(f"âš ï¸  ç¯€é»äº’é€£å¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {link_err}", exc_info=True)

        # â”€â”€ å¯«å…¥å†ªç­‰æ€§å…ƒæ•¸æ“š â”€â”€
        try:
            meta_payload = {
                "graph_id": graph_id,
                "processed": True,
                "last_processed_time": datetime.now().isoformat(),
                "ragflow_id": ragflow_doc_id,
            }
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(meta_payload, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ å·²å¯«å…¥å†ªç­‰æ€§æ¨™è¨˜: {metadata_file.name}")
        except Exception as meta_err:
            logger.warning(f"âš ï¸  å¯«å…¥å…ƒæ•¸æ“šå¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {meta_err}")

        logger.info(f"âœ… Saga å®Œæˆ: {file_path.name} | steps={list(saga_steps.keys())}")

    except FileNotFoundError as e:
        logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path} - {e}")
    except Exception as e:
        logger.error(
            f"âŒ Saga å¤±æ•—: {file_path.name} - {type(e).__name__}: {e}",
            exc_info=True,
        )
        dlq.record(file_path, "saga_exception", str(e),
                    ragflow_doc_id=ragflow_doc_id,
                    kuzu_entity_id=kuzu_entity_id,
                    saga_steps=saga_steps)
