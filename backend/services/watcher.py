"""
AI æª”æ¡ˆç›£æ§æœå‹™
ç›£æ§æŒ‡å®šç›®éŒ„ï¼Œè‡ªå‹•ä¸Šå‚³æ–°æª”æ¡ˆè‡³ RAGFlow ä¸¦åŒæ­¥è‡³çŸ¥è­˜åœ–è­œ
æ”¯æ´è£œå„Ÿæ©Ÿåˆ¶ (Compensation) ç¢ºä¿è·¨ç³»çµ±è³‡æ–™ä¸€è‡´æ€§
"""
import os
import time
import logging
import hashlib
import uuid
import asyncio
import json
import sqlite3
from pathlib import Path
from typing import Optional, Set, Union
from datetime import datetime
import pandas as pd
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileSystemEvent

from backend.rag_client import RAGFlowClient
from backend.core.kuzu_manager import KuzuDBManager, MockKuzuManager, AsyncKuzuWrapper
from backend.services.task_queue import task_queue, TaskStatus

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

# DLQ (Dead Letter Queue) è·¯å¾‘
_DATA_DIR = Path(os.environ.get("BRUV_DATA_DIR", str(Path.home() / "BruV_Data")))
_DLQ_DB_PATH = _DATA_DIR / "saga_dlq.db"

# æœ€å¤§é‡è©¦æ¬¡æ•¸èˆ‡é€€é¿åŸºæ•¸
MAX_RETRY_ATTEMPTS = 3
RETRY_BACKOFF_BASE = 2  # seconds


class DeadLetterQueue:
    """
    æ­»ä¿¡ä½‡åˆ— â€” è¨˜éŒ„è£œå„Ÿå¤±æ•—æˆ–è™•ç†å¤±æ•—çš„æ“ä½œï¼Œ
    ä¾›ç®¡ç†å“¡é€é /api/system/saga-dlq ç«¯é»æ‰‹å‹•é‡è©¦æˆ–ç¢ºèªã€‚
    """

    def __init__(self, db_path: Path = _DLQ_DB_PATH):
        self._db_path = db_path
        db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_db()

    def _get_conn(self) -> sqlite3.Connection:
        conn = sqlite3.connect(str(self._db_path), timeout=5)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS dlq (
                    id TEXT PRIMARY KEY,
                    file_path TEXT,
                    file_name TEXT,
                    graph_id TEXT,
                    failed_step TEXT,
                    error_message TEXT,
                    ragflow_doc_id TEXT,
                    kuzu_entity_id TEXT,
                    saga_steps TEXT,
                    created_at TEXT,
                    resolved BOOLEAN DEFAULT 0,
                    resolved_at TEXT
                )
            """)

    def record(self, file_path: Path, failed_step: str, error: str,
               ragflow_doc_id: str = None, kuzu_entity_id: str = None,
               graph_id: str = None, saga_steps: dict = None):
        """è¨˜éŒ„åˆ° DLQ"""
        try:
            with self._get_conn() as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO dlq
                    (id, file_path, file_name, graph_id, failed_step,
                     error_message, ragflow_doc_id, kuzu_entity_id,
                     saga_steps, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    str(uuid.uuid4()),
                    str(file_path),
                    file_path.name if isinstance(file_path, Path) else str(file_path),
                    graph_id,
                    failed_step,
                    error,
                    ragflow_doc_id,
                    kuzu_entity_id,
                    json.dumps(saga_steps or {}, ensure_ascii=False),
                    datetime.now().isoformat(),
                ))
            logger.warning(f"ğŸ“¥ å·²è¨˜éŒ„åˆ° DLQ: {file_path} (step={failed_step})")
        except Exception as e:
            logger.error(f"âŒ DLQ å¯«å…¥å¤±æ•—: {e}")

    def list_unresolved(self, limit: int = 50) -> list:
        """åˆ—å‡ºæœªè§£æ±ºçš„ DLQ é …ç›®"""
        try:
            with self._get_conn() as conn:
                rows = conn.execute(
                    "SELECT * FROM dlq WHERE resolved = 0 ORDER BY created_at DESC LIMIT ?",
                    (limit,)
                ).fetchall()
                return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"âŒ DLQ æŸ¥è©¢å¤±æ•—: {e}")
            return []

    def mark_resolved(self, dlq_id: str) -> bool:
        """æ¨™è¨˜ç‚ºå·²è§£æ±º"""
        try:
            with self._get_conn() as conn:
                conn.execute(
                    "UPDATE dlq SET resolved = 1, resolved_at = ? WHERE id = ?",
                    (datetime.now().isoformat(), dlq_id)
                )
            return True
        except Exception as e:
            logger.error(f"âŒ DLQ æ¨™è¨˜å¤±æ•—: {e}")
            return False


# å…¨åŸŸ DLQ å¯¦ä¾‹
dlq = DeadLetterQueue()


class AIFileEventHandler(FileSystemEventHandler):
    """AI æª”æ¡ˆäº‹ä»¶è™•ç†å™¨"""
    
    # æ”¯æ´çš„æª”æ¡ˆå‰¯æª”å
    SUPPORTED_EXTENSIONS: Set[str] = {'.pdf', '.txt', '.md', '.docx', '.xlsx'}
    
    def __init__(
        self, 
        rag_client: RAGFlowClient, 
        kuzu_manager: Optional[Union[KuzuDBManager, MockKuzuManager, AsyncKuzuWrapper]],
        dataset_id: str
    ):
        """
        åˆå§‹åŒ–æª”æ¡ˆäº‹ä»¶è™•ç†å™¨
        
        Args:
            rag_client: RAGFlow å®¢æˆ¶ç«¯å¯¦ä¾‹
            kuzu_manager: KuzuDB ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆæ”¯æŒ KuzuDBManagerã€MockKuzuManagerã€AsyncKuzuWrapper æˆ– Noneï¼‰
            dataset_id: RAGFlow çŸ¥è­˜åº« ID
        """
        super().__init__()
        self.rag_client = rag_client
        self.kuzu_manager = kuzu_manager
        self.dataset_id = dataset_id
        logger.info(f"âœ… AIFileEventHandler åˆå§‹åŒ–å®Œæˆï¼Œç›®æ¨™çŸ¥è­˜åº«: {dataset_id}")
    
    def on_created(self, event: FileSystemEvent) -> None:
        """
        ç•¶åµæ¸¬åˆ°æ–°æª”æ¡ˆå‰µå»ºæ™‚è§¸ç™¼
        
        Args:
            event: æª”æ¡ˆç³»çµ±äº‹ä»¶
        """
        # éæ¿¾æ‰è³‡æ–™å¤¾
        if event.is_directory:
            logger.debug(f"â­ï¸  å¿½ç•¥ç›®éŒ„: {event.src_path}")
            return
        
        file_path = Path(event.src_path)
        
        # å¿½ç•¥å…ƒæ•¸æ“šæ–‡ä»¶
        if file_path.suffix == '.json' and '.meta.json' in file_path.name:
            logger.debug(f"â­ï¸  å¿½ç•¥å…ƒæ•¸æ“šæ–‡ä»¶: {file_path.name}")
            return
        
        file_extension = file_path.suffix.lower()
        
        # æª¢æŸ¥å‰¯æª”åæ˜¯å¦æ”¯æ´
        if file_extension not in self.SUPPORTED_EXTENSIONS:
            logger.debug(f"â­ï¸  å¿½ç•¥ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {file_path.name} ({file_extension})")
            return
        
        logger.info(f"ğŸ” åµæ¸¬åˆ°æ–°æª”æ¡ˆ: {file_path}")
        
        # ç­‰å¾…æª”æ¡ˆå®Œå…¨å¯«å…¥ï¼ˆä½¿ç”¨éé˜»å¡ç¡çœ æ›¿ä»£ time.sleepï¼‰
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # åœ¨ async ç’°å¢ƒä¸‹ï¼Œä½¿ç”¨ thread-safe æ–¹å¼ç­‰å¾…
                import time as _time
                _time.sleep(1)  # watchdog å›å‘¼åœ¨ç¨ç«‹ç·šç¨‹ï¼Œtime.sleep ä¸é˜»å¡äº‹ä»¶è¿´åœˆ
            else:
                time.sleep(1)
        except RuntimeError:
            time.sleep(1)
        
        # è™•ç†æª”æ¡ˆä¸Šå‚³èˆ‡åœ–è­œæ›´æ–°
        self._process_file(file_path)
    
    def _process_file(self, file_path: Path) -> None:
        """
        è™•ç†æª”æ¡ˆï¼šä¸Šå‚³è‡³ RAGFlow ä¸¦æ›´æ–°çŸ¥è­˜åœ–è­œ
        å¼•å…¥ Saga è£œå„Ÿæ©Ÿåˆ¶ï¼Œç¢ºä¿ RAGFlow â†” KuzuDB çš„è·¨ç³»çµ±ä¸€è‡´æ€§ã€‚
        
        Saga æµç¨‹ï¼š
          Step A: ä¸Šå‚³è‡³ RAGFlow (with retry + exponential backoff)
          Step B: å¯«å…¥ KuzuDB ä¸»ç¯€é» (å¤±æ•—æ™‚è£œå„Ÿ Step A)
          Step C: Excel æ·±åº¦è§£æ (å¯é¸ï¼Œå¤±æ•—ä¸è£œå„Ÿ)
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
        """
        saga_steps = {}
        ragflow_doc_id = None
        kuzu_entity_id = None
        try:
            # è®€å–åœ–è­œå…ƒæ•¸æ“š + å†ªç­‰æ€§æª¢æŸ¥
            metadata_file = file_path.with_suffix(file_path.suffix + '.meta.json')
            graph_id = None  # å¿…é ˆå¾ meta.json è®€å–
            
            if metadata_file.exists():
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                        graph_id = metadata.get('graph_id')

                    # å†ªç­‰æ€§æª¢æŸ¥ï¼šå·²è™•ç† + æª”æ¡ˆæœªä¿®æ”¹ â†’ è·³é
                    if metadata.get('processed') is True:
                        file_mtime = datetime.fromtimestamp(
                            file_path.stat().st_mtime
                        ).isoformat()
                        last_processed = metadata.get('last_processed_time', '')
                        if last_processed and file_mtime <= last_processed:
                            logger.info(
                                f"â© å†ªç­‰æ€§è·³é (å·²è™•ç†ä¸”æœªä¿®æ”¹): {file_path.name}"
                            )
                            return
                        logger.info(
                            f"ğŸ”„ æª”æ¡ˆå·²ä¿®æ”¹ï¼Œé‡æ–°è™•ç†: {file_path.name} "
                            f"(mtime={file_mtime} > last={last_processed})"
                        )
                    else:
                        logger.info(f"ğŸ“‹ è®€å–åœ–è­œå…ƒæ•¸æ“š: graph_id={graph_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸  è®€å–å…ƒæ•¸æ“šå¤±æ•—ï¼Œä½¿ç”¨é è¨­åœ–è­œ: {e}")
            
            # graph_id å®‰å…¨æª¢æŸ¥
            if not graph_id:
                logger.error(f"âŒ ç„¡æ³•ç¢ºå®šç›®æ¨™åœ–è­œ IDï¼Œè·³éè™•ç†: {file_path.name}")
                return
            
            # â”€â”€ Step A: ä¸Šå‚³æª”æ¡ˆè‡³ RAGFlow (with retry) â”€â”€
            logger.info(f"ğŸ“¤ æ­£åœ¨ä¸Šå‚³æª”æ¡ˆè‡³ RAGFlow: {file_path.name}")
            upload_result = None

            for attempt in range(MAX_RETRY_ATTEMPTS):
                try:
                    upload_result = self.rag_client.upload_file(
                        dataset_id=self.dataset_id,
                        file_path=str(file_path)
                    )
                    ragflow_doc_id = self._extract_ragflow_doc_id(upload_result)
                    saga_steps["ragflow_upload"] = {
                        "status": "COMPLETED",
                        "doc_id": ragflow_doc_id,
                        "attempt": attempt + 1
                    }
                    logger.info(f"âœ… RAGFlow ä¸Šå‚³æˆåŠŸ (attempt {attempt + 1}): {file_path.name}")
                    break
                except Exception as upload_err:
                    if attempt < MAX_RETRY_ATTEMPTS - 1:
                        backoff = RETRY_BACKOFF_BASE ** attempt
                        logger.warning(
                            f"âš ï¸  RAGFlow ä¸Šå‚³é‡è©¦ {attempt + 1}/{MAX_RETRY_ATTEMPTS} "
                            f"(backoff {backoff}s): {upload_err}"
                        )
                        time.sleep(backoff)  # watchdog å›å‘¼åœ¨ç¨ç«‹ç·šç¨‹
                    else:
                        saga_steps["ragflow_upload"] = {
                            "status": "FAILED",
                            "error": str(upload_err),
                            "attempts": MAX_RETRY_ATTEMPTS
                        }
                        logger.error(f"âŒ RAGFlow ä¸Šå‚³æœ€çµ‚å¤±æ•—: {file_path.name} - {upload_err}")
                        dlq.record(file_path, "ragflow_upload", str(upload_err),
                                   graph_id=graph_id, saga_steps=saga_steps)
                        return  # ä¸Šå‚³å¤±æ•— â†’ ä¸é€²è¡Œå¾ŒçºŒæ­¥é©Ÿ
            
            logger.debug(f"ä¸Šå‚³å›æ‡‰: {upload_result}")
            
            # â”€â”€ Step B: å¯«å…¥çŸ¥è­˜åœ–è­œï¼ˆä½¿ç”¨åœ–è­œ IDï¼‰â”€â”€
            kuzu_entity_id = self._add_to_graph(file_path, upload_result, graph_id)

            if kuzu_entity_id:
                saga_steps["kuzu_write"] = {
                    "status": "COMPLETED",
                    "entity_id": kuzu_entity_id
                }
            else:
                saga_steps["kuzu_write"] = {"status": "FAILED"}
                # â”€â”€ è£œå„Ÿ Step A: åˆªé™¤å·²ä¸Šå‚³çš„ RAGFlow æ–‡ä»¶ â”€â”€
                if ragflow_doc_id:
                    try:
                        self.rag_client.delete_document(
                            dataset_id=self.dataset_id,
                            document_id=ragflow_doc_id
                        )
                        logger.info(
                            f"ğŸ”„ è£œå„Ÿå®Œæˆ: å·²æ’¤éŠ· RAGFlow ä¸Šå‚³ {ragflow_doc_id} "
                            f"(å›  KuzuDB å¯«å…¥å¤±æ•—)"
                        )
                        saga_steps["compensation_ragflow_delete"] = {"status": "COMPLETED"}
                    except Exception as comp_err:
                        logger.error(f"âŒ è£œå„Ÿå¤±æ•— (åˆªé™¤ RAGFlow æ–‡ä»¶): {comp_err}")
                        saga_steps["compensation_ragflow_delete"] = {
                            "status": "FAILED",
                            "error": str(comp_err)
                        }
                        dlq.record(file_path, "compensation_ragflow_delete",
                                   str(comp_err), ragflow_doc_id=ragflow_doc_id,
                                   graph_id=graph_id, saga_steps=saga_steps)
                else:
                    logger.warning("ragflow_doc_id ç‚ºç©ºï¼Œè·³é RAGFlow è£œå„Ÿ")
                return  # KuzuDB å¯«å…¥å¤±æ•— â†’ çµæŸ
            
            # â”€â”€ Step C: Excel æ·±åº¦è§£æ (å¯é¸ï¼Œå¤±æ•—ä¸éœ€è£œå„Ÿ) â”€â”€
            if file_path.suffix.lower() == '.xlsx' and kuzu_entity_id:
                try:
                    self._parse_excel_and_link(file_path, kuzu_entity_id, graph_id)
                    saga_steps["excel_parse"] = {"status": "COMPLETED"}
                except Exception as excel_err:
                    saga_steps["excel_parse"] = {
                        "status": "FAILED",
                        "error": str(excel_err)
                    }
                    logger.error(
                        f"âš ï¸  Excel è§£æå¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {excel_err}",
                        exc_info=True
                    )

            # â”€â”€ Step D: ç¯€é»äº’é€£ (åŸºæ–¼ domain + é—œéµå­—å…±ç¾) â”€â”€
            if kuzu_entity_id:
                try:
                    inter_links = self._build_inter_node_links(
                        file_path, kuzu_entity_id, graph_id
                    )
                    saga_steps["inter_node_links"] = {
                        "status": "COMPLETED",
                        "links_created": inter_links
                    }
                except Exception as link_err:
                    saga_steps["inter_node_links"] = {
                        "status": "FAILED",
                        "error": str(link_err)
                    }
                    logger.error(
                        f"âš ï¸  ç¯€é»äº’é€£å¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {link_err}",
                        exc_info=True
                    )

            # â”€â”€ å¯«å…¥å†ªç­‰æ€§å…ƒæ•¸æ“š (.meta.json) â”€â”€
            try:
                meta_payload = {
                    "graph_id": graph_id,
                    "processed": True,
                    "last_processed_time": datetime.now().isoformat(),
                    "ragflow_id": ragflow_doc_id,
                }
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(meta_payload, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ å·²å¯«å…¥å†ªç­‰æ€§æ¨™è¨˜: {metadata_file.name}")
            except Exception as meta_err:
                logger.warning(f"âš ï¸  å¯«å…¥å…ƒæ•¸æ“šå¤±æ•— (ä¸å½±éŸ¿ä¸»æµç¨‹): {meta_err}")

            logger.info(f"âœ… Saga å®Œæˆ: {file_path.name} | steps={list(saga_steps.keys())}")
            
        except FileNotFoundError as e:
            logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path} - {e}")
        except Exception as e:
            logger.error(
                f"âŒ Saga å¤±æ•—: {file_path.name} - {type(e).__name__}: {e}",
                exc_info=True
            )
            dlq.record(file_path, "saga_exception", str(e),
                       ragflow_doc_id=ragflow_doc_id,
                       kuzu_entity_id=kuzu_entity_id,
                       saga_steps=saga_steps)
    
    def _extract_ragflow_doc_id(self, upload_result: dict) -> Optional[str]:
        """å¾ RAGFlow ä¸Šå‚³çµæœä¸­æå– document ID"""
        if not upload_result:
            return None
        data = upload_result.get('data')
        if isinstance(data, dict):
            return data.get('id')
        elif isinstance(data, list) and data:
            return data[0].get('id')
        return None
    
    def _add_to_graph(self, file_path: Path, upload_result: dict, graph_id: str = None) -> Optional[str]:
        """
        å°‡æª”æ¡ˆè³‡è¨Šæ·»åŠ åˆ°çŸ¥è­˜åœ–è­œï¼ˆå‰µå»ºæª”æ¡ˆä¸»ç¯€é»ï¼‰
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            upload_result: RAGFlow ä¸Šå‚³å›æ‡‰
            graph_id: ç›®æ¨™åœ–è­œ ID
            
        Returns:
            Optional[str]: æˆåŠŸè¿”å›ç¯€é» IDï¼Œå¤±æ•—è¿”å› None
        """
        # å¦‚æœ KuzuDB ä¸å¯ç”¨ï¼Œè·³éåœ–è­œå¯«å…¥
        if not self.kuzu_manager:
            logger.debug(f"â­ï¸  KuzuDB ä¸å¯ç”¨ï¼Œè·³éåœ–è­œå¯«å…¥: {file_path.name}")
            return None
        
        try:
            # æå– Entity IDï¼ˆå„ªå…ˆä½¿ç”¨ RAGFlow å›å‚³çš„ IDï¼‰
            entity_id = self._extract_entity_id(file_path, upload_result)
            
            # æº–å‚™å¯¦é«”å±¬æ€§
            properties = {
                'path': str(file_path.absolute()),
                'size': file_path.stat().st_size,
                'extension': file_path.suffix.lower(),
                'created_time': file_path.stat().st_ctime,
                'dataset_id': self.dataset_id
            }
            
            # å¦‚æœ RAGFlow å›å‚³äº†æ–‡æª” IDï¼Œä¹ŸåŠ å…¥å±¬æ€§
            if 'data' in upload_result and upload_result['data']:
                doc_data = upload_result['data']
                if isinstance(doc_data, dict):
                    properties['document_id'] = doc_data.get('id', entity_id)
                elif isinstance(doc_data, list) and len(doc_data) > 0:
                    properties['document_id'] = doc_data[0].get('id', entity_id)
            
            logger.info(f"ğŸ“Š æ­£åœ¨å¯«å…¥çŸ¥è­˜åœ–è­œ: {file_path.name}")
            
            # æ·»åŠ å¯¦é«”åˆ°åœ–è­œï¼ˆä½¿ç”¨ graph_idï¼‰
            success = self.kuzu_manager.add_entity(
                entity_id=entity_id,
                name=file_path.name,
                entity_type='document',
                properties=properties,
                graph_id=graph_id  # å‚³éåœ–è­œ ID
            )
            
            if success:
                logger.info(f"âœ… åœ–è­œå¯«å…¥æˆåŠŸ: {file_path.name} (ID: {entity_id})")
                return entity_id  # è¿”å›ç¯€é» ID
            else:
                logger.error(f"âŒ åœ–è­œå¯«å…¥å¤±æ•—: {file_path.name}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ åˆ°åœ–è­œå¤±æ•—: {file_path.name} - {type(e).__name__}: {e}", exc_info=True)
            return None
    
    def _extract_entity_id(self, file_path: Path, upload_result: dict) -> str:
        """
        æå–å¯¦é«” IDï¼ˆå„ªå…ˆä½¿ç”¨ RAGFlow å›å‚³çš„ IDï¼Œå¦å‰‡ä½¿ç”¨æª”æ¡ˆåç¨±çš„ Hashï¼‰
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            upload_result: RAGFlow ä¸Šå‚³å›æ‡‰
            
        Returns:
            å¯¦é«” ID
        """
        # å˜—è©¦å¾ RAGFlow å›æ‡‰ä¸­æå– ID
        if 'data' in upload_result and upload_result['data']:
            doc_data = upload_result['data']
            
            # è™•ç†å›æ‡‰å¯èƒ½æ˜¯å­—å…¸æˆ–åˆ—è¡¨çš„æƒ…æ³
            if isinstance(doc_data, dict) and 'id' in doc_data:
                entity_id = doc_data['id']
                logger.debug(f"ä½¿ç”¨ RAGFlow å›å‚³çš„ ID: {entity_id}")
                return entity_id
            elif isinstance(doc_data, list) and len(doc_data) > 0 and 'id' in doc_data[0]:
                entity_id = doc_data[0]['id']
                logger.debug(f"ä½¿ç”¨ RAGFlow å›å‚³çš„ ID: {entity_id}")
                return entity_id
        
        # å¦å‰‡ä½¿ç”¨æª”æ¡ˆåç¨±çš„ Hash
        file_hash = hashlib.md5(str(file_path.absolute()).encode()).hexdigest()
        entity_id = f"doc_{file_hash[:16]}"
        logger.debug(f"ä½¿ç”¨ Hash ID: {entity_id}")
        return entity_id
    
    def _parse_excel_and_link(self, file_path: Path, file_node_id: str, graph_id: str = "1") -> None:
        """
        è§£æ Excel æª”æ¡ˆï¼Œå°‡æ¯ä¸€åˆ—è³‡æ–™è½‰æ›ç‚ºç¨ç«‹çš„åœ–è­œå­ç¯€é»ï¼Œä¸¦å»ºç«‹èˆ‡ä¸»ç¯€é»çš„é€£ç·š
        
        Args:
            file_path: Excel æª”æ¡ˆè·¯å¾‘
            file_node_id: æª”æ¡ˆä¸»ç¯€é»çš„ ID
            graph_id: ç›®æ¨™åœ–è­œ ID
        """
        # å¦‚æœ KuzuDB ä¸å¯ç”¨ï¼Œè·³éè™•ç†
        if not self.kuzu_manager:
            logger.debug(f"â­ï¸  KuzuDB ä¸å¯ç”¨ï¼Œè·³é Excel è§£æ: {file_path.name}")
            return
        
        try:
            logger.info(f"ğŸ“Š é–‹å§‹ Excel æ·±åº¦è§£æ: {file_path.name}")
            
            # è®€å– Excel æª”æ¡ˆ
            df = pd.read_excel(file_path)
            
            # å°‡æ¬„ä½åç¨±è½‰ç‚ºå°å¯«ä»¥é€²è¡Œä¸å€åˆ†å¤§å°å¯«çš„åŒ¹é…
            df.columns = df.columns.str.lower()
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å¿…è¦æ¬„ä½ï¼ˆæ”¯æŒ description å’Œ distribtionï¼‰
            required_columns = ['srl', 'title', 'link']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ description æˆ– distribtion
            has_description = 'description' in df.columns or 'distribtion' in df.columns
            if not has_description:
                missing_columns.append('description/distribtion')
            
            if missing_columns:
                logger.warning(
                    f"âš ï¸  Excel æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ {missing_columns}ï¼Œè·³éæ·±åº¦è§£æ: {file_path.name}"
                )
                return
            
            logger.info(f"âœ… Excel æ ¼å¼é©—è­‰é€šéï¼ŒåŒ…å« {len(df)} åˆ—è³‡æ–™")
            
            # éæ­·æ¯ä¸€åˆ—ï¼Œå‰µå»ºå­ç¯€é»ä¸¦å»ºç«‹é€£ç·š
            success_count = 0
            error_count = 0
            link_count = 0
            
            for index, row in df.iterrows():
                try:
                    # æå–æ¬„ä½å€¼
                    srl = str(row.get('srl', '')).strip()
                    title = str(row.get('title', '')).strip()
                    link = str(row.get('link', '')).strip()
                    # æ”¯æŒ description æˆ– distribtion
                    description = str(row.get('description', row.get('distribtion', ''))).strip()
                    
                    # è·³éç©ºç™½åˆ—
                    if not srl or not title:
                        logger.debug(f"â­ï¸  è·³éç©ºç™½åˆ— {index + 1}")
                        continue
                    
                    # ç”Ÿæˆå”¯ä¸€çš„å­ç¯€é» ID
                    child_node_id = f"{file_path.stem}_row_{srl}"
                    
                    # æº–å‚™ç¯€é»å±¬æ€§ï¼ˆå°‡ link å’Œ description æ”¾å…¥ propertiesï¼‰
                    properties = {
                        'srl': srl,
                        'link': link,
                        'description': description,
                        'source_file': file_path.name,
                        'row_index': index + 1,
                        'dataset_id': self.dataset_id
                    }
                    
                    # æ­¥é©Ÿ 1: å‰µå»ºå­ç¯€é»ï¼ˆä½¿ç”¨ graph_idï¼‰
                    node_success = self.kuzu_manager.add_entity(
                        entity_id=child_node_id,
                        name=title,
                        entity_type='Resource',  # å°æ‡‰å‰ç«¯é…è‰²
                        properties=properties,
                        graph_id=graph_id  # å‚³éåœ–è­œ ID
                    )
                    
                    if node_success:
                        success_count += 1
                        logger.debug(f"âœ… åˆ— {index + 1} å­ç¯€é»å‰µå»ºæˆåŠŸ: {title} (ID: {child_node_id})")
                        
                        # æ­¥é©Ÿ 2: å»ºç«‹é€£ç·š (æª”æ¡ˆä¸»ç¯€é» -> å­ç¯€é»)
                        link_success = self.kuzu_manager.add_relation(
                            source_id=file_node_id,
                            target_id=child_node_id,
                            relation_type="contains",
                            properties={'row': index + 1}
                        )
                        
                        if link_success:
                            link_count += 1
                            logger.debug(f"ğŸ”— åˆ— {index + 1} é€£ç·šå‰µå»ºæˆåŠŸ: {file_node_id} -[contains]-> {child_node_id}")
                        else:
                            logger.warning(f"âš ï¸  åˆ— {index + 1} é€£ç·šå‰µå»ºå¤±æ•—")
                    else:
                        error_count += 1
                        logger.warning(f"âš ï¸  åˆ— {index + 1} å­ç¯€é»å‰µå»ºå¤±æ•—: {title}")
                        
                except Exception as e:
                    error_count += 1
                    logger.error(
                        f"âŒ è™•ç†åˆ— {index + 1} å¤±æ•—: {type(e).__name__}: {e}",
                        exc_info=True
                    )
            
            logger.info(
                f"ğŸ“Š Excel æ·±åº¦è§£æå®Œæˆ: {file_path.name} | "
                f"å­ç¯€é»: {success_count}, é€£ç·š: {link_count}, å¤±æ•—: {error_count}"
            )
            
        except pd.errors.EmptyDataError:
            logger.warning(f"âš ï¸  Excel æª”æ¡ˆç‚ºç©º: {file_path.name}")
        except pd.errors.ParserError as e:
            logger.error(f"âŒ Excel è§£æéŒ¯èª¤: {file_path.name} - {e}")
        except Exception as e:
            logger.error(
                f"âŒ Excel æ·±åº¦è§£æå¤±æ•—: {file_path.name} - {type(e).__name__}: {e}",
                exc_info=True
            )

    def _build_inter_node_links(self, file_path: Path, file_node_id: str, graph_id: str = "1") -> int:
        """
        åˆ†æåŒä¸€åœ–è­œä¸­çš„ Resource ç¯€é»ï¼Œæ ¹æ“šä»¥ä¸‹è¦å‰‡å»ºç«‹äº’é€£ï¼š
          1. Link Domain æ­¸é¡ â€” ç›¸åŒç¶²åŸŸçš„è³‡æºäº’ç›¸é€£ç·š (same_domain)
          2. é—œéµå­—å…±ç¾ â€” title/description ä¸­çš„å…±åŒé—œéµå­— (keyword_overlap)
        
        Args:
            file_path: ä¾†æºæª”æ¡ˆï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
            file_node_id: æª”æ¡ˆä¸»ç¯€é» ID
            graph_id: ç›®æ¨™åœ–è­œ ID
            
        Returns:
            å»ºç«‹çš„é€£ç·šæ•¸é‡
        """
        if not self.kuzu_manager:
            return 0

        try:
            from urllib.parse import urlparse
            import re

            logger.info(f"ğŸ”— é–‹å§‹å»ºç«‹ç¯€é»äº’é€£: graph_id={graph_id}")

            # æŸ¥è©¢åŒåœ–è­œçš„æ‰€æœ‰ Resource ç¯€é»
            entities = self.kuzu_manager.query("""
                MATCH (e:Entity {graph_id: $graph_id})
                WHERE e.type = 'Resource'
                RETURN e.id AS id, e.name AS name, e.properties AS properties
            """, parameters={"graph_id": graph_id})

            if len(entities) < 2:
                logger.info(f"â­ï¸  ç¯€é»æ•¸é‡ä¸è¶³ ({len(entities)})ï¼Œè·³éäº’é€£")
                return 0

            # æŸ¥è©¢å·²å­˜åœ¨çš„æ‰€æœ‰é€£ç·šï¼Œé¿å…é‡è¤‡å»ºç«‹
            existing_relations = set()
            try:
                rels = self.kuzu_manager.query("""
                    MATCH (a:Entity {graph_id: $graph_id})-[:Relation]->(b:Entity {graph_id: $graph_id})
                    RETURN a.id AS src, b.id AS dst
                """, parameters={"graph_id": graph_id})
                for r in rels:
                    existing_relations.add((r['src'], r['dst']))
                    existing_relations.add((r['dst'], r['src']))  # é›™å‘é¿å…é‡è¤‡
            except Exception:
                pass  # æŸ¥è©¢å¤±æ•—ä¸ä¸­æ–·

            # â”€â”€ ç¬¬ 1 å±¤ï¼šLink Domain æ­¸é¡ â”€â”€
            domain_groups = {}  # domain â†’ [entity_id, ...]
            entity_map = {}     # id â†’ {name, link, description, keywords}

            for e in entities:
                eid = e['id']
                name = e.get('name', '')
                props_raw = e.get('properties', '{}')
                
                # è§£æ properties å­—ä¸²
                try:
                    if isinstance(props_raw, str):
                        props = eval(props_raw) if props_raw.startswith('{') else {}
                    else:
                        props = props_raw or {}
                except Exception:
                    props = {}

                link = props.get('link', '')
                description = props.get('description', '')

                entity_map[eid] = {
                    'name': name,
                    'link': link,
                    'description': description
                }

                # æå– domain
                if link:
                    try:
                        parsed = urlparse(link)
                        domain = parsed.netloc.replace('www.', '').lower()
                        if domain:
                            domain_groups.setdefault(domain, []).append(eid)
                    except Exception:
                        pass

            # å»ºç«‹åŒ domain é€£ç·š
            link_count = 0
            for domain, ids in domain_groups.items():
                if len(ids) < 2:
                    continue
                # é™åˆ¶åŒ domain æœ€å¤šå»ºç«‹ 20 æ¢é€£ç·šï¼Œé¿å…çˆ†ç‚¸
                pairs = []
                for i in range(len(ids)):
                    for j in range(i + 1, min(len(ids), i + 5)):
                        pairs.append((ids[i], ids[j]))
                
                for src_id, dst_id in pairs[:20]:
                    if (src_id, dst_id) in existing_relations:
                        continue
                    success = self.kuzu_manager.add_relation(
                        source_id=src_id,
                        target_id=dst_id,
                        relation_type="same_domain",
                        properties={'domain': domain, 'auto': True}
                    )
                    if success:
                        link_count += 1
                        existing_relations.add((src_id, dst_id))
                        existing_relations.add((dst_id, src_id))

            logger.info(f"ğŸŒ Domain æ­¸é¡é€£ç·š: {link_count} æ¢")

            # â”€â”€ ç¬¬ 2 å±¤ï¼šé—œéµå­—å…±ç¾åˆ†æ â”€â”€
            # å¾ title + description æå–é—œéµå­—ï¼Œæ‰¾å…±åŒè©çš„ç¯€é»äº’é€£
            stopwords = {
                'the', 'a', 'an', 'is', 'are', 'was', 'were', 'in', 'on', 'at',
                'to', 'for', 'of', 'and', 'or', 'not', 'with', 'by', 'from',
                'this', 'that', 'it', 'its', 'be', 'has', 'have', 'had', 'do',
                'does', 'did', 'but', 'if', 'as', 'no', 'so', 'up', 'out',
                'about', 'into', 'than', 'then', 'can', 'will', 'just',
                'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'èˆ‡', 'æˆ–', 'ä¸', 'æœ‰', 'ä¹Ÿ',
                'éƒ½', 'è¦', 'æœƒ', 'æŠŠ', 'è¢«', 'è®“', 'é€™', 'é‚£', 'å°±',
                'post', 'page', 'http', 'https', 'www', 'com'
            }

            def extract_keywords(text: str) -> set:
                if not text:
                    return set()
                # ä¸­æ–‡ï¼š2å­—ä»¥ä¸Šçš„è©ï¼›è‹±æ–‡ï¼š3å­—ä»¥ä¸Šçš„è©
                words = re.findall(r'[\u4e00-\u9fff]{2,}|[a-zA-Z]{3,}', text.lower())
                return {w for w in words if w not in stopwords and len(w) <= 20}

            entity_keywords = {}
            for eid, info in entity_map.items():
                kws = extract_keywords(info['name']) | extract_keywords(info['description'])
                if kws:
                    entity_keywords[eid] = kws

            keyword_link_count = 0
            eids = list(entity_keywords.keys())
            for i in range(len(eids)):
                for j in range(i + 1, len(eids)):
                    if keyword_link_count >= 100:  # é™åˆ¶æœ€å¤§é€£ç·šæ•¸
                        break
                    eid_a, eid_b = eids[i], eids[j]
                    if (eid_a, eid_b) in existing_relations:
                        continue
                    
                    common = entity_keywords[eid_a] & entity_keywords[eid_b]
                    # è‡³å°‘è¦æœ‰ 2 å€‹å…±åŒé—œéµå­—æ‰å»ºç«‹é€£ç·š
                    if len(common) >= 2:
                        success = self.kuzu_manager.add_relation(
                            source_id=eid_a,
                            target_id=eid_b,
                            relation_type="keyword_overlap",
                            properties={
                                'keywords': list(common)[:5],
                                'score': len(common),
                                'auto': True
                            }
                        )
                        if success:
                            keyword_link_count += 1
                            existing_relations.add((eid_a, eid_b))
                            existing_relations.add((eid_b, eid_a))

            logger.info(f"ğŸ”‘ é—œéµå­—å…±ç¾é€£ç·š: {keyword_link_count} æ¢")
            total = link_count + keyword_link_count
            logger.info(f"âœ… ç¯€é»äº’é€£å®Œæˆ: å…± {total} æ¢æ–°é€£ç·š (domain={link_count}, keyword={keyword_link_count})")

            # æ›´æ–°åœ–è­œçµ±è¨ˆ
            try:
                self.kuzu_manager.update_graph_stats(graph_id)
            except Exception:
                pass

            return total

        except Exception as e:
            logger.error(f"âŒ ç¯€é»äº’é€£å¤±æ•—: {e}", exc_info=True)
            return 0


class WatcherService:
    """æª”æ¡ˆç›£æ§æœå‹™ç®¡ç†å™¨"""
    
    def __init__(
        self,
        rag_client: RAGFlowClient,
        kuzu_manager: Optional[Union[KuzuDBManager, MockKuzuManager, AsyncKuzuWrapper]],
        dataset_id: str
    ):
        """
        åˆå§‹åŒ–ç›£æ§æœå‹™
        
        Args:
            rag_client: RAGFlow å®¢æˆ¶ç«¯å¯¦ä¾‹
            kuzu_manager: KuzuDB ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆæ”¯æŒ KuzuDBManagerã€MockKuzuManagerã€AsyncKuzuWrapper æˆ– Noneï¼‰
            dataset_id: RAGFlow çŸ¥è­˜åº« ID
        """
        self.rag_client = rag_client
        self.kuzu_manager = kuzu_manager
        self.dataset_id = dataset_id
        self.observer: Optional[Observer] = None
        self.event_handler: Optional[AIFileEventHandler] = None
        self.watch_directory: Optional[str] = None
        
        logger.info("âœ… WatcherService åˆå§‹åŒ–å®Œæˆ")
    
    def start(self, directory: str) -> None:
        """
        å•Ÿå‹•æª”æ¡ˆç›£æ§
        
        Args:
            directory: è¦ç›£æ§çš„ç›®éŒ„è·¯å¾‘
            
        Raises:
            FileNotFoundError: ç›®éŒ„ä¸å­˜åœ¨
            RuntimeError: ç›£æ§æœå‹™å·²åœ¨é‹è¡Œ
        """
        if self.observer is not None and self.observer.is_alive():
            raise RuntimeError("ç›£æ§æœå‹™å·²åœ¨é‹è¡Œä¸­")
        
        # é©—è­‰ç›®éŒ„å­˜åœ¨
        watch_path = Path(directory)
        if not watch_path.exists():
            raise FileNotFoundError(f"ç›£æ§ç›®éŒ„ä¸å­˜åœ¨: {directory}")
        
        if not watch_path.is_dir():
            raise ValueError(f"è·¯å¾‘ä¸æ˜¯ç›®éŒ„: {directory}")
        
        # å‰µå»ºäº‹ä»¶è™•ç†å™¨
        self.event_handler = AIFileEventHandler(
            rag_client=self.rag_client,
            kuzu_manager=self.kuzu_manager,
            dataset_id=self.dataset_id
        )
        
        # å‰µå»ºä¸¦å•Ÿå‹• Observer
        self.observer = Observer()
        self.observer.schedule(
            self.event_handler,
            path=str(watch_path),
            recursive=True  # éè¿´ç›£æ§å­ç›®éŒ„
        )
        
        self.watch_directory = str(watch_path)
        self.observer.start()
        
        logger.info(f"ğŸš€ æª”æ¡ˆç›£æ§å·²å•Ÿå‹•: {self.watch_directory}")
        logger.info(f"ğŸ“ ç›£æ§æ¨¡å¼: éè¿´ç›£æ§æ‰€æœ‰å­ç›®éŒ„")
        logger.info(f"ğŸ“„ æ”¯æ´æ ¼å¼: {', '.join(AIFileEventHandler.SUPPORTED_EXTENSIONS)}")
        
        # è™•ç†å•Ÿå‹•æ™‚å·²å­˜åœ¨çš„æª”æ¡ˆ
        self._process_existing_files(watch_path)
    
    def stop(self) -> None:
        """åœæ­¢æª”æ¡ˆç›£æ§"""
        if self.observer is None:
            logger.warning("âš ï¸  ç›£æ§æœå‹™æœªå•Ÿå‹•")
            return
        
        if not self.observer.is_alive():
            logger.warning("âš ï¸  ç›£æ§æœå‹™æœªé‹è¡Œ")
            return
        
        logger.info("ğŸ›‘ æ­£åœ¨åœæ­¢æª”æ¡ˆç›£æ§...")
        self.observer.stop()
        self.observer.join(timeout=5)
        
        logger.info(f"âœ… æª”æ¡ˆç›£æ§å·²åœæ­¢: {self.watch_directory}")
        self.observer = None
        self.event_handler = None
        self.watch_directory = None
    
    def _process_existing_files(self, directory: Path) -> None:
        """
        è™•ç†å•Ÿå‹•æ™‚å·²å­˜åœ¨çš„æª”æ¡ˆ
        
        Args:
            directory: ç›£æ§ç›®éŒ„è·¯å¾‘
        """
        logger.info("ğŸ” æƒæå·²å­˜åœ¨çš„æª”æ¡ˆ...")
        processed_count = 0
        skipped_count = 0
        
        try:
            # éè¿´æƒææ‰€æœ‰æ”¯æ´çš„æª”æ¡ˆ
            for file_path in directory.rglob("*"):
                # è·³éç›®éŒ„
                if file_path.is_dir():
                    continue
                
                # è·³éå…ƒæ•¸æ“šæ–‡ä»¶
                if file_path.suffix == '.json' and '.meta.json' in file_path.name:
                    skipped_count += 1
                    continue
                
                # æª¢æŸ¥å‰¯æª”å
                if file_path.suffix.lower() in AIFileEventHandler.SUPPORTED_EXTENSIONS:
                    # å†ªç­‰æ€§æª¢æŸ¥ï¼šå¦‚æœå·²æœ‰ .meta.json ä¸”æ¨™è¨˜ processed â†’ è·³é
                    meta_path = file_path.with_suffix(
                        file_path.suffix + '.meta.json'
                    )
                    if meta_path.exists():
                        try:
                            with open(meta_path, 'r', encoding='utf-8') as mf:
                                meta = json.load(mf)
                            if meta.get('processed') is True:
                                file_mtime = datetime.fromtimestamp(
                                    file_path.stat().st_mtime
                                ).isoformat()
                                last_processed = meta.get(
                                    'last_processed_time', ''
                                )
                                if last_processed and file_mtime <= last_processed:
                                    logger.debug(
                                        f"â© è·³éå·²è™•ç†æª”æ¡ˆ: {file_path.name}"
                                    )
                                    skipped_count += 1
                                    continue
                        except Exception:
                            pass  # å…ƒæ•¸æ“šæå£ï¼Œé‡æ–°è™•ç†

                    logger.info(f"ğŸ“„ è™•ç†å·²å­˜åœ¨çš„æª”æ¡ˆ: {file_path.name}")
                    if self.event_handler:
                        self.event_handler._process_file(file_path)
                        processed_count += 1
                else:
                    skipped_count += 1
            
            logger.info(f"âœ… å·²è™•ç† {processed_count} å€‹æª”æ¡ˆï¼Œè·³é {skipped_count} å€‹")
            
        except Exception as e:
            logger.error(f"âŒ æƒæå·²å­˜åœ¨æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
    
    def is_running(self) -> bool:
        """
        æª¢æŸ¥ç›£æ§æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ
        
        Returns:
            True å¦‚æœæ­£åœ¨é‹è¡Œï¼Œå¦å‰‡ False
        """
        return self.observer is not None and self.observer.is_alive()
    
    def get_status(self) -> dict:
        """
        ç²å–ç›£æ§æœå‹™ç‹€æ…‹
        
        Returns:
            ç‹€æ…‹è³‡è¨Šå­—å…¸
        """
        return {
            'running': self.is_running(),
            'watch_directory': self.watch_directory,
            'dataset_id': self.dataset_id,
            'supported_extensions': list(AIFileEventHandler.SUPPORTED_EXTENSIONS)
        }


# ============= æ¸¬è©¦å€å¡Š =============
if __name__ == "__main__":
    """
    æ¸¬è©¦ç›£æ§æœå‹™
    """
    import sys
    
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    try:
        # ===== è«‹å¡«å…¥ä»¥ä¸‹åƒæ•¸ =====
        API_KEY = "ragflow-xxxxx"  # æ›¿æ›æˆä½ çš„ RAGFlow API Key
        DATASET_ID = "your-dataset-id"  # æ›¿æ›æˆä½ çš„çŸ¥è­˜åº« ID
        WATCH_DIR = "./test_watch"  # æ›¿æ›æˆè¦ç›£æ§çš„ç›®éŒ„
        DB_PATH = "C:/BruV_Data/kuzu_db"  # KuzuDB çµ±ä¸€è³‡æ–™åº«è·¯å¾‘
        # =========================
        
        # åˆå§‹åŒ–å®¢æˆ¶ç«¯
        logger.info("åˆå§‹åŒ– RAGFlow å®¢æˆ¶ç«¯...")
        rag_client = RAGFlowClient(api_key=API_KEY)
        
        logger.info("åˆå§‹åŒ– KuzuDB ç®¡ç†å™¨...")
        kuzu_manager = KuzuDBManager(db_path=DB_PATH)
        
        # å‰µå»ºç›£æ§æœå‹™
        logger.info("å‰µå»ºç›£æ§æœå‹™...")
        watcher = WatcherService(
            rag_client=rag_client,
            kuzu_manager=kuzu_manager,
            dataset_id=DATASET_ID
        )
        
        # ç¢ºä¿ç›£æ§ç›®éŒ„å­˜åœ¨
        os.makedirs(WATCH_DIR, exist_ok=True)
        
        # å•Ÿå‹•ç›£æ§
        watcher.start(WATCH_DIR)
        
        logger.info("=" * 60)
        logger.info("ç›£æ§æœå‹™å·²å•Ÿå‹•ï¼")
        logger.info(f"è«‹å°‡æª”æ¡ˆæ”¾å…¥ {WATCH_DIR} ç›®éŒ„é€²è¡Œæ¸¬è©¦")
        logger.info("æŒ‰ Ctrl+C åœæ­¢ç›£æ§")
        logger.info("=" * 60)
        
        # ä¿æŒé‹è¡Œ
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("\næ”¶åˆ°åœæ­¢ä¿¡è™Ÿ...")
            watcher.stop()
            logger.info("ç›£æ§æœå‹™å·²åœæ­¢")
            
    except Exception as e:
        logger.error(f"âŒ éŒ¯èª¤: {type(e).__name__}: {e}", exc_info=True)
        sys.exit(1)
