"""
AI æª”æ¡ˆç›£æ§æœå‹™
ç›£æ§æŒ‡å®šç›®éŒ„ï¼Œè‡ªå‹•ä¸Šå‚³æ–°æª”æ¡ˆè‡³ RAGFlow ä¸¦åŒæ­¥è‡³çŸ¥è­˜åœ–è­œ
æ”¯æ´è£œå„Ÿæ©Ÿåˆ¶ (Compensation) ç¢ºä¿è·¨ç³»çµ±è³‡æ–™ä¸€è‡´æ€§

æ ¸å¿ƒé‚è¼¯å·²æ‹†åˆ†è‡³:
  - file_processor.py  (Saga æµç¨‹ / RAGFlow ä¸Šå‚³ / KuzuDB å¯«å…¥ / Excel è§£æ)
  - node_linker.py     (Domain æ­¸é¡ / é—œéµå­—å…±ç¾é€£ç·š)
"""
import os
import time
import logging
import json
import uuid
import sqlite3
from pathlib import Path
from typing import Optional, Set, Union
from datetime import datetime

from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileSystemEvent

from backend.rag_client import RAGFlowClient
from backend.core.kuzu_manager import KuzuDBManager, MockKuzuManager, AsyncKuzuWrapper

# æ‹†åˆ†æ¨¡çµ„
from backend.services.file_processor import process_file as _process_file_impl
from backend.services.node_linker import build_inter_node_links

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

# DLQ (Dead Letter Queue) è·¯å¾‘
_DATA_DIR = Path(os.environ.get("BRUV_DATA_DIR", str(Path.home() / "BruV_Data")))
_DLQ_DB_PATH = _DATA_DIR / "saga_dlq.db"

# æœ€å¤§é‡è©¦æ¬¡æ•¸èˆ‡é€€é¿åŸºæ•¸
MAX_RETRY_ATTEMPTS = 3
RETRY_BACKOFF_BASE = 2  # seconds


class DeadLetterQueue:
    """
    æ­»ä¿¡ä½‡åˆ— â€” è¨˜éŒ„è£œå„Ÿå¤±æ•—æˆ–è™•ç†å¤±æ•—çš„æ“ä½œï¼Œ
    ä¾›ç®¡ç†å“¡é€é /api/system/saga-dlq ç«¯é»æ‰‹å‹•é‡è©¦æˆ–ç¢ºèªã€‚
    """

    def __init__(self, db_path: Path = _DLQ_DB_PATH):
        self._db_path = db_path
        db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_db()

    def _get_conn(self) -> sqlite3.Connection:
        conn = sqlite3.connect(str(self._db_path), timeout=5)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS dlq (
                    id TEXT PRIMARY KEY,
                    file_path TEXT,
                    file_name TEXT,
                    graph_id TEXT,
                    failed_step TEXT,
                    error_message TEXT,
                    ragflow_doc_id TEXT,
                    kuzu_entity_id TEXT,
                    saga_steps TEXT,
                    created_at TEXT,
                    resolved BOOLEAN DEFAULT 0,
                    resolved_at TEXT
                )
            """)

    def record(self, file_path: Path, failed_step: str, error: str,
               ragflow_doc_id: str = None, kuzu_entity_id: str = None,
               graph_id: str = None, saga_steps: dict = None):
        """è¨˜éŒ„åˆ° DLQ"""
        try:
            with self._get_conn() as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO dlq
                    (id, file_path, file_name, graph_id, failed_step,
                     error_message, ragflow_doc_id, kuzu_entity_id,
                     saga_steps, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    str(uuid.uuid4()),
                    str(file_path),
                    file_path.name if isinstance(file_path, Path) else str(file_path),
                    graph_id,
                    failed_step,
                    error,
                    ragflow_doc_id,
                    kuzu_entity_id,
                    json.dumps(saga_steps or {}, ensure_ascii=False),
                    datetime.now().isoformat(),
                ))
            logger.warning(f"ğŸ“¥ å·²è¨˜éŒ„åˆ° DLQ: {file_path} (step={failed_step})")
        except Exception as e:
            logger.error(f"âŒ DLQ å¯«å…¥å¤±æ•—: {e}")

    def list_unresolved(self, limit: int = 50) -> list:
        """åˆ—å‡ºæœªè§£æ±ºçš„ DLQ é …ç›®"""
        try:
            with self._get_conn() as conn:
                rows = conn.execute(
                    "SELECT * FROM dlq WHERE resolved = 0 ORDER BY created_at DESC LIMIT ?",
                    (limit,)
                ).fetchall()
                return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"âŒ DLQ æŸ¥è©¢å¤±æ•—: {e}")
            return []

    def mark_resolved(self, dlq_id: str) -> bool:
        """æ¨™è¨˜ç‚ºå·²è§£æ±º"""
        try:
            with self._get_conn() as conn:
                conn.execute(
                    "UPDATE dlq SET resolved = 1, resolved_at = ? WHERE id = ?",
                    (datetime.now().isoformat(), dlq_id)
                )
            return True
        except Exception as e:
            logger.error(f"âŒ DLQ æ¨™è¨˜å¤±æ•—: {e}")
            return False


# å…¨åŸŸ DLQ å¯¦ä¾‹
dlq = DeadLetterQueue()


class AIFileEventHandler(FileSystemEventHandler):
    """AI æª”æ¡ˆäº‹ä»¶è™•ç†å™¨"""
    
    # æ”¯æ´çš„æª”æ¡ˆå‰¯æª”å
    SUPPORTED_EXTENSIONS: Set[str] = {'.pdf', '.txt', '.md', '.docx', '.xlsx'}
    
    def __init__(self, rag_client: RAGFlowClient,
                 kuzu_manager: Optional[Union[KuzuDBManager, MockKuzuManager, AsyncKuzuWrapper]],
                 dataset_id: str):
        super().__init__()
        self.rag_client = rag_client
        self.kuzu_manager = kuzu_manager
        self.dataset_id = dataset_id
        logger.info(f"âœ… AIFileEventHandler åˆå§‹åŒ–å®Œæˆï¼Œç›®æ¨™çŸ¥è­˜åº«: {dataset_id}")

    def on_created(self, event: FileSystemEvent) -> None:
        """åµæ¸¬åˆ°æ–°æª”æ¡ˆæ™‚è§¸ç™¼"""
        if event.is_directory:
            return
        file_path = Path(event.src_path)
        if file_path.suffix == '.json' and '.meta.json' in file_path.name:
            return
        if file_path.suffix.lower() not in self.SUPPORTED_EXTENSIONS:
            return
        logger.info(f"ğŸ” åµæ¸¬åˆ°æ–°æª”æ¡ˆ: {file_path}")
        time.sleep(1)  # watchdog å›å‘¼åœ¨ç¨ç«‹ç·šç¨‹ï¼Œä¸é˜»å¡äº‹ä»¶è¿´åœˆ
        self._process_file(file_path)
    
    def _process_file(self, file_path: Path) -> None:
        """å§”æ´¾çµ¦ file_processor.process_file"""
        _process_file_impl(
            rag_client=self.rag_client,
            kuzu_manager=self.kuzu_manager,
            dataset_id=self.dataset_id,
            file_path=file_path,
            dlq=dlq,
            build_inter_node_links_fn=build_inter_node_links,
        )


class WatcherService:
    """æª”æ¡ˆç›£æ§æœå‹™ç®¡ç†å™¨"""
    
    def __init__(
        self,
        rag_client: RAGFlowClient,
        kuzu_manager: Optional[Union[KuzuDBManager, MockKuzuManager, AsyncKuzuWrapper]],
        dataset_id: str
    ):
        self.rag_client = rag_client
        self.kuzu_manager = kuzu_manager
        self.dataset_id = dataset_id
        self.observer: Optional[Observer] = None
        self.event_handler: Optional[AIFileEventHandler] = None
        self.watch_directory: Optional[str] = None
        logger.info("âœ… WatcherService åˆå§‹åŒ–å®Œæˆ")
    
    def start(self, directory: str) -> None:
        """å•Ÿå‹•æª”æ¡ˆç›£æ§"""
        if self.observer is not None and self.observer.is_alive():
            raise RuntimeError("ç›£æ§æœå‹™å·²åœ¨é‹è¡Œä¸­")
        
        # é©—è­‰ç›®éŒ„å­˜åœ¨
        watch_path = Path(directory)
        if not watch_path.exists():
            raise FileNotFoundError(f"ç›£æ§ç›®éŒ„ä¸å­˜åœ¨: {directory}")
        
        if not watch_path.is_dir():
            raise ValueError(f"è·¯å¾‘ä¸æ˜¯ç›®éŒ„: {directory}")
        
        # å‰µå»ºäº‹ä»¶è™•ç†å™¨
        self.event_handler = AIFileEventHandler(
            rag_client=self.rag_client,
            kuzu_manager=self.kuzu_manager,
            dataset_id=self.dataset_id
        )
        
        # å‰µå»ºä¸¦å•Ÿå‹• Observer
        self.observer = Observer()
        self.observer.schedule(
            self.event_handler,
            path=str(watch_path),
            recursive=True  # éè¿´ç›£æ§å­ç›®éŒ„
        )
        
        self.watch_directory = str(watch_path)
        self.observer.start()
        
        logger.info(f"ğŸš€ æª”æ¡ˆç›£æ§å·²å•Ÿå‹•: {self.watch_directory}")
        logger.info(f"ğŸ“ ç›£æ§æ¨¡å¼: éè¿´ç›£æ§æ‰€æœ‰å­ç›®éŒ„")
        logger.info(f"ğŸ“„ æ”¯æ´æ ¼å¼: {', '.join(AIFileEventHandler.SUPPORTED_EXTENSIONS)}")
        
        # è™•ç†å•Ÿå‹•æ™‚å·²å­˜åœ¨çš„æª”æ¡ˆ
        self._process_existing_files(watch_path)
    
    def stop(self) -> None:
        """åœæ­¢æª”æ¡ˆç›£æ§"""
        if self.observer is None:
            logger.warning("âš ï¸  ç›£æ§æœå‹™æœªå•Ÿå‹•")
            return
        
        if not self.observer.is_alive():
            logger.warning("âš ï¸  ç›£æ§æœå‹™æœªé‹è¡Œ")
            return
        
        logger.info("ğŸ›‘ æ­£åœ¨åœæ­¢æª”æ¡ˆç›£æ§...")
        self.observer.stop()
        self.observer.join(timeout=5)
        
        logger.info(f"âœ… æª”æ¡ˆç›£æ§å·²åœæ­¢: {self.watch_directory}")
        self.observer = None
        self.event_handler = None
        self.watch_directory = None
    
    def _process_existing_files(self, directory: Path) -> None:
        """è™•ç†å•Ÿå‹•æ™‚ç›®éŒ„ä¸­å·²å­˜åœ¨çš„æª”æ¡ˆ"""
        logger.info("ğŸ” æƒæå·²å­˜åœ¨çš„æª”æ¡ˆ...")
        processed_count = 0
        skipped_count = 0
        
        try:
            # éè¿´æƒææ‰€æœ‰æ”¯æ´çš„æª”æ¡ˆ
            for file_path in directory.rglob("*"):
                # è·³éç›®éŒ„
                if file_path.is_dir():
                    continue
                
                # è·³éå…ƒæ•¸æ“šæ–‡ä»¶
                if file_path.suffix == '.json' and '.meta.json' in file_path.name:
                    skipped_count += 1
                    continue
                
                # æª¢æŸ¥å‰¯æª”å
                if file_path.suffix.lower() in AIFileEventHandler.SUPPORTED_EXTENSIONS:
                    # å†ªç­‰æ€§æª¢æŸ¥ï¼šå¦‚æœå·²æœ‰ .meta.json ä¸”æ¨™è¨˜ processed â†’ è·³é
                    meta_path = file_path.with_suffix(
                        file_path.suffix + '.meta.json'
                    )
                    if meta_path.exists():
                        try:
                            with open(meta_path, 'r', encoding='utf-8') as mf:
                                meta = json.load(mf)
                            if meta.get('processed') is True:
                                file_mtime = datetime.fromtimestamp(
                                    file_path.stat().st_mtime
                                ).isoformat()
                                last_processed = meta.get(
                                    'last_processed_time', ''
                                )
                                if last_processed and file_mtime <= last_processed:
                                    logger.debug(
                                        f"â© è·³éå·²è™•ç†æª”æ¡ˆ: {file_path.name}"
                                    )
                                    skipped_count += 1
                                    continue
                        except Exception:
                            pass  # å…ƒæ•¸æ“šæå£ï¼Œé‡æ–°è™•ç†

                    logger.info(f"ğŸ“„ è™•ç†å·²å­˜åœ¨çš„æª”æ¡ˆ: {file_path.name}")
                    if self.event_handler:
                        self.event_handler._process_file(file_path)
                        processed_count += 1
                else:
                    skipped_count += 1
            
            logger.info(f"âœ… å·²è™•ç† {processed_count} å€‹æª”æ¡ˆï¼Œè·³é {skipped_count} å€‹")
            
        except Exception as e:
            logger.error(f"âŒ æƒæå·²å­˜åœ¨æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
    
    def is_running(self) -> bool:
        return self.observer is not None and self.observer.is_alive()
    
    def get_status(self) -> dict:
        return {
            'running': self.is_running(),
            'watch_directory': self.watch_directory,
            'dataset_id': self.dataset_id,
            'supported_extensions': list(AIFileEventHandler.SUPPORTED_EXTENSIONS)
        }


# ============= æ¸¬è©¦å€å¡Š =============
if __name__ == "__main__":
    """
    æ¸¬è©¦ç›£æ§æœå‹™
    """
    import sys
    
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    try:
        # ===== è«‹å¡«å…¥ä»¥ä¸‹åƒæ•¸ =====
        API_KEY = "ragflow-xxxxx"  # æ›¿æ›æˆä½ çš„ RAGFlow API Key
        DATASET_ID = "your-dataset-id"  # æ›¿æ›æˆä½ çš„çŸ¥è­˜åº« ID
        WATCH_DIR = "./test_watch"  # æ›¿æ›æˆè¦ç›£æ§çš„ç›®éŒ„
        DB_PATH = "C:/BruV_Data/kuzu_db"  # KuzuDB çµ±ä¸€è³‡æ–™åº«è·¯å¾‘
        # =========================
        
        # åˆå§‹åŒ–å®¢æˆ¶ç«¯
        logger.info("åˆå§‹åŒ– RAGFlow å®¢æˆ¶ç«¯...")
        rag_client = RAGFlowClient(api_key=API_KEY)
        
        logger.info("åˆå§‹åŒ– KuzuDB ç®¡ç†å™¨...")
        kuzu_manager = KuzuDBManager(db_path=DB_PATH)
        
        # å‰µå»ºç›£æ§æœå‹™
        logger.info("å‰µå»ºç›£æ§æœå‹™...")
        watcher = WatcherService(
            rag_client=rag_client,
            kuzu_manager=kuzu_manager,
            dataset_id=DATASET_ID
        )
        
        # ç¢ºä¿ç›£æ§ç›®éŒ„å­˜åœ¨
        os.makedirs(WATCH_DIR, exist_ok=True)
        
        # å•Ÿå‹•ç›£æ§
        watcher.start(WATCH_DIR)
        
        logger.info("=" * 60)
        logger.info("ç›£æ§æœå‹™å·²å•Ÿå‹•ï¼")
        logger.info(f"è«‹å°‡æª”æ¡ˆæ”¾å…¥ {WATCH_DIR} ç›®éŒ„é€²è¡Œæ¸¬è©¦")
        logger.info("æŒ‰ Ctrl+C åœæ­¢ç›£æ§")
        logger.info("=" * 60)
        
        # ä¿æŒé‹è¡Œ
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("\næ”¶åˆ°åœæ­¢ä¿¡è™Ÿ...")
            watcher.stop()
            logger.info("ç›£æ§æœå‹™å·²åœæ­¢")
            
    except Exception as e:
        logger.error(f"âŒ éŒ¯èª¤: {type(e).__name__}: {e}", exc_info=True)
        sys.exit(1)
