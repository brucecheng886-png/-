"""
AI æª”æ¡ˆç›£æ§æœå‹™
ç›£æ§æŒ‡å®šç›®éŒ„ï¼Œè‡ªå‹•ä¸Šå‚³æ–°æª”æ¡ˆè‡³ RAGFlow ä¸¦åŒæ­¥è‡³çŸ¥è­˜åœ–è­œ
"""
import os
import time
import logging
import hashlib
import uuid
import asyncio
from pathlib import Path
from typing import Optional, Set, Union
import pandas as pd
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileSystemEvent

from backend.rag_client import RAGFlowClient
from backend.core.kuzu_manager import KuzuDBManager, MockKuzuManager
from backend.services.task_queue import task_queue, TaskStatus

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)


class AIFileEventHandler(FileSystemEventHandler):
    """AI æª”æ¡ˆäº‹ä»¶è™•ç†å™¨"""
    
    # æ”¯æ´çš„æª”æ¡ˆå‰¯æª”å
    SUPPORTED_EXTENSIONS: Set[str] = {'.pdf', '.txt', '.md', '.docx', '.xlsx'}
    
    def __init__(
        self, 
        rag_client: RAGFlowClient, 
        kuzu_manager: Optional[Union[KuzuDBManager, MockKuzuManager]],
        dataset_id: str
    ):
        """
        åˆå§‹åŒ–æª”æ¡ˆäº‹ä»¶è™•ç†å™¨
        
        Args:
            rag_client: RAGFlow å®¢æˆ¶ç«¯å¯¦ä¾‹
            kuzu_manager: KuzuDB ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆæ”¯æŒ KuzuDBManagerã€MockKuzuManager æˆ– Noneï¼‰
            dataset_id: RAGFlow çŸ¥è­˜åº« ID
        """
        super().__init__()
        self.rag_client = rag_client
        self.kuzu_manager = kuzu_manager
        self.dataset_id = dataset_id
        logger.info(f"âœ… AIFileEventHandler åˆå§‹åŒ–å®Œæˆï¼Œç›®æ¨™çŸ¥è­˜åº«: {dataset_id}")
    
    def on_created(self, event: FileSystemEvent) -> None:
        """
        ç•¶åµæ¸¬åˆ°æ–°æª”æ¡ˆå‰µå»ºæ™‚è§¸ç™¼
        
        Args:
            event: æª”æ¡ˆç³»çµ±äº‹ä»¶
        """
        # éæ¿¾æ‰è³‡æ–™å¤¾
        if event.is_directory:
            logger.debug(f"â­ï¸  å¿½ç•¥ç›®éŒ„: {event.src_path}")
            return
        
        file_path = Path(event.src_path)
        
        # å¿½ç•¥å…ƒæ•¸æ“šæ–‡ä»¶
        if file_path.suffix == '.json' and '.meta.json' in file_path.name:
            logger.debug(f"â­ï¸  å¿½ç•¥å…ƒæ•¸æ“šæ–‡ä»¶: {file_path.name}")
            return
        
        file_extension = file_path.suffix.lower()
        
        # æª¢æŸ¥å‰¯æª”åæ˜¯å¦æ”¯æ´
        if file_extension not in self.SUPPORTED_EXTENSIONS:
            logger.debug(f"â­ï¸  å¿½ç•¥ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {file_path.name} ({file_extension})")
            return
        
        logger.info(f"ğŸ” åµæ¸¬åˆ°æ–°æª”æ¡ˆ: {file_path}")
        
        # ç­‰å¾…æª”æ¡ˆå®Œå…¨å¯«å…¥ï¼ˆä½¿ç”¨éé˜»å¡ç¡çœ æ›¿ä»£ time.sleepï¼‰
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # åœ¨ async ç’°å¢ƒä¸‹ï¼Œä½¿ç”¨ thread-safe æ–¹å¼ç­‰å¾…
                import time as _time
                _time.sleep(1)  # watchdog å›å‘¼åœ¨ç¨ç«‹ç·šç¨‹ï¼Œtime.sleep ä¸é˜»å¡äº‹ä»¶è¿´åœˆ
            else:
                time.sleep(1)
        except RuntimeError:
            time.sleep(1)
        
        # è™•ç†æª”æ¡ˆä¸Šå‚³èˆ‡åœ–è­œæ›´æ–°
        self._process_file(file_path)
    
    def _process_file(self, file_path: Path) -> None:
        """
        è™•ç†æª”æ¡ˆï¼šä¸Šå‚³è‡³ RAGFlow ä¸¦æ›´æ–°çŸ¥è­˜åœ–è­œ
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
        """
        try:
            # è®€å–åœ–è­œå…ƒæ•¸æ“š
            metadata_file = file_path.with_suffix(file_path.suffix + '.meta.json')
            graph_id = "1"  # é è¨­åœ–è­œ ID
            
            if metadata_file.exists():
                try:
                    import json
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                        graph_id = metadata.get('graph_id', "1")
                        logger.info(f"ğŸ“‹ è®€å–åœ–è­œå…ƒæ•¸æ“š: graph_id={graph_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸  è®€å–å…ƒæ•¸æ“šå¤±æ•—ï¼Œä½¿ç”¨é è¨­åœ–è­œ: {e}")
            
            # å‹•ä½œ A: ä¸Šå‚³æª”æ¡ˆè‡³ RAGFlow
            logger.info(f"ğŸ“¤ æ­£åœ¨ä¸Šå‚³æª”æ¡ˆè‡³ RAGFlow: {file_path.name}")
            
            upload_result = self.rag_client.upload_file(
                dataset_id=self.dataset_id,
                file_path=str(file_path)
            )
            
            logger.info(f"âœ… æª”æ¡ˆä¸Šå‚³æˆåŠŸ: {file_path.name}")
            logger.debug(f"ä¸Šå‚³å›æ‡‰: {upload_result}")
            
            # å‹•ä½œ B: å¯«å…¥çŸ¥è­˜åœ–è­œï¼ˆä½¿ç”¨åœ–è­œ IDï¼‰
            # å…ˆå‰µå»ºæª”æ¡ˆä¸»ç¯€é»
            file_node_id = self._add_to_graph(file_path, upload_result, graph_id)
            
            # å¦‚æœæ˜¯ Excel æª”æ¡ˆï¼Œé€²è¡Œæ·±åº¦è§£æ
            if file_path.suffix.lower() == '.xlsx' and file_node_id:
                self._parse_excel_and_link(file_path, file_node_id, graph_id)
            
        except FileNotFoundError as e:
            logger.error(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path} - {e}")
        except Exception as e:
            logger.error(f"âŒ è™•ç†æª”æ¡ˆå¤±æ•—: {file_path.name} - {type(e).__name__}: {e}", exc_info=True)
    
    def _add_to_graph(self, file_path: Path, upload_result: dict, graph_id: str = "1") -> Optional[str]:
        """
        å°‡æª”æ¡ˆè³‡è¨Šæ·»åŠ åˆ°çŸ¥è­˜åœ–è­œï¼ˆå‰µå»ºæª”æ¡ˆä¸»ç¯€é»ï¼‰
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            upload_result: RAGFlow ä¸Šå‚³å›æ‡‰
            graph_id: ç›®æ¨™åœ–è­œ ID
            
        Returns:
            Optional[str]: æˆåŠŸè¿”å›ç¯€é» IDï¼Œå¤±æ•—è¿”å› None
        """
        # å¦‚æœ KuzuDB ä¸å¯ç”¨ï¼Œè·³éåœ–è­œå¯«å…¥
        if not self.kuzu_manager:
            logger.debug(f"â­ï¸  KuzuDB ä¸å¯ç”¨ï¼Œè·³éåœ–è­œå¯«å…¥: {file_path.name}")
            return None
        
        try:
            # æå– Entity IDï¼ˆå„ªå…ˆä½¿ç”¨ RAGFlow å›å‚³çš„ IDï¼‰
            entity_id = self._extract_entity_id(file_path, upload_result)
            
            # æº–å‚™å¯¦é«”å±¬æ€§
            properties = {
                'path': str(file_path.absolute()),
                'size': file_path.stat().st_size,
                'extension': file_path.suffix.lower(),
                'created_time': file_path.stat().st_ctime,
                'dataset_id': self.dataset_id
            }
            
            # å¦‚æœ RAGFlow å›å‚³äº†æ–‡æª” IDï¼Œä¹ŸåŠ å…¥å±¬æ€§
            if 'data' in upload_result and upload_result['data']:
                doc_data = upload_result['data']
                if isinstance(doc_data, dict):
                    properties['document_id'] = doc_data.get('id', entity_id)
                elif isinstance(doc_data, list) and len(doc_data) > 0:
                    properties['document_id'] = doc_data[0].get('id', entity_id)
            
            logger.info(f"ğŸ“Š æ­£åœ¨å¯«å…¥çŸ¥è­˜åœ–è­œ: {file_path.name}")
            
            # æ·»åŠ å¯¦é«”åˆ°åœ–è­œï¼ˆä½¿ç”¨ graph_idï¼‰
            success = self.kuzu_manager.add_entity(
                entity_id=entity_id,
                name=file_path.name,
                entity_type='document',
                properties=properties,
                graph_id=graph_id  # å‚³éåœ–è­œ ID
            )
            
            if success:
                logger.info(f"âœ… åœ–è­œå¯«å…¥æˆåŠŸ: {file_path.name} (ID: {entity_id})")
                return entity_id  # è¿”å›ç¯€é» ID
            else:
                logger.error(f"âŒ åœ–è­œå¯«å…¥å¤±æ•—: {file_path.name}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ åˆ°åœ–è­œå¤±æ•—: {file_path.name} - {type(e).__name__}: {e}", exc_info=True)
            return None
    
    def _extract_entity_id(self, file_path: Path, upload_result: dict) -> str:
        """
        æå–å¯¦é«” IDï¼ˆå„ªå…ˆä½¿ç”¨ RAGFlow å›å‚³çš„ IDï¼Œå¦å‰‡ä½¿ç”¨æª”æ¡ˆåç¨±çš„ Hashï¼‰
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            upload_result: RAGFlow ä¸Šå‚³å›æ‡‰
            
        Returns:
            å¯¦é«” ID
        """
        # å˜—è©¦å¾ RAGFlow å›æ‡‰ä¸­æå– ID
        if 'data' in upload_result and upload_result['data']:
            doc_data = upload_result['data']
            
            # è™•ç†å›æ‡‰å¯èƒ½æ˜¯å­—å…¸æˆ–åˆ—è¡¨çš„æƒ…æ³
            if isinstance(doc_data, dict) and 'id' in doc_data:
                entity_id = doc_data['id']
                logger.debug(f"ä½¿ç”¨ RAGFlow å›å‚³çš„ ID: {entity_id}")
                return entity_id
            elif isinstance(doc_data, list) and len(doc_data) > 0 and 'id' in doc_data[0]:
                entity_id = doc_data[0]['id']
                logger.debug(f"ä½¿ç”¨ RAGFlow å›å‚³çš„ ID: {entity_id}")
                return entity_id
        
        # å¦å‰‡ä½¿ç”¨æª”æ¡ˆåç¨±çš„ Hash
        file_hash = hashlib.md5(str(file_path.absolute()).encode()).hexdigest()
        entity_id = f"doc_{file_hash[:16]}"
        logger.debug(f"ä½¿ç”¨ Hash ID: {entity_id}")
        return entity_id
    
    def _parse_excel_and_link(self, file_path: Path, file_node_id: str, graph_id: str = "1") -> None:
        """
        è§£æ Excel æª”æ¡ˆï¼Œå°‡æ¯ä¸€åˆ—è³‡æ–™è½‰æ›ç‚ºç¨ç«‹çš„åœ–è­œå­ç¯€é»ï¼Œä¸¦å»ºç«‹èˆ‡ä¸»ç¯€é»çš„é€£ç·š
        
        Args:
            file_path: Excel æª”æ¡ˆè·¯å¾‘
            file_node_id: æª”æ¡ˆä¸»ç¯€é»çš„ ID
            graph_id: ç›®æ¨™åœ–è­œ ID
        """
        # å¦‚æœ KuzuDB ä¸å¯ç”¨ï¼Œè·³éè™•ç†
        if not self.kuzu_manager:
            logger.debug(f"â­ï¸  KuzuDB ä¸å¯ç”¨ï¼Œè·³é Excel è§£æ: {file_path.name}")
            return
        
        try:
            logger.info(f"ğŸ“Š é–‹å§‹ Excel æ·±åº¦è§£æ: {file_path.name}")
            
            # è®€å– Excel æª”æ¡ˆ
            df = pd.read_excel(file_path)
            
            # å°‡æ¬„ä½åç¨±è½‰ç‚ºå°å¯«ä»¥é€²è¡Œä¸å€åˆ†å¤§å°å¯«çš„åŒ¹é…
            df.columns = df.columns.str.lower()
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«å¿…è¦æ¬„ä½ï¼ˆæ”¯æŒ description å’Œ distribtionï¼‰
            required_columns = ['srl', 'title', 'link']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ description æˆ– distribtion
            has_description = 'description' in df.columns or 'distribtion' in df.columns
            if not has_description:
                missing_columns.append('description/distribtion')
            
            if missing_columns:
                logger.warning(
                    f"âš ï¸  Excel æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ {missing_columns}ï¼Œè·³éæ·±åº¦è§£æ: {file_path.name}"
                )
                return
            
            logger.info(f"âœ… Excel æ ¼å¼é©—è­‰é€šéï¼ŒåŒ…å« {len(df)} åˆ—è³‡æ–™")
            
            # éæ­·æ¯ä¸€åˆ—ï¼Œå‰µå»ºå­ç¯€é»ä¸¦å»ºç«‹é€£ç·š
            success_count = 0
            error_count = 0
            link_count = 0
            
            for index, row in df.iterrows():
                try:
                    # æå–æ¬„ä½å€¼
                    srl = str(row.get('srl', '')).strip()
                    title = str(row.get('title', '')).strip()
                    link = str(row.get('link', '')).strip()
                    # æ”¯æŒ description æˆ– distribtion
                    description = str(row.get('description', row.get('distribtion', ''))).strip()
                    
                    # è·³éç©ºç™½åˆ—
                    if not srl or not title:
                        logger.debug(f"â­ï¸  è·³éç©ºç™½åˆ— {index + 1}")
                        continue
                    
                    # ç”Ÿæˆå”¯ä¸€çš„å­ç¯€é» ID
                    child_node_id = f"{file_path.stem}_row_{srl}"
                    
                    # æº–å‚™ç¯€é»å±¬æ€§ï¼ˆå°‡ link å’Œ description æ”¾å…¥ propertiesï¼‰
                    properties = {
                        'srl': srl,
                        'link': link,
                        'description': description,
                        'source_file': file_path.name,
                        'row_index': index + 1,
                        'dataset_id': self.dataset_id
                    }
                    
                    # æ­¥é©Ÿ 1: å‰µå»ºå­ç¯€é»ï¼ˆä½¿ç”¨ graph_idï¼‰
                    node_success = self.kuzu_manager.add_entity(
                        entity_id=child_node_id,
                        name=title,
                        entity_type='Resource',  # å°æ‡‰å‰ç«¯é…è‰²
                        properties=properties,
                        graph_id=graph_id  # å‚³éåœ–è­œ ID
                    )
                    
                    if node_success:
                        success_count += 1
                        logger.debug(f"âœ… åˆ— {index + 1} å­ç¯€é»å‰µå»ºæˆåŠŸ: {title} (ID: {child_node_id})")
                        
                        # æ­¥é©Ÿ 2: å»ºç«‹é€£ç·š (æª”æ¡ˆä¸»ç¯€é» -> å­ç¯€é»)
                        link_success = self.kuzu_manager.add_relation(
                            source_id=file_node_id,
                            target_id=child_node_id,
                            relation_type="contains",
                            properties={'row': index + 1}
                        )
                        
                        if link_success:
                            link_count += 1
                            logger.debug(f"ğŸ”— åˆ— {index + 1} é€£ç·šå‰µå»ºæˆåŠŸ: {file_node_id} -[contains]-> {child_node_id}")
                        else:
                            logger.warning(f"âš ï¸  åˆ— {index + 1} é€£ç·šå‰µå»ºå¤±æ•—")
                    else:
                        error_count += 1
                        logger.warning(f"âš ï¸  åˆ— {index + 1} å­ç¯€é»å‰µå»ºå¤±æ•—: {title}")
                        
                except Exception as e:
                    error_count += 1
                    logger.error(
                        f"âŒ è™•ç†åˆ— {index + 1} å¤±æ•—: {type(e).__name__}: {e}",
                        exc_info=True
                    )
            
            logger.info(
                f"ğŸ“Š Excel æ·±åº¦è§£æå®Œæˆ: {file_path.name} | "
                f"å­ç¯€é»: {success_count}, é€£ç·š: {link_count}, å¤±æ•—: {error_count}"
            )
            
        except pd.errors.EmptyDataError:
            logger.warning(f"âš ï¸  Excel æª”æ¡ˆç‚ºç©º: {file_path.name}")
        except pd.errors.ParserError as e:
            logger.error(f"âŒ Excel è§£æéŒ¯èª¤: {file_path.name} - {e}")
        except Exception as e:
            logger.error(
                f"âŒ Excel æ·±åº¦è§£æå¤±æ•—: {file_path.name} - {type(e).__name__}: {e}",
                exc_info=True
            )


class WatcherService:
    """æª”æ¡ˆç›£æ§æœå‹™ç®¡ç†å™¨"""
    
    def __init__(
        self,
        rag_client: RAGFlowClient,
        kuzu_manager: Optional[Union[KuzuDBManager, MockKuzuManager]],
        dataset_id: str
    ):
        """
        åˆå§‹åŒ–ç›£æ§æœå‹™
        
        Args:
            rag_client: RAGFlow å®¢æˆ¶ç«¯å¯¦ä¾‹
            kuzu_manager: KuzuDB ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆæ”¯æŒ KuzuDBManagerã€MockKuzuManager æˆ– Noneï¼‰
            dataset_id: RAGFlow çŸ¥è­˜åº« ID
        """
        self.rag_client = rag_client
        self.kuzu_manager = kuzu_manager
        self.dataset_id = dataset_id
        self.observer: Optional[Observer] = None
        self.event_handler: Optional[AIFileEventHandler] = None
        self.watch_directory: Optional[str] = None
        
        logger.info("âœ… WatcherService åˆå§‹åŒ–å®Œæˆ")
    
    def start(self, directory: str) -> None:
        """
        å•Ÿå‹•æª”æ¡ˆç›£æ§
        
        Args:
            directory: è¦ç›£æ§çš„ç›®éŒ„è·¯å¾‘
            
        Raises:
            FileNotFoundError: ç›®éŒ„ä¸å­˜åœ¨
            RuntimeError: ç›£æ§æœå‹™å·²åœ¨é‹è¡Œ
        """
        if self.observer is not None and self.observer.is_alive():
            raise RuntimeError("ç›£æ§æœå‹™å·²åœ¨é‹è¡Œä¸­")
        
        # é©—è­‰ç›®éŒ„å­˜åœ¨
        watch_path = Path(directory)
        if not watch_path.exists():
            raise FileNotFoundError(f"ç›£æ§ç›®éŒ„ä¸å­˜åœ¨: {directory}")
        
        if not watch_path.is_dir():
            raise ValueError(f"è·¯å¾‘ä¸æ˜¯ç›®éŒ„: {directory}")
        
        # å‰µå»ºäº‹ä»¶è™•ç†å™¨
        self.event_handler = AIFileEventHandler(
            rag_client=self.rag_client,
            kuzu_manager=self.kuzu_manager,
            dataset_id=self.dataset_id
        )
        
        # å‰µå»ºä¸¦å•Ÿå‹• Observer
        self.observer = Observer()
        self.observer.schedule(
            self.event_handler,
            path=str(watch_path),
            recursive=True  # éè¿´ç›£æ§å­ç›®éŒ„
        )
        
        self.watch_directory = str(watch_path)
        self.observer.start()
        
        logger.info(f"ğŸš€ æª”æ¡ˆç›£æ§å·²å•Ÿå‹•: {self.watch_directory}")
        logger.info(f"ğŸ“ ç›£æ§æ¨¡å¼: éè¿´ç›£æ§æ‰€æœ‰å­ç›®éŒ„")
        logger.info(f"ğŸ“„ æ”¯æ´æ ¼å¼: {', '.join(AIFileEventHandler.SUPPORTED_EXTENSIONS)}")
        
        # è™•ç†å•Ÿå‹•æ™‚å·²å­˜åœ¨çš„æª”æ¡ˆ
        self._process_existing_files(watch_path)
    
    def stop(self) -> None:
        """åœæ­¢æª”æ¡ˆç›£æ§"""
        if self.observer is None:
            logger.warning("âš ï¸  ç›£æ§æœå‹™æœªå•Ÿå‹•")
            return
        
        if not self.observer.is_alive():
            logger.warning("âš ï¸  ç›£æ§æœå‹™æœªé‹è¡Œ")
            return
        
        logger.info("ğŸ›‘ æ­£åœ¨åœæ­¢æª”æ¡ˆç›£æ§...")
        self.observer.stop()
        self.observer.join(timeout=5)
        
        logger.info(f"âœ… æª”æ¡ˆç›£æ§å·²åœæ­¢: {self.watch_directory}")
        self.observer = None
        self.event_handler = None
        self.watch_directory = None
    
    def _process_existing_files(self, directory: Path) -> None:
        """
        è™•ç†å•Ÿå‹•æ™‚å·²å­˜åœ¨çš„æª”æ¡ˆ
        
        Args:
            directory: ç›£æ§ç›®éŒ„è·¯å¾‘
        """
        logger.info("ğŸ” æƒæå·²å­˜åœ¨çš„æª”æ¡ˆ...")
        processed_count = 0
        skipped_count = 0
        
        try:
            # éè¿´æƒææ‰€æœ‰æ”¯æ´çš„æª”æ¡ˆ
            for file_path in directory.rglob("*"):
                # è·³éç›®éŒ„
                if file_path.is_dir():
                    continue
                
                # è·³éå…ƒæ•¸æ“šæ–‡ä»¶
                if file_path.suffix == '.json' and '.meta.json' in file_path.name:
                    skipped_count += 1
                    continue
                
                # æª¢æŸ¥å‰¯æª”å
                if file_path.suffix.lower() in AIFileEventHandler.SUPPORTED_EXTENSIONS:
                    logger.info(f"ğŸ“„ è™•ç†å·²å­˜åœ¨çš„æª”æ¡ˆ: {file_path.name}")
                    if self.event_handler:
                        self.event_handler._process_file(file_path)
                        processed_count += 1
                else:
                    skipped_count += 1
            
            logger.info(f"âœ… å·²è™•ç† {processed_count} å€‹æª”æ¡ˆï¼Œè·³é {skipped_count} å€‹")
            
        except Exception as e:
            logger.error(f"âŒ æƒæå·²å­˜åœ¨æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
    
    def is_running(self) -> bool:
        """
        æª¢æŸ¥ç›£æ§æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ
        
        Returns:
            True å¦‚æœæ­£åœ¨é‹è¡Œï¼Œå¦å‰‡ False
        """
        return self.observer is not None and self.observer.is_alive()
    
    def get_status(self) -> dict:
        """
        ç²å–ç›£æ§æœå‹™ç‹€æ…‹
        
        Returns:
            ç‹€æ…‹è³‡è¨Šå­—å…¸
        """
        return {
            'running': self.is_running(),
            'watch_directory': self.watch_directory,
            'dataset_id': self.dataset_id,
            'supported_extensions': list(AIFileEventHandler.SUPPORTED_EXTENSIONS)
        }


# ============= æ¸¬è©¦å€å¡Š =============
if __name__ == "__main__":
    """
    æ¸¬è©¦ç›£æ§æœå‹™
    """
    import sys
    
    # é…ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    try:
        # ===== è«‹å¡«å…¥ä»¥ä¸‹åƒæ•¸ =====
        API_KEY = "ragflow-xxxxx"  # æ›¿æ›æˆä½ çš„ RAGFlow API Key
        DATASET_ID = "your-dataset-id"  # æ›¿æ›æˆä½ çš„çŸ¥è­˜åº« ID
        WATCH_DIR = "./test_watch"  # æ›¿æ›æˆè¦ç›£æ§çš„ç›®éŒ„
        DB_PATH = "C:/BruV_Data/kuzu_db"  # KuzuDB çµ±ä¸€è³‡æ–™åº«è·¯å¾‘
        # =========================
        
        # åˆå§‹åŒ–å®¢æˆ¶ç«¯
        logger.info("åˆå§‹åŒ– RAGFlow å®¢æˆ¶ç«¯...")
        rag_client = RAGFlowClient(api_key=API_KEY)
        
        logger.info("åˆå§‹åŒ– KuzuDB ç®¡ç†å™¨...")
        kuzu_manager = KuzuDBManager(db_path=DB_PATH)
        
        # å‰µå»ºç›£æ§æœå‹™
        logger.info("å‰µå»ºç›£æ§æœå‹™...")
        watcher = WatcherService(
            rag_client=rag_client,
            kuzu_manager=kuzu_manager,
            dataset_id=DATASET_ID
        )
        
        # ç¢ºä¿ç›£æ§ç›®éŒ„å­˜åœ¨
        os.makedirs(WATCH_DIR, exist_ok=True)
        
        # å•Ÿå‹•ç›£æ§
        watcher.start(WATCH_DIR)
        
        logger.info("=" * 60)
        logger.info("ç›£æ§æœå‹™å·²å•Ÿå‹•ï¼")
        logger.info(f"è«‹å°‡æª”æ¡ˆæ”¾å…¥ {WATCH_DIR} ç›®éŒ„é€²è¡Œæ¸¬è©¦")
        logger.info("æŒ‰ Ctrl+C åœæ­¢ç›£æ§")
        logger.info("=" * 60)
        
        # ä¿æŒé‹è¡Œ
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("\næ”¶åˆ°åœæ­¢ä¿¡è™Ÿ...")
            watcher.stop()
            logger.info("ç›£æ§æœå‹™å·²åœæ­¢")
            
    except Exception as e:
        logger.error(f"âŒ éŒ¯èª¤: {type(e).__name__}: {e}", exc_info=True)
        sys.exit(1)
